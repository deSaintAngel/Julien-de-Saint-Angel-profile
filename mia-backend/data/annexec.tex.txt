Cette petite annexe vise \`a observer le phénomène de collapse mentionné par (voir aussi paragraphe ) qui a pour conséquence la n\'ecessité de fixer les paramètres de l'hypersphère recherchée. La structure de l'encodeur $_W$ donnée dans la figure . Dans les expérimentations à suivre, on considère une sphère en dimension 64 pour laquelle les hyper-paramètres ont été fixés comme $= 0.1$ et $ = 0.0003321558199348189$. La fonction de coût a optimiser est celle de la Deep SVDD. \\ [hbp] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(2)$}; (linear1) [rectangle, right of=input, draw, rotate=90] { Linear $(64)$}; (bn1) [rectangle, draw, right of=linear1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (linear2) [rectangle, draw, right of=lrelu1, rotate=90] { Linear $(64)$}; (bn2) [rectangle, draw, right of=linear2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (linear3) [rectangle, draw, right of=lrelu2, rotate=90] { Linear $(64)$}; [->] (input) -- (linear1); [->] (linear1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (linear2); [->] (linear2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (linear3); On s'intéresse tout d'abord cas où le centre et le rayon de la sphère ne sont pas optimisés ; le rayon est fixé à 0 et le centre est fixé à une valeur arbitraire (Deep soft-boundary SVDD). La figure montre l'évolution du rayon de l'hypersphère, de la moyenne de la norme de $_W(x)$ au cours des itérations et l'histogramme des $^2 - ||_{W^*}() - ||^2$ à la fin de l'algorithme.\\ [htpb] {0.26} {0.26} $} {0.44} () - ||^2$} $ fixés} On observe dans les sous-figures et que l'encodeur $_W$ apprend à projeter les points d'entrée $$ sur le centre de l'hypersphère puisque la norme de $_W(x)$ converge vers une valeur non nulle et la distribution de $-||_{W^*}() - ||^2$ est autour de $0$.\\ On se place maintenant dans le cas où le centre de l'hypersphère est fixé à une valeur arbitraire et le rayon est fixé à 1. La figure montre les mêmes éléments que la figure mais pour le cas où le rayon de l'hypersphère est fixé à 1.\\ [htpb] {0.26} {0.26} $} {0.44} () - ||^2$} $ fixés} La méthode correspondante à cette expérimentation n'est pas la méthode SVDD car il n'y a pas d'optimisation concernant le rayon. L'encodeur $_W$ a placé 90\ On considère maintenant le cas où les paramètres de l'hypersphère sont libres, c'est-à-dire que le centre et le rayon de l'hypersphère sont optimisés pendant l'entraînement. La figure montre l'évolution du rayon de l'hypersphère (initialisé à 0), de la moyenne de la norme de $_W(x)$ au cours des itérations et l'histogramme des $^{*2} - ||_{W^*}() - ^*||^2$ après entraînement.\\ [htpb] {0.26} {0.26} $} {0.44} - ||_{W^*}() - ^*||^2$} $ libres} On observe dans que le rayon reste constant à 0 alors que $$ converge lentement vers une valeur fixe supérieure à 0. La sphère a donc dégénéré et les scores $||_{W^*}() - ^*||^2$ pour les données d'entraînement et surtout de test sont toutes proches de $0$. La méthode est donc inutilisable pour la détection d'anomalie.\\ Enfin, si on initialise le rayon de l'hypersphère à 1 tout en laissant les paramètres $$ et $$ libres, on observe le même phénomène de dégénérescence. Cela est illustré dans la figure .\\ [htpb] {0.26} {0.26} $} {0.44} - ||_{W^*}() - ^*||^2$} $ libres}