{chapter}{Introduction} Cette thèse explore une variante des couches classiques de réseaux de neurones, remplaçant les hyperplans par des hypersphères. Contrairement aux couches denses classiques, qui sont généralement définies par des équations linéaires des formes $W + = 0$, les couches hypersphériques reposent sur des frontières définies par des hypersphères, permettant une partition non linéaire de l'espace $^n$. \ [H] {valign=t,minipage=} Ce changement de représentation ouvre de nouvelles perspectives pour la modélisation de structures complexes dans des espaces de données en utilisant l'algèbre géométrique conforme. Cette approche permet de représenter de manière unifiée hyperplans et hypersphères sous forme de vecteurs, facilitant ainsi l'intégration de cette approche dans des architectures de réseaux neuronaux traditionnels.\\ Les couches hypersphériques s'appliquent aussi bien aux couches denses qu'aux couches convolutives. En remplaçant les hyperplans par des hypersphères, ces couches permettent de définir des frontières de décision non linéaires, adaptées aux tâches de partitionnement et de classification complexes. Les hypersphères peuvent ainsi offrir une nouvelle façon de capturer la complexité des relations entre les données et permettent de mieux modéliser les structures sous-jacentes.\\ Cependant, l'intégration de ces couches dans des architectures de réseaux de neurones soulève plusieurs défis pratiques. L'un des plus importants concerne l'initialisation des paramètres des couches hypersphériques. En effet, cette question est un défi particulièrement complexe en raison de la non-linéarité de la transformation et de la difficulté d'assurer une convergence stable lors de l'apprentissage. La thèse explore différentes stratégies d'initialisation des poids, notamment l'adaptation des méthodes classiques comme celle de Glorot et Bengio. Il est essentiel de bien paramétrer l'initialisation des neurones hypersphériques pour éviter des divergences numériques, garantir une convergence efficace des modèles et offrir la possibilité d'enchaîner et d'intégrer ces couches dans des architectures plus complexes.\\ La détection d'anomalies est une tâche cruciale dans de nombreux domaines, tels que la sécurité informatique, la surveillance industrielle, la finance, et le diagnostic médical. Elle vise à identifier les points de données qui s'écartent significativement des modèles de distribution attendus, souvent appelés valeurs aberrantes ou anomalies. Cette problématique devient particulièrement complexe dans des contextes où les données sont de grande dimension, non linéaires, ou lorsque les distributions des données ne suivent pas des lois statistiques simples comme la loi normale. Ainsi, l'amélioration des méthodes de détection d'anomalies reste un défi de taille, surtout dans les situations complexes où les modèles classiques échouent à capturer des structures sous-jacentes complexes.\\ Les approches classiques de détection d'anomalies, telles que les méthodes basées sur la distance (k-plus proches voisins), la densité (Local Outlier Factor) ou le clustering (k-means), se confrontent à des limitations face à des données complexes, non linéaires, ou avec des distributions non gaussiennes. Ces méthodes se heurtent souvent aux problèmes liés aux données qui sont souvent hétérogènes, déséquilibrées et de grande dimension. Il devient ainsi nécessaire de développer des approches plus robustes et interprétables, capables de mieux gérer ces complexités. Ce sont les problématiques auxquelles les approches par apprentissage profond vont pouvoir répondre.\\ L'une des applications principales de cette thèse concerne la détection d'anomalies. En exploitant les couches hypersphériques, cette thèse propose une nouvelle méthode, le Deep sph-SVDD, qui revisite l'approche du Support Vector Data Description (SVDD) en l'intégrant dans le cadre de l'algèbre géométrique conforme. Ce modèle apprend directement les paramètres du centre et du rayon de l'hypersphère dans un espace transformé, permettant ainsi une meilleure séparation des données normales et des anomalies. De plus, une extension de cette méthode, nommée Deep M sph-SVDD, introduit plusieurs hypersphères pour mieux capturer des groupes distincts de données normales, renforçant ainsi la capacité du modèle à détecter des anomalies dans des ensembles de données complexes et variés.\\ Afin d'explorer le potentiel de ces couches hypersphériques en détection d'anomalies, cette thèse adopte une méthodologie combinant des approches théoriques, algorithmiques et expérimentales~:\\ Étude théorique : Un théorème d'approximation est établi, démontrant que les réseaux de neurones à couches hypersphériques peuvent approximer des fonctions continues définies sur un compact. Cette approche repose sur des résultats issus du théorème de Schwartz pour les fonctions d'une seule variable. Développement algorithmique : La thèse propose de nouveaux algorithmes pour la détection d'anomalies, incluant le Deep sph-SVDD et le Deep M sph-SVDD, qui apprennent directement les paramètres associés aux hypersphères englobantes (centre, rayon, etc.) à partir des données. Implémentation et expérimentation : Les algorithmes sont implémentés à l'aide de bibliothèques spécialisées pour l'apprentissage profond, telles que TensorFlow et PyTorch, puis évalués sur des ensembles de données comme MNIST et CIFAR-10. Les résultats expérimentaux montrent que les méthodes proposées sont compétitives par rapport aux méthodes classiques en termes de performance (AUC-ROC, AUC-PR, score F1) et d'interprétabilité. En résumé, cette thèse propose une exploration approfondie des réseaux de neurones à couches hypersphériques, alliant avancées théoriques et applications pratiques dans le domaine de la détection d'anomalies. Les résultats démontrent non seulement que les modèles développés restent compétitifs et complémentaires par rapport aux modèles classiques, mais aussi leur potentiel à améliorer l'interprétabilité des modèles. Néanmoins, des défis demeurent, notamment en ce qui concerne la stabilisation de l'apprentissage, ouvrant ainsi des pistes pour des recherches futures visant à rendre ces modèles plus robustes et applicables à une plus large gamme de tâches.