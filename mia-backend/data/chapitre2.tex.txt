Dans ce chapitre, nous explorons l’utilisation d’une couche hypersphérique pour résoudre des problèmes de régression, en nous intéressant notamment à sa capacité d’approximation. Cette étude s’inscrit dans le prolongement naturel des travaux sur l'approximation universelle dans les réseaux neuronaux classiques. Rappelons que dans ce cadre, une question fondamentale concerne la capacité d’une couche dense, composée d’un nombre fini \( J \) de neurones classiques (dotés de fonctions d’activation standard comme la sigmoïde ou ), à approcher une fonction donnée. Dans ce chapitre, on s'intéresse à l'approximation d'une fonction continue à support compact $ f ^0(K) $, où $K$ est un compact de $^n$. Derrière cette question, il y a l'idée que c'est la capacité d'un réseau de neurones à approximer une grande variété de fonctions qui en fait un outil particulièrement adapté à de nombreuses tâches, telles que la classification ou la régression.\\ Nous adoptons ici la convention usuelle de la communauté pour l'approximation d'une fonction de $^n$ vers $$. Cependant, dans le chapitre suivant, afin d'éviter toute ambiguïté, nous réserverons la notation $n$ pour désigner la dimension de l'entrée et introduirons $m$ pour la dimension du plongement. Par cohérence, nous avons choisi de garder $J$ pour le nombre de fonctions ridge aussi bien que pour le nombre de sphères.\\ Bien que différentes architectures de réseaux de neurones soient possibles, nous posons ici la question de savoir si une couche composée de neurones hypersphériques possède des capacités d'approximation intéressantes. Plus précisément, peut-on, grâce à une combinaison linéaire des $ J $ sorties de ces neurones, approximer avec une précision arbitraire toute fonction $ f ^0(K)$, $K$ compact de $^n$, au sens de la norme uniforme ?\\ Pour répondre à cette question, nous commençons par rappeler quelques notions concernant le théorème d’approximation universelle dans le cadre classique des réseaux à une couche cachée. Dans ce cadre, le théorème est dit universel : il garantit la capacité d’un réseau dense, composé d’un nombre fini de neurones classiques avec des fonctions d’activation appropriées, à approcher toute fonction continue définie sur un compact de $^n$. L'approche classique pour démontrer le théorème d'approximation universelle repose sur des propriétés et des méthodes analytiques qui ne peuvent pas toutes être directement appliquées dans le contexte des neurones à couches hypersphériques.\\ Cela nous a conduit à adopter une approche différente. Plus précisément, nous établissons un théorème d’approximation pour les fonctions continues définies sur un compact de $$, en nous appuyant sur un théorème de Schwartz. Ce dernier exploite les combinaisons linéaires de translations. Cette approche, simple et efficace, présente ici une limitation notable : elle n’est pas universelle au sens large, car elle impose que les fonctions à approcher soient scalaires.\\ \\ Ce paragraphe est un rappel sur les propriétés d’approximation bien établies des réseaux de neurones denses à une couche cachée. Nous y rappelons les conditions permettant de démontrer que ces réseaux peuvent approximer toute fonction continue \( f ^0(K) \), où \(K\) est un compact de \(^n\), au sens de la norme uniforme. Ces résultats posent une base essentielle pour ensuite discuter des capacités d’approximation des neurones dans des architectures non conventionnelles. \\ Dans cette perspective, tout d’abord la sortie d’un réseau de neurones dense à une couche cachée est décrite, ce qui permet d’illustrer le rôle des combinaisons linéaires des sorties des neurones, avec application d’une fonction d’activation, dans le processus d’approximation. Cette formalisation conduit naturellement à introduire la notion de fonction . Les fonctions permettent, en effet, de simplifier l’analyse en réduisant la question d’approximation à l’étude de propriétés de fonctions unidimensionnelles. Enfin, cette section pose la question de la densité des espaces fonctionnels engendrés par les sorties de réseaux, c’est-à-dire de déterminer si ces espaces sont suffisamment riches pour permettre une approximation uniforme de toute fonction continue définie sur un compact donné. \\ {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!80]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,3} (K-) at (0,-1.8-3) {$x_{}$}; / in {1,...,5} (J-) at (2,-1.8-1.5) { $ _{}. + b_{}$}; / in {1,...,5} (K-1) edge (J-); / in {1,...,5} (K-2) edge (J-); / in {1,...,5} (K-3) edge (J-); (I) {$ _{j=1}^{5}_j(_{j}.+b_{j})$}; in {1,...,5} (J-) edge (I); ^3$ et à valeur dans $$} Dans le cas classique des couches denses, le schéma correspond à l'architecture du réseaux de neurone considéré. On notera \[ = (x_1, , x_n) ^n \] les valeurs d'entrée, et \[ H()=(_1 + b_1, , _J + b_J) ^J \]\\ est le vecteur composé des éléments de sortie de chaque neurone de la couche cachée avant application de la fonction d'activation $ : $. On peut écrire la sortie d'un réseau de $J$ neurones en une couche cachée comme suit~: \\ y() = _{j=1}^{J} _j (_j + b_j) , \\ les \( _j = (w_{j,i})_{i=1}^n ^n \) (respectivement \( b_j \)), \( j = 1, , J \), étant des vecteurs poids (respectivement des biais). Les coefficients $_j$ de la combinaison linéaire sont également des scalaires. Le produit scalaire de deux vecteurs \( \) et \( = (x_i)_{i=1}^n \) de \( ^n \) est noté \( \) et défini par~:\\ \[ = _{i=1}^{n} w_i x_i. \]\\ Si on considère une fonction donnée, supposée à valeur scalaire, \( f : ^n \), alors, dans l'idéal, le but est d'écrire la fonction $f$ sous une forme correspondant à la structure d'un réseau de neurones~: \[ f() = _{j=1}^{J} _j (_j + b_j) = y(). \] Comme la structure du réseau est fixée, les inconnues sont la dimension \( J \) de la couche cachée ainsi que les paramètres \( _j, _{j}, b_j \), \( 1 j J \), qui dépendent de la fonction \( f \). Il faut noter cependant que la fonction $$ ne doit pas dépendre de $f$, sinon cela impliquerait que la fonction d'activation change selon la fonction à approcher. L'égalité ci-dessus pour une classe infinie de fonctions est donc ``difficile" à obtenir. On cherche plutôt une approximation.\\ En reprenant l’écriture de la sortie d’un réseau de neurones, il est possible d’observer que, pour tout choix de \( w_{j,i} \) et \( b_j \), les termes \( (_{j} + b_j) \) apparaissant dans la somme peuvent être identifiés comme des fonctions , définies selon la définition suivante~: [Pinkus ] Une fonction est une fonction à plusieurs variables $h : ^n $ de la forme )$} où $g : $ et $= (a_1, , a_n) ^n $. Il est possible d’écrire la sortie du réseau sous la forme d'une combinaison linéaire de fonctions ridge~:\\ $${ll} y( ) & = _{j} _{j} ( _{ j} _{}+b_{j})\\ \\ & := _{j} _{j} g_j(_j )\\ \\ & \{g() : ^n \} $$ en posant $g_j(x):=(x+b_j)$. On voit en particulier que la sortie du réseau appartient à un espace vectoriel engendré par des fonctions ridge. Il faut donc voir quel type de fonction peut être approchée efficacement par cet espace vectoriel engendré. C'est l'objet du paragraphe suivant. Dans son article "Approximation theory of the MLP model in neural networks", A. Pinkus effectue un état de l'art sur la question de densité qui consiste à montrer que l'ensemble () := \{ ( - b_{}): b , ^n\}$} est dense dans ^0(^n)$}. L’objectif est d’approcher toute fonction continue sur un compact par une combinaison linéaire de fonctions . En terme de densité, il s’agit de montrer que, toute fonction \( f ^{0}( ^n) \) appartient à l'adhérence de $()$~:\\ f \{( + b): ^n, b\}}= ()}, l'adhérence $()}$ étant considérée au sens de la norme qu'on choisit pour mesurer la qualité de l'approximation de $f$ par une fonction de $()$ : on utilise dans ce chapitre la norme uniforme}$ est la notation pour désigner l'adhérence au sens de la norme uniforme}. \\ Les étapes du raisonnement détaillé dans sont les suivantes~:\\ Étape 1~: Se ramener au cas unidimensionnel. Cette première étape correspond à la proposition 3.3 de l'article de Pinkus (cf. Annexe ) Étape 2~: Montrer que tout élément ^{0}(K)$}, peut être approché par une combinaison linéaire d'éléments de $(,,)$ où (,,) = vect\{( -)~: , \, \} avec $^{}()$, $ $ un sous-ensemble de $$ contenant une suite tendant vers 0 et $ $ un intervalle ouvert quelconque de $$. Cette étape correspond à la proposition 3.4 de l'article de Pinkus (cf. Annexe ). Étape 3~: Réduire l'hypothèse de régularité sur l'activation. A l'étape 2, on a supposé $^{}()$. Ici on montre qu'on peut se contenter de l'hypothèse $^{0}()$ (cf. Annexe ). Cela permet d'établir le théorème suivant~:\\ [colframe=blue, colback=white!10, title=] Soit $ ^{0}()$. Alors, $()$ est dense dans $^{0}( ^{n})$ au sens de la convergence uniforme sur un compact si et seulement si $$ n'est pas un polynôme. La démonstration de Pinkus est détaillée en annexe car elle est très différente de celle que nous allons construire dans la suite. Au delà de la référence mentionnée ci-dessus de Pinkus , la question de l'approximation universelle d'une fonction par un réseau dense a été très largement étudiée. On retiendra par exemple les éléments suivants : Pour un nombre de couches donn\'ees, un nombre de neurones par couche $m$ potentiellement très grand ($m$) : l'approximation universelle est \'etablie pour toute activation continue qui ne soit pas un polyn\^ome (condition n\'ecessaire et suffisante, travaux de Cybenko et Hornik, 1989 , voir aussi mentionné ci-dessus) ; Pour un nombre de couches qui tend vers l'infini mais $n+1$ neurones par couche : l'approximation universelle est \'etablie avec l'activation pour toute fonction $f$ Lebesgue int\'egrable (Lu et al. 2017 ; le nombre de neurones par couche peut \^etre diminu\'e si $f$ est continue) ;\\ Pour un nombre de couches qui tend vers l'infini mais $n+3$ neurones par couche : l'approximation universelle est \'etablie avec une activation non affine pour toute fonction $f$ continue ; Pour un cadre r\'ealiste, avec un nombre de couches et de neurones par couche fix\'es : Ismailov et Guliyev ont construit une fonction d'activation, du type sigmo\" de (donc croissante de 0 \`a 1 et $^$) qui permet d'atteindre l'approximation universelle par un r\'eseau \`a 3 couches et $2n+2$ neurones par couche si $n 2$ (et 2 couches \`a 2 neurones si $n=1$). La preuve est bas\'ee sur le th\'eor\`eme de superposition de Kolmogorov ; l'activation est construite algorithmiquement. Pour l'approximation d'une classe beaucoup plus large de fonctions avec des activations dans des r\'eseaux de taille contr\^ol\'ee, on pourra consulter , ou encore (toujours une taille contr\^ol\'ee, mais une activation beaucoup plus complexe) . Dans cette section, nous abordons la question de l'approximation des fonctions continues par des réseaux de neurones à couche hypersphérique, en examinant une approche différente de celle discutée précédemment. \\ Nous nous appuyons sur le théorème de Schwartz , qui établit que des fonctions continues à support compact peuvent être approximées de manière uniforme par des combinaisons linéaires de translatées d'une fonction donnée. Cette idée est particulièrement pertinente dans le contexte des réseaux de neurones, car les sorties peuvent être assimilées à une combinaison de fonctions translatées. En ce sens, l'application de ce théorème permet de montrer que ces réseaux peuvent effectivement approximer des fonctions continues sur des compacts de $$, à travers des combinaisons linéaires de translatées de fonctions spécifiques. De façon analogue à l'équation , la sortie du réseau à couche hypersphérique à $J$ neurones (donc $J$ sphères en dimension $n$) peut être réécrite comme~:\\ y() = _{j=1}^{J} _{j}{2} _{i=1}^{n} )}_{(-{2}\|-\|^2_{}+{2}_j^2)} \\ où $_j $, $_j ^n$, $_j $, $1 j J$.\\ On choisit de construire une approche plus directe que la stratégie utilisée pour le réseau à couche dense du paragraphe précédent. Le prix à payer est de se restreindre au cas des fonctions à une variable ($n=1$).\\ Il s'agit de montrer si un réseau composé d'une couche cachée hypersphérique permet l'approximation de fonctions de \( \) vers \( \). La sortie du réseau à couche hypersphérique dans le contexte où l'on considère les fonctions scalaires s'écrit, avec une entrée à une dimension, de la façon suivante~: y(x) = _{j=1}^{J} _{j} ( -{2} ) avec $_j $, les centres $c_j $, les rayons $_j $, $1 j J$.\\ L'architecture du réseau de neurones considéré ($J$ sphères en dimension 1) peut se résumer par le schéma . [H] [shorten >=1pt,->,draw=black!50, node distance=2.5cm] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!120]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] (K-1) at (-2,-1) {$x_1=:x$}; (K-3) at (-2,-3+1/2) {$1$}; (K-4) at (-2,-4+1/2) {${2}$}; / in {1,...,5} (H-) at (0,-+1) { $_{} _i$}; in {1} in {1,...,5} (K-) edge (H-); in {3,4} in {1,...,5} (K-) edge [dashed,red] (H-); (J) at (4,-2) { $y_i = _{j=1}^5 _j(_j._i_i)$}; in {1,...,5} (H-) edge (J); La question de densité dans le cas considéré consiste à montrer qu'une combinaison linéaire de sorties de neurones d'un couche hypersphérique peut approcher l'ensemble des fonctions continues à support compact au sens de la convergence uniforme. Autrement dit, l'ensemble $ \{(-{2});c, ^{*}\}$ est-il dense dans l'ensemble des fonctions continues à support compact, soit \{(-{2});c, ^{*}\}}= ^0() , pour toute activation $ ^0()$, au sens de la norme uniforme sur les compacts ? On commence par énoncer le théorème suivant, dû à Schwartz {Schwartz}~:\\ [colframe=blue, colback=white!10, title=] {Schwartz} Si $$ et $$ sont deux fonctions continues à supports compacts, $ 0$, alors $$ est limite uniforme sur tout compact de combinaisons linéaires des translatées de $$. Un énoncé équivalent, où on reconnaît déjà la structure de la sortie d'un réseau de neurones, est le suivant. Soit \( \) un compact de \( \). Il existe donc des couples \( (a_j, _j) ^2 \) pour \( j \), tels que~: _{J + } _{x K} (x) -^{J}_{j=1} _j (x-a_j)=0 . \\ Soit $:=\{(x-a), a\}$. Chaque élément de la somme dans appartenant à $$, la limite de cette somme pour $J +$ est dans l'adhérence $}$ de $$. Cette limite $$ étant par ailleurs dans $C^0_c()$ ^0_c()$ l'ensemble des fonctions continues à support compacts}, on déduit du théorème de Schwartz que $$C^0_c() }$$ Tout élément de $$ étant une fonction continue à support compact alors $ C^0_c()$. Il suit que $} C^0_c()$. Cette inclusion réciproque conduit à \{(x-a); a\}}= ^0_c() Pour établir un théorème d'approximation dans notre cas spécifique, on va appliquer cette propriété à deux cas distincts~: Les rayons des sphères sont fixés et égaux. Les rayons des sphères ne sont pas fixés (cas général). On commence par le cas des rayons fixés. Pour $ $ donné, on introduit la fonction $_{}$ définie sur $$ par~: $$_{}(x)= (-{2}).$$ Une sortie hypersph\'erique centr\'ee en $c $ s'\'ecrit comme une translat\'ee de $_$ : $$_{}(x-c) = (-{2})$$ On suppose que $^{0}_{c}()$ et $ 0$, le Th\'eor\`eme 1 s'applique et assure l'existence de $(c_j,_j) ^2$, $j $, tels que :\\ $$ _{J} _{x K} (x) -^{J}_{j=1} _j (-{2}) =0 $$ ce qui permet de conclure en reprenant un raisonnement analogue à l'obtention de l'équation que~: pour tout $ $ donné _}= ^0_c() où $_ = \{(-{2});c\}$. [{ Lien avec les fonctions à bases radiales (RBF)}] Ici on peut faire le lien entre un réseau à couche hypersphérique (RHS) et un réseau de fonctions de base radiales (RBFN). La sortie d'un RBFN s'exprime comme : y() = _{j}^J _j (_j \| - _j\|) La fonction $$ est dite à base radiale car elle ne dépend que de la distance euclidienne entre $$ et $$ dans $^n$. Pour rapprocher les sorties des deux types de réseaux, les sphères doivent être dégénérées (\( _j = 0 \)) en points. L'activation qui suit la couche hypersphérique joue un rôle analogue à celui du noyau dans les fonctions à base radiale. La sortie d'un RHS pour l'activation $ (x) = x $ (resp. $ (x) = e^x $) est par exemple proportionnelle à la sortie d'un RBFN en utilisant le noyau quadratique $ (x) = x^2 $ (resp. le noyau $ (x) = e^{-{2}x^2} $). { En prenant $=0$ dans le résultat précédent, on re-prouve donc le fait que le RBFN est également un approximateur universel en dimension 1 (conformément à ).} Passons au cas g\'enéral où les rayons ne sont pas fixés. Soit $$ une fonction continue à support compact. D'après ce qui précède en choisissant $ =0$ dans , elle peut être utilisée comme fonction d'activation pour approcher $f$ sous la forme $$ f(x) _{j=1}^{J_1} _j ( (x-c_j)^2 ) $$ pour certains $J_1 $, centres $c_j $ et coefficients $_j $, $1 j J_1$. Par ailleurs, $$ peut aussi être approchée { via} des translat\'ees de l'activation $$ grâce au théorème de Schwartz ( On pose $X_j = (x-c_j)^2 $): $$ (X_j) _{j'=1}^{J_2} _{j'} ( -{2} (X_j - _{j'}^2) )$$ pour certains $J_2 $, rayons $_{j'} $ et coefficients $_{j'} $, $1 j' J_2$. En assemblant ces deux dernières approximations, on obtient $$ f(x) _{j=1}^{J_1} _j _{k=1}^{J_2} _k ( -{2} ((x-c_j)^2 - _k^2) ) $$ qui peut se ré-écrire plus simplement $$ f(x) _{j=1}^{J} _j ( -{2} ((x-c_j)^2 - _j^2) ) .$$ Finalement on a donc établi le théorème suivant~: \\ [colframe=blue, colback=white!10, title=] Soit $ _c^{0}()$ une fonction d'activation continue à support compact non identiquement nulle. Alors, $_ = \{(-{2});c\}$ pour tout $ $, et $_{c} = \{(-{2});c, \, \}$ sont denses dans $^{0}()$ au sens de la convergence uniforme sur un compact. Le théorème précédent a été démontré pour une activation $ C^0_c() $. Mais cette hypothèse peut être relaxée. Le résultat se généralise par exemple au cas où $$ L^1()$$ pour peu qu'on remplace la norme uniforme par la norme $L^1$. En effet, pour le prouver, il suffit d'utiliser le fait que l'espace de Lebesgue $L^1$ est par définition l'adhérence de l'ensemble des fonctions continues à support compact pour la norme $L^1$ et le résultat du théorème précédent. Les expérimentations menées n'ont pas pour but d'offrir une étude exhaustive des capacités d'approximation des réseaux de neurones, mais plutôt de valider empiriquement les résultats théoriques établis précédemment. Dans cette optique, nous présentons ci-dessous les résultats obtenus pour différentes fonctions tests. Afin de vérifier expérimentalement la conclusion établie dans la partie précédente (Théorème ), l'approximation de fonctions continues à support compact à une variable a été testée. Pour cela, des réseaux de \( J \) neurones sur une couche cachée de type hypersphérique ou dense ont été utilisés. Pour chaque réseau, on a testé des valeurs de $J$ de 2 à 64. Comme fonction de perte, l'erreur quadratique moyenne (EQM) a été utilisée, et le jeu de données (composé de 512 points) a été séparé selon les proportions suivantes : \( {2} \) pour l'entraînement, \( {4} \) pour la validation et \( {4} \) pour le test. Les données ont été échantillonnées sur l'intervalle \( [-0.05, 1.05] \). L'erreur finale a été évaluée sur le jeu de test.\\ Dans les tests numériques qui vont suivre, différentes fonctions d'activation ont été testées puis sélectionnées et appliquées aux neurones de la couche cachée (cf. Fig )~: (sans activation)} $$ ~: & \\ x & x $$ (Rectified Linear Unit)} $$ ~: & [0, + \\ x & (0, (1, {5} + {2})) $$ (fonction de base radiale)} $$ ~: & ]0,1] \\ x & e^{-x^2} $$ )} $$ ~: & ]-1,1[ \\ x & }{e^x + e^{-x}} $$ } $$ ~: & ]0,2[ \\ x & (x) + 1 $$ } $$ ~: & [-1,1] \\ x & (x) $$ _{x} \)} $$ ~: & \\ x & ( ._i }{}) e^{-( ._i }{})^2}, . $$ La figure montre l'évaluation des fonctions d'activation sur le produit interne $._i$, en considérant une sphère $$ de centre 0 et de rayon 1 et en faisant varier $x$ sur l'intervalle $[-,]$. En pointillé, sont représentées les fonctions d'activations usuelles. La courbe verte correspondant à l'activation "linear" montre le produit interne $._i$, c'est-à-dire la sortie du neurone sans activation. [htpb] $} Les preuves précédentes ont été construites en commençant par utiliser des fonctions d'activation à support compact. Dans le but de se rapprocher au plus près des conditions, l'attention a été portée sur les fonctions \( L^2 \) qui s'annulent au bord d'un compact sur lequel on souhaite approcher une fonction. C'est pourquoi la fonction \( (x) := (x) + 1 \) a été ajoutée. De même, la fonction \( _{xd} \) a été introduite, possédant la propriété de conserver le signe de \( ._i \). Enfin, la fonction sinus a été incluse comme non-linéarité, car elle permet de modéliser efficacement des signaux complexes . Dans un premier temps, l'objectif a été d'approcher deux fonctions simples construites pour être à support compact : la fonction Triangle, facilement approchable par une couche dense, et la fonction Omega, correspondant à l'activation d'une couche à un neurone hypersphérique. Dans un deuxième temps, deux fonctions plus complexes ont été étudiées : la fonction \( \), qui est la combinaison linéaire (+ ) de 32 sphères aléatoires, et la fonction \( ({x}) \), qui oscille fortement autour de 0. Chaque fonction à approcher a été mise à 0 en dehors de l'intervalle \( K=[0, 1] \). La figure illustre les fonctions considérées.\\ [H] \\ La figure montre l'approximation des fonctions Triangle et Omega. La légende indique qu'il est nécessaire de monter à 64 neurones pour bien approximer également pour les intervalles $[-0.05. 0]$ et $[1, 1.05]$, c'est-à-dire en dehors du compact $K=[0,1]$. [htbp] Il est à noter que le modèle de neurones à couche hypersphérique permet de réduire les erreurs au bord, mais présente quelques oscillations sur les parties purement linéaires de la fonction. Cependant, avec la fonction d'activation \( _{x} \), une EQM de \( 8 10^{-6} \) est obtenue, inférieure à l'EQM maximale obtenue dans le cas des réseaux à couche dense, qui est égale à \( 2 10^{-5} \). \\ La difficulté d'entraîner les modèles à couche hypersphérique est également notable. En effet, pour la fonction Omega, construite à partir du produit interne avec une hypersphère suivi d'une activation , on pourrait s'attendre à ce qu'un seul neurone suffise pour approximativement retrouver les paramètres de l'hypersphère. Cependant, cela n'est pas le cas, et il faut au moins deux neurones (deux hypersphères) pour obtenir une approximation efficace.\\ Les graphiques et représente les EQM, en fonction du nombres de neurones pour les fonctions plus complexes. Les courbes continues représentent les résultats obtenus avec les modèles de neurones à couche hypersphérique, tandis que les courbes en pointillés représentent les résultats obtenus avec les modèles à couche dense.\\ [H] [H] Les EQM restent dans les mêmes ordres de grandeur, quel que soit le modèle utilisé. On peut observer que pour l'approximation de $Sph32$, l'utilisation de la fonction d'activation $sinus$ permet d'obtenir de bons résultats avec peu de neurones. Cependant, que ce soit pour les modèles à couche dense ou hypersphérique, en augmentant suffisamment le nombre de neurones, l'utilisation de la fonction d'activation $Gauss_{x}$ a permis de réduire les erreurs obtenues pour l'ensemble des fonctions que nous avons approximées.\\ L'approximation de fonctions a également été testée en fixant le rayon \( \) des sphères. Aucun changement significatif n'a été noté sur les résultats obtenus, les EQM restant toujours dans les mêmes ordres de grandeur.\\ Dans ce chapitre, on a choisi de faire un focus sur l'approximation de fonctions scalaires. On sait en effet (voir par exemple le Th. 3.2 dans Pinkus ) qu'un résultat d'approximation des fonctions de $$ dans $$ peut être étendu aux fonctions de $^n$ dans $^{n'}$ avec $n,n' $ quelconques. \\ Les résultats expérimentaux permettent de confirmer le résultat théorique suivant : un réseau de neurones à une couche hypersphérique est capable d'approcher en norme uniforme sur tout compact toute fonction continue de \( \) à support compact. Une nouvelle fonction d'activation \( }_{x} \) a été proposée. Il a également été vérifié que l'approximation est possible, malgré le fait que les fonctions et ne soient pas à support compact (mais \( L^2 \)), ce qui correspond au résultat théorique du paragraphe (on rappelle que la convergence en norme $L^1$ implique la convergence presque partout).\\ Les expériences menées comparent aussi les performances des approximations obtenues avec des couches hypersphériques et des couches denses classiques. \\ Par rapport à la démonstration présentée dans l’article , nous avons proposé une approche simplifiée en allégeant certains aspects techniques de la preuve initiale. Les résultats théoriques comme les expérimentations ont permis d'illustrer les qualités d'approximation universelle des réseaux hypersphériques.