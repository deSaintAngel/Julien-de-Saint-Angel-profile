Cette annexe détaille un certain nombre de calculs (notamment de covariance) qui nous permis de justifier notre proposition d'initialisation pour les couches hypersphériques. (2 x_i s_i - x_i^2 - s_{ji}^2) = (2 x_i s_{ji} - x_i^2) + (s_{ji}^2) + 2 (2 x_i s_{ji} - x_i^2, s_{ji}^2) Or, (2x_i s_{ij} - x_i^2, -s_{ij}^2) &= [(2x_i s_{ij} - x_i^2)(-s_{ij}^2)] - [2x_i s_{ij} - x_i^2] [-s_{ij}^2] \\ \\ &= -[2x_i s_{ij}^3] + [x_i^2 s_{ij}^2] + [2x_i s_{ij}] [s_{ij}^2] - [x_i^2] [s_{ij}^2] \\ \\ &= -2[x_i] [s_{ij}^3] + [x_i^2] [s_{ij}^2] + 2[s_{ij}] [x_i] [s_{ij}^2] - [x_i^2] [s_{ij}^2] \\ \\ &= -2[x_i] [s_{ij}^3] + 2[s_{ij}] [x_i] [s_{ij}^2] \\ \\ &= 2_s _x (_s^2 + _s^2) - 2 _x (_s^3 + 3 _s _s^2) \\ \\ &= -4 _s _x _s^2 (2 x_i s_{ji} - x_i^2) = (2 x_i s_{ji}) + (-x_i^2) + 2 (2 x_i s_{ji}, x_i^2) Or, (2 x_i s_{ji}, -x_i^2) &= [(2 x_i s_{ji})(-x_i^2)] - [2 x_i s_{ji}] [-x_i^2] \\ \\ &= -2[s_{ji}] [x_i^3] + 2 [x_i][s_{ji}] [x_i^2] \\ \\ &= 2_s _x (_x^2 + _x^2) - 2 _s (_x^3 + 3 _s _x^2) \\ \\ &= -4 _s _x _x^2 Donc, (2 x_i s_{ji} - x_i^2 - s_{ji}^2) = (2 x_i s_{ji}) + (-x_i^2) + (s_{ji}^2) - 8 _s _x (_x^2 + _s^2 ) Les calculs précédents ont pris en compte uniquement des couches hypersphériques de type dense. Cependant, la plupart des réseaux sont constitués de couches convolutionnelles. Chaque élément du tenseur de sortie d'un neurone correspond au produit scalaire entre la version vectorisée de la sphère et une partie du tenseur d'entrée de taille identique au noyau de convolution. Il est donc possible de considérer des vecteurs constitués des parties vectorisées par blocs de l'entrée. En conséquence, pour une sphère donnée, contrairement au cas des couches de type dense où la sortie est un scalaire, on obtient un vecteur dont chaque élément provient du produit scalaire entre la sphère et les sous-vecteurs correspondants. \\ Pour les calculs suivants, on considère une couche convolutive où le tenseur d'entrée est de taille $(c,n)$ (un vecteur à $c$ canaux), la sortie est un tenseur de taille $(m,d')$ où $m$ représente le nombre de sphères, et $d'$ le nombre de composantes issues du produit de convolution (on a $d'= n-d-1$, dans le cas d'une convolution sans padding, et $d'= n$ avec padding). Les éléments de la sortie sont donc les $y_{i'j}$ définis par $y_{i'j} = } _i }{}$.\\ Pour le produit entre un vecteur de composantes $}^{}$ et une sphère $}$ qui donne l'élément $y_{i'j}$ de la convolution, avec $d$ la taille du filtre tel que $i\{1,,d\}$ et $d'$ le nombre de composantes tel que $i'\{1,,d'\}, $ on obtient pour une sphère $j$ donnée l'écriture de la $i'$-ième composante ~:\\ $${ y_{i'j} = {2}}$$ L'élément $x_{i'ci}$ représente le $i$-ème élément pour le canal $c$ de la composante $}^{}$. On considère pour l'écriture suivante les composantes $p$ et $q$ pour la sphère $}$. Pour alléger la notation l'indice $j$ n'est pas mentionné. Le terme en $^2$ est intégré dans la double somme.\\ $$ y_p = _i = _{c=1}^{C}_{i=1}^{d} , p \{1,,d'\}$$\\ $$ y_q = _i = _{c=1}^{C}_{i=1}^{d} , q \{1,,d'\}$$ On calcule alors : \\ { ${ll} & (y_p,y_q) = cov( , ) p<q = q-p ] \\ \\ & =( _{c=1}^{C} _{i=1}^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd}, _{c'=1}^{C} _{i"=1}^{d}{{x_{qi"c'}}} {{s_{i"}}}-{2}{{x^{2}_{qi"c'}}}-{2}{{s^{2}_{i"c'}}}+}{2Cd})\\ \\ & =_{c=1}^{C}(_{i=1}^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{c'=1}^{C}_{i"=1}^{d}{{x_{qi"c'}}} {{s_{i"c'}}}-{2}{{x^{2}_{qi"c'}}}-{2}{{s^{2}_{i"c'}}}+}{2Cd})\\ \\ \\ & =_{c=1}^{C} ^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{i"=1}^{d}{{x_{qi"c}}} {{s_{i"c}}}-{2}{{x^{2}_{qi"c}}}-{2}{{s^{2}_{i"c}}}+}{2Cd})}_{ c=c'}\\ \\ & ++_{c=1}^{C} ^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{i"=1}^{d}{{x_{qi"c'}}} {{s_{i"c'}}}-{2}{{x^{2}_{qi"c'}}}-{2}{{s^{2}_{i"c'}}}+}{2Cd})}_{ c c'} $}\\ On peut restreindre le calcul à la condition $c=c'$. En effet, on cherche à calculer la somme des covariances de termes independants deux à deux. Cela implique que si on développe les termes de la somme pour $c c"$, alors ils sont tous nuls. Donc pour $c=c'$ fixé on a~: \\ $$ (y_p,y_q) = _{c=1}^{C} (_{i=1}^{d}{{x_{pi}}} {{s_{i}}}-{2}{{x^{2}_{pi}}}-{2}{{s^{2}_{i}}}+}{2Cd},_{i=1}^{d}{{x_{qi}}} {{s_{i}}}-{2}{{x^{2}_{qi}}}-{2}{{s^{2}_{i}}}+}{2Cd})}_{cov(_i ,_i )_c}$$\\ Ainsi le calcul du terme cov$(y_p,y_q)_c$ donne~:\\ ${ll} (y_p,y_q)_c & = (_i ,_i )_c\\ \\ & = \\ \\ \\ & = \\ $\\ ${ll} (y_p,y_q)_c & = ^{ d}(x_{pi} s_i-{2}x^{2}_{pi}) ,_{i=1}^{d}(x_{qi} s_i-{2}x^{2}_{ql})]}_{(1)}\\ \\ & + }_{(2)}\\ \\ \\ & }_{(3)}+ }_{(4)} $\\ ${rl} (4) : & s_i ^2 \\ & _s, s_{i} (_s,^{2}_{s}), ~:\\ \\ &{ccl} var(s^{2}_{i}) = -^{2} = 2 _s^2 (_s^2 + 2 _s^2 ) $\\ ${rl} & \\ \\ & {lll} & = & \\ \\ & = & (_{i=1}^{ d}-{2}s^{2}_{i})+ ({2cd})}_{0}+(_{i=1}^{ d}-{2}s^{2}_{i},{2cd})}_{0}\\ & = & {4}(s^{2}_{i}) s_i \\ \\ & = & {2} _s^2 (_s^2 + 2 _s^2 ) $\\ Les termes (2) et (3) étant symétriques, leur calcul est identique.\\ ${lll} & = & _{i=1}^{d}_{i=1}^{d} \\ $ ${ll} = & _{i=1}^{d}_{i=1}^{d} $\\ {$ et les $s_i$, pour tout $i \{1, , d\}$, sont indépendants alors on a l'égalité [x^{2}_{pi}s^{2}_{i}]=[x^{2}_{pi}][s^{2}_{i}]$} qui permet d’établir que la covariance du terme donné est nulle.\\ }} ${ll} (*) & _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,-{2}s^{2}_{i})=_{i=1}^{d} $\\ $$ - = - $$\\ Ainsi, $$ =-d_s _x _x^2$$\\ En reprenant le terme (1), on a~:\\ $${ll} & \\ \\ = & _{i=1}^{d}_{i=1}^{d} \\ $$\\ {ll} = & _{i=1}^{d}_{i=1}^{d}\\ \\ Le terme est donc décomposé en 4 termes que l'on calcule~: \\ $${ll} (B),(C) & _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,-{2}x^{2}_{qi})= _{i=1}^{ d}_{i=1}^{ d}(-{2}x^{2}_{pi},x_{qi} s_i)= 0.\\ \\ \\ & i \{1,, d\} \ :\\ & (-{2} x_{pi} s_{i} x^{2}_{qi})-(x_{pi} s_{i})(-{2}x^{2}_{qi})\\ \\ &= (s_{i})(-{2}x_{pi}x^{2}_{qi})-(x_{pi})(s_{i}) (-{2}x^{2}_{qi})\\ \\ &= -{2}(s_{i})(x_{pi} ) (x^{2}_{qi})+{2}(x_{pi})(s_{i}) (x^{2}_{qi})\\ \\ & = 0\\ $$\\ Pour tout $i \{1, , d\}$, les $s_{i}$ sont indépendants des $x_{p_i}$ et des $x_{q_i}$, ce qui implique que l'espérance des produits entre ces termes est égale au produit des espérances. Cela justifie que les termes $(B)$ et $(C)$ sont nuls.\\ Donc $$ (x_{pi} s_i,-{2}x^{2}_{qi}) =0 (x_{qi} s_i,-{2}x^{2}_{pi}) =0 $$\\ $(A)$ $$ les vecteurs $X_{p}$ et $X_{q}$, pour $p q$, correspondent à une partie de la forme vectorisée du vecteur d'entrée $X$, dont la taille est identique à la forme vectorisée du noyau de convolution. Ainsi les éléments des deux vecteurs $X_p$ et $X_q$ ont respectivement pour indices $p+k$ $q+k$ avec $k\{0,,d-1\}$. En posant $k' = k + 1$, il en résulte que~: $$X_p=\{x_{pi}\}_{i\{1,,d\}}=\{x_{p+k'-1}\}_{k'\{1,,d\}}$$ et $$X_q=\{x_{qi}\}_{i\{1,,d\}}=\{x_{q+k'-1}\}_{k'\{1,,d\}}$$\\ La double somme du terme A peut s'écrire de la façon suivante~:\\ $$_{k'=1}^{d}_{k'=1}^{d}(x_{p+k'-1} s_{k'},x_{{q}+k'-1} s_{k'})=_{k'=1}^{d}_{k'=1}^{d}(x_{p+k'-1} s_{k'},x_{{p+} +k'-1} s_{k'}) $$\\ ou bien encore~:\\ $$_{k'=1}^{d}_{k''=1}^{d}(x_{{p}+k'-1} s_{k'},x_{{p+} +k''-1} s_{k''}) $$\\ En réarangeant les indices en fonction de $k'$ et $k''$, on obtient~:\\ $$_{k'={p}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1}) $$. \\ Pour continuer le calcul, il faut émetre l'hypothèse que $ d$ et est non nul. En effet dans le cas contraire, pour tout $k'\{p,,p+d-1\}$ et tout $k''\{p+,,p++d-1\}$, les $x_{k'}$, $x_{k''}$, $s_{k'-p+1}$ et $s_{k''-p-+1}$ sont tous distincts, et on peut donc justifier le passage des espérances des produits au produit des espérances de chaque terme et observer que la covariance des termes calculés est nulle. En effet, les $x_{k'}$ et $x_{k''}$ sont indépendants entre eux, et indépendants des $s_{k'-p+1}$ et $s_{k''-p+1}$. En développant les termes de la covariance, il apparaît que l'expression dépend uniquement des espérances des produits de ces termes.\\ $${lll} (x_{k'} s_{k'-p+1}, x_{k''} s_{k''-p-+1}) & = & \\ \\ & & - \\ \\ \\ & = & [x_{k'}] [x_{k''}] [s_{k'-p+1} s_{k''-p-+1}]\\ \\ & & - [x_{k'}] [s_{k'-p+1}] [x_{k''}] [s_{k''-p-+1}]\\ \\ \\ & = & [x_{k'}] [x_{k''}] [s_{k'-p+1}] [s_{k''-p-+1}]\\ \\ & & - [x_{k'}] [s_{k'-p+1}] [x_{k''}] [s_{k''-p-+1}]\\ $$\\ Tous les produits d'espérance de ces termes se simplifient et donc sont nuls. Ainsi, chaque terme de la somme est nul, ce qui implique que la somme est aussi nulle.\\ Par linéarité de la covariance, on peut séparer la double somme selon les termes suivants~: $$ {p}}^{{p+-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})}_{=0}+ _{k'={p+}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1}) $$ $ et tout $k''\{p+,,p+d-1\}$, les $x_{k'}$ et $x_{k''}$ sont tous distincts. Un raisonnement analogue à l'étape précedente, permet de conclure que chaque terme de la somme est nul, donc la somme est nulle.} $$=_{k'=p+}^{p+d-1}_{k''={p+}}^{{p+d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})+ ^{p+d-1}_{k''={p+d}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})}_{=0} $$ {$ et tout $k''\{p+,,p++d-1\}$, les $x_{k'}$ et $x_{k''}$ sont tous distincts. Ainsi chaque terme de la somme est nul, donc la somme totale est nulle. }} Dans la double somme, les termes où $k' k''$ vont donner des $x_{k'}$ et $x_{k''}$ distincts, ce qui permet également de conclure que les termes de covariance calculés sont nuls. Il ne reste donc que les termes où $k'=k''$. Ce qui permet de simplifier la double somme sou la forme suivante~:\\ $$_{k'=p+}^{p+d-1}(x_{k'} s_{k'-p+1},x_{k'} s_{k'-p-+1})$$\\ Pour chaque terme de cette dernière somme le calcul de la covariance donne~:\\ $${lll} (x_{k'} s_{k'-p+1},x_{k'} s_{k'-p-+1})& = & [x_{k'}^2] [s_{k'-p+1}] [s_{k'-p-+1}]\\ \\ & & - [s_{k'-p+1}] \\ \\ & = & ({_x^2 + _x^2}) _s^2 - _x^2 _s^2 $$ $$ _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,x_{qi} s_i)=(d-) _s^2 _x^2$$\\ Pour le calcul du terme $D$, le raisonnement sur les indices de sommation est similaire à ce qui a été effectué pour le calcul du terme $A$. \\ Comme énoncé précédemment, les éléments des deux vecteurs $X_p$ et $X_q$ ont respectivement pour indices $p+k$ et $q+k$ avec $k\{0,,d-1\}$. On fait l'hypthèse que $ d$, et on a $=q-p$. On pose $k' = k + 1$, il en résulte que~: $$X_p=\{x_{pi}\}_{i\{1,,d\}}=\{x_{p+k'-1}\}_{k'\{1,,d\}}$$ et $$X_q=\{x_{qi}\}_{i\{1,,d\}}=\{x_{q+k'-1}\}_{k'\{1,,d\}}$$. ${ll} (D) & = _{i=1}^{d}_{i=1}^{d}(-{2}x^{2}_{pi},-{2}x^{2}_{qi})\\ \\ & = _{k'=1}^{d}_{k''=1}^{d}(-{2}x^{2}_{p+k'-1},-{2}x^{2}_{q+k''-1}) \\ \\ & = _{k'=1}^{d}_{k''=1}^{d}(-{2}x^{2}_{p+k'-1},-{2}x^{2}_{p++k''-1}) \\ \\ & = _{k'={p}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}} (-{2}x^{2}_{k'},-{2}x^{2}_{k''})\\ \\ & = {p}}^{{p+-1}}_{k''={p+}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})}_{=0}+ _{k'={p+}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})\\ \\ & = _{k'=p+}^{p+d-1}_{k''={p+}}^{{p+d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})+ ^{p+d-1}_{k''={p+d}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})}_{=0}\\ $ $$ {ll} (D) & = _{k'=p+}^{p+d-1} (-{2}x^{2}_{k'}, -{2}x^{2}_{k'}) x_{pi} (0,^{2}_{x})\\ \\ & = _{i=1+}^{d} (x_{k'}^2)}{4} = \{ {l} {2} ^{2}_{x} (^{2}_{x} + 2 _x^2 ) (=q-p) \{1, , d\}, \\ 0 . $$\\ {_{k'}$ et $x^{2}_{k''}$ distincts. Grâce à l'hypothèse d'indépendance on a [x^{2}_{k'}x^{2}_{k''}]=[x^{2}_{k'}][x^{2}_{k''}]$}. Cela permet de conclure que les termes de covariance calculés sont nuls. Il ne reste donc que les termes où $k'=k''$.}} Reprenons , enfin, le calcul . Cette covariance correspond à la somme des termes suivants~: $$\{ {cl} (1) : & (A = {(d-) _s^2 _x^2 }) + (B = {0} ) + (C = {0}) \\ \\ & +(D = {{2} ^{2}_{x} (^{2}_{x} + 2 _x^2 )} \{1, , d\} \\ \\ (2) : & {-d _x _s _x^2} \\ \\ (3) : & {-d _x _s _x^2}\\ \\ (4) : & {{2} _s^2 (^{2}_{s} + 2_s^2)} . $$\\ soit donc~: $${ll} (y_p,y_q) &= \{{l} _{c=1}^{C}{2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 ) \{ 1,,d\}. \\ \\ _{c=1}^{C}{2}_s^2 (_s^2 + 2 _s^2 -2 _x _s ) .\\ \\ &= \{{l} C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 ) ) \{1,,d\}. \\ \\ C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )) .\\ $$\\ Pour le cas où $^t$ est un tenseur de dimension $(m,n,c)$, c'est-à-dire une image à $c$ canaux, nommons $$ et $$ les indices de colonnes des éléments du tenseur $Y$ de sortie. On reprend $=q-p$ et on pose $ = -$. On peut rappeler que $d'$ représente le nombre de composantes dans la direction $i'$ telle que $i'\{1,,d'\}$. Si l'on considère un filtre de convolution de taille $d d$ alors, $d''$ représente le nombre de composantes dans la direction $i''$ telle que $i''\{1,,d''\}$. Le tenseur $Y$ de sortie est donc constitué des éléments $\{y_{i'i''j}\}$. On suppose $ $. On considère donc pour une sphère $j$ fixée les composantes $y_{p}$ et $y_{q}$ telles que $ \{1,,d''\}$ et $ \{1,,d''\}$. Le nombre total de d'éléments pour la partie vectorisée d'une composante donnée sera alors $d'd''$. On a donc~:\\ $${ll} (y_{p},y_{q}) & {l} = \{{l} C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 )) \\ , \{ 0,,d\}. \\ \\ C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )) . \\ $$\\ On a finalement obtenu la matrice de covariance, dont la largeur de la bande dépendant du facteur $({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s ))$ correspond à la dimension du noyau de convolution~: { $ (y_j) & _1+ _2 & _1 & & _1 \\ \\ _1 + _2 & (y_j) & _1 + _2 & & \\ \\ _1 & _1 + _2 & (y_j) & & _1 \\ \\ & & & & _1 + _2 \\ \\ _1 & & _1 & _1 + _2 & (y_j) $ } avec,\\ { \{ {l} (y_j) = {2 ^2} (_s^2 + _x^2) ( 2(_x^2 - _s^2)^2 + 2_s_x + _s^2 + _x^2 ) \\ \\ _1 = C({2}_s^2 (_s^2 + 2 _s^2 - 2 _x _s ) ) \\ \\ _2 = C({2} _x^2 (_x^2 + 2 _x^2 + 2_s^2 ) ) . } ${ll} \\ ({ ^{l}_{}} ) & = ( {}(S^{l+1}_{}- ^{l+1}_{}){ ^{l+1}_{}} ) \\ \\ \\ & = ( _{j'=1}^{m'}{}(s^{l+1}_{jj'}- x^{l+1}_{j}){ z^{l+1}_{j'}} )\\ \\ \\ & = m'( {}(s^{l+1}_{jj'}- x^{l+1}_{j}){ z^{l+1}_{j'}} )\\ \\ \\ & = {^2}\\ \\ \\ & = {^2}\\ $