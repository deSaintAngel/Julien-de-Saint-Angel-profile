	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Réseaux inspirés du formalisme de l'algèbre géométrique conforme}

Ce chapitre est consacré à la construction de réseaux de neurones à couches hypersphériques. 
Leur définition est inspirée du modèle conforme défini par Hestenes et al. \cite{Hestenes2001}.
Les neurones définissent des hypersphères en dimension \(n\), paramétrées par un vecteur de l’algèbre géométrique conforme \(R^{n+1,1}\) (les poids des neurones correspondent aux paramètres de l’hypersphère).
Une approche similaire pour les couches denses a déjà été proposée par Banarer et al. \cite{Banarer2003}.
Ici, nous allons plus loin en introduisant également un modèle convolutif de couches hypersphériques.\\

L’implémentation de ces nouveaux types de couches, en utilisant des opérateurs d’algèbre linéaire ou de convolution classiques, est décrite en détail, ainsi que les contraintes pour la mise à jour des poids.  

Les premières expérimentations sur des données synthétiques et des bases d’images montrent que les variantes hypersphériques des couches denses et convolutives améliorent le comportement de la fonction de coût et permettent une convergence plus rapide, à nombre de paramètres égal. Cependant, ces couches se révèlent parfois plus sensibles à l’initialisation, ce qui peut entraîner une certaine instabilité. 

Nous nous penchons donc sur la question de l'initialisation.
On montre d'abord pourquoi l'approche heuristique de Glorot et Bengio \cite{glorot} et donc la normalisation classique associée s'avèrent inefficaces dans le cadre des couches hypersphériques.
Il faut donc comprendre comment une distribution suivant une loi de probabilité complexe se propage dans le réseau hypersphérique.
Nous mettons alors en place une stratégie d'initialisation basée sur des propriétés asymptotiques des lois de type Gamma généralisées.
La pertinence de la méthode est confirmée par les expérimentations numériques.

\section{Vers des réseaux de neurones à couches hypersphériques}

Les réseaux de neurones "classiques" reposent sur un produit scalaire $\mathbf{x} \cdot \mathbf{w}$ (appliqué localement dans le cas convolutif), où $\mathbf{x}$ représente l'entrée et $\mathbf{w}$ le vecteur de poids\footnote{Pour simplifier, on peut supposer que $\mathbf{x}$ est augmenté pour intégrer le biais dans le vecteur des poids: $\mathbf{w}\cdot \mathbf{x} +b = (\mathbf{w},b)\cdot (\mathbf{x},1)$.} (ou le filtre de convolution). Pour un vecteur $\mathbf{x}$ de taille $n$, le nombre de paramètres de la couche est $n+1$. Une fonction d'activation peut ensuite être ajoutée. Si on utilise une activation "\emph{ReLU}", la sortie de chaque couche partitionne alors $\mathbb{R}^n$ en deux sous-espaces, séparés par un hyperplan (plus généralement, selon le choix de la fonction d'activation, les valeurs positives du produit scalaire sont conservées, tandis que les valeurs négatives sont atténuées ou annulées). \\

L'idée développée dans cette thèse est de faire une partition de $\mathbb{R}^n$ par des hypersphères plutôt que par des hyperplans. On note que la partition induite dans ce cas est faite entre deux sous-espaces dont l’un est compact. \\

Les hypersphères pourraient être paramétrées de manière classique, si bien qu'à l'étape d'apprentissage, l’optimisation des centres et des rayons serait distincts. Cette méthode serait donc similaire à ce qui est classiquement utilisé pour les fonctions à base radiale (réseaux dits RBF). 
Bien que cette approche soit valide, elle occulte le lien géométrique entre hyperplan et hypersphère : un hyperplan peut être considéré comme une hypersphère dont le centre est situé à l'infini et dont le rayon est infini. 
Pour répondre à ce besoin d'unification, nous adoptons dans la suite le formalisme des algèbres géométriques conformes, dans lequel hyperplans comme hypersphères sont représentés par un vecteur de dimension $n+2$. Dans ce cadre mathématique, Banarer et al. \cite{Banarer2003} ont déjà défini un modèle de neurone hypersphérique qui constitue la base de ce travail.
Nous allons l'incorporer dans des réseaux à couches denses ou convolutives. Mais nous commençons par quelques rappels sur le formalisme de l'algèbre géométrique conforme.


\subsection*{Formalisme de l'algèbre géométrique conforme }
\label{sec:formaliste_alc}

On décrit ici des éléments concernant les algèbres géométriques et le modèle conforme. 
Le vocabulaire est mathématique mais, il faut garder à l'esprit que l'idée derrière la mobilisation des outils algébriques qui suivent est de fournir une représentation uniforme et ''simple'' des hyperplans et des hypersphères. 

\subsubsection{Le modèle conforme d'Hestenes}


Nous considérons l'espace euclidien $\mathbb{R}^n$, l'espace vectoriel sous-jacent $\mathbb R^n$ étant muni d'une base orthonormale $(e_1,\dots,e_n)$. La construction suivante explique en particulier par l'utilisation de la projection stéréographique pourquoi le modèle d'espace auquel on aboutit est qualifié de conforme. On commence par plonger l'espace euclidien dans un espace de dimension $n+1$ au moyen de l'inverse d'une projection stéréographique :
\begin{equation*}
{\bf x}=x_1 e_1 + \cdots + x_n e_n \longmapsto {2(x_1 e_1 + \cdots + x_n e_n) \over x_1^2 + \cdots + x_n^2 + 1}+ {x_1^2 + \cdots + x_n^2 - 1 \over x_1^2 + \cdots + x_n^2 + 1} e_+,
\end{equation*}
où $e_+$ désigne le point à l'infini. Ensuite on homogénéise le résultat obtenu en ajoutant le vecteur $e_-$. Ceci donne le plongement de $\mathbb{R}^n$ défini par :
\begin{gather*}
{\bf x}=x_1 e_1 + \cdots + x_n e_n \longmapsto x_1 e_1 + \cdots + x_n e_n+{(x_1^2 + \cdots + x_n^2)\over 2}(e_++e_-)+{(e_--e_+)\over 2}\\
{\bf x}\longmapsto {\bf x}+{(x_1^2 + \cdots + x_n^2)\over 2}(e_++e_-)+{(e_--e_+)\over 2}=\tilde x
\end{gather*}

L'intérêt de ce plongement est qu'il va permettre de considérer les objets de base de la géométrie euclidienne comme des vecteurs. Pour rendre compte de cette idée, il est nécessaire d'introduire un peu de vocabulaire mathématique. Tout d'abord, on considère sur l'espace $\mathbb{R}^{n+1,1}$ muni de la base formée des vecteurs $e_1$,$\ldots$, $e_n$, $e_+$, et $e_-$ la forme quadratique
\begin{equation*}
Q_{n+1,1}=\left(\begin{array}{ccccc}1 & 0 & \cdots & 0 & 0 \\0 & 1 & \cdots & 0 & 0 \\\vdots & \vdots & \ddots & \vdots & \vdots \\0 & 0 & \cdots & 1 & 0 \\0 & 0 & \cdots & 0 & -1\end{array}\right).
\end{equation*}

Au couple $(\mathbb{R}^{n+1,1}, Q_{n+1,1})$, on associe l'algèbre géométrique conforme qui, formellement, est le quotient de l'algèbre tensorielle de $\mathbb{R}^{n+1,1}$ par l'idéal bilatère engendré par les éléments du type $a\otimes a-Q_{n+1,1}(a)$. On dispose alors d'un produit, dit géométrique, tel que pour tout vecteur $a$ de l'algèbre géométrique conforme, on a
\begin{equation*}
aa=a^2=Q_{n+1,1}(a).
\end{equation*}
Plus généralement, le produit géométrique de deux vecteurs $a$ et $b$ s'écrit
\begin{gather*}
	ab = \underbrace{\frac{1}{2}(ab + ba)}_{\text{symétrique}}+ \underbrace{\frac{1}{2}(ab - ba)}_{\text{anti-symétrique}}
\end{gather*}
avec
\begin{gather*}
{\frac{1}{2}(ab + ba)}=a._i b=b._i a= B_{n+1,1}(a,b)
\end{gather*}
$B_{n+1,1}$ étant la forme bilinéaire symétrique associée à $Q_{n+1,1}$, et
\begin{gather*}
{\frac{1}{2}(ab - ba)}=a\wedge b=-b\wedge a.
\end{gather*}
Traditionnellement, le produit $a._i b$ est appelé produit interne, il s'agit d'un réel. Le produit extérieur $a\wedge b$ est un bivecteur qui peut s'interpréter comme l'analogue en dimension deux d'un vecteur, {\em i.e.} ``un morceau'' de plan vectoriel muni d'une orientation.

Le bivecteur
\begin{equation*}
E=e_+e_-=e_+\wedge e_-
\end{equation*}
détermine une décomposition dite conforme
\begin{equation*}
\mathbb{R}^{n+1,1}=\mathbb{R}^{n}\oplus\mathbb{R}^{1,1}
\end{equation*}
où $\oplus$ désigne la somme directe  et  $\mathbb{R}^{1,1}$ le plan de Minkowski.
Plus précisément, si nous notons $e_\infty=e_++e_-$, $e_0=(e_--e_+)/2$ (voir Li {\it et al} \cite{Li2001}), et
\begin{equation}
a= a_1 e_1 + \cdots + a_n e_n + a_\infty e_\infty + a_0e_0,
\end{equation}
un vecteur générique de $\mathbb{R}^{n+1,1}$, on peut décomposer $a$ en
\begin{equation*}
a=\pi_E(a)+\pi_E^{\bot}(a),
\end{equation*}
où $\pi_E$ est la projection définie par
\begin{equation*}
\pi_E(a)=(a._i E)E=a_\infty e_\infty+a_0e_0,
\end{equation*}
et $\pi_E^{\bot}$ est la réjection définie par
\begin{equation*}
\pi_E^{\bot}(a)=(a\wedge E)E=a_1 e_1 + \cdots + a_n e_n
\end{equation*}
permet de retrouver la partie euclidienne de $a$. \\
Pour définir la projection $\pi_E$ et la réjection $\pi_E^{\bot}$, il faut utiliser le produit interne et le produit extérieur entre un vecteur et un bivecteur. Comme nous ne les utiliserons pas par la suite, nous omettons ici les détails.

On vérifie facilement que  
\begin{equation} 
	e_{0}^2 = e_{\infty}^2 =0,~~e_{\infty}._i e_0 =-1
	\label{eq:propminkoski}
\end{equation}
Désignons par $\mathcal P^{n+1}(e_\infty,e_0)$ l'hyperplan de $\mathbb R^{n+1,1}$  normal à  $e_\infty$ et contenant $e_0$, donné par l'équation
\begin{equation*}
e_\infty._i(a-e_0)=0,
\end{equation*}
et par $\mathcal N^{n+1}$ le cône nul de $\mathbb R^{n+1,1}$ donné par l'équation
\begin{equation*}
a^2=a._i a=0.
\end{equation*}

\noindent{\bf Définition. }
La représentation conforme de l'espace euclidien $\mathbb{R}^n$ associée à la décomposition conforme donnée par $E=e_+\wedge e_-=e_\infty\wedge e_0$, est l'horosphère
\begin{equation*}
\mathcal H^n(e_\infty, e_0)=\mathcal N^{n+1}\cap \mathcal P^{n+1}(e_\infty,e_0)=\left\{a\in\mathbb R^{n+1,1},\ a^2=0,\ e_\infty\cdot(a-e_0)=0\right\}.
\end{equation*}

Dans la suite, nous noterons $\widetilde x$ le vecteur de $\mathbb R^{n+1,1}$ image du plongement du point ${\bf x}$ de l'espace euclidien $\mathbb R^n$ dans cette horosphère, {\em i.e.}
\begin{equation}
\widetilde x={\bf x}+{{\bf x}^2\over 2}e_\infty+e_0.
\label{eq:plongement}
\end{equation}


\subsubsection{Géométrie euclidienne et algèbre géométrique conforme}
En notant $\mathbf{p}$ et $\mathbf{q}$ deux points de l'espace euclidien $\mathbb R^n$, on a en particulier,
 \begin{gather*}
	\tilde{p}._i\tilde{q}  =(\mathbf{p}+\frac{1}{2}\Vert\mathbf{p}\Vert^2 e_{\infty} + e_0)._i(\mathbf{q}+\frac{1}{2}\Vert\mathbf{q}\Vert^2 e_{\infty} + e_0)=\mathbf{p}\cdot\mathbf{q} + \frac{1}{2}\Vert\mathbf{q}\Vert^2 \underbrace{\mathbf{p}._ie_{\infty}}_{0}+ \underbrace{\mathbf{p}._ie_0}_{0}\nonumber\\
	+\frac{1}{2}||\mathbf{p}||^2 \left(\underbrace{\mathbf{q}._ie_{\infty}}_{0} + \frac{1}{2}\Vert\mathbf{q}\Vert^2 \underbrace{e_{\infty}._ie_{\infty}}_{0} + \underbrace{e_{\infty}._ie_{0}}_{-1}\right) + 
	 \underbrace{e_{0}._i\mathbf{q}}_{0} + \frac{1}{2}\Vert\mathbf{q}\Vert^2 \underbrace{e_{0}._ie_{\infty}}_{-1} + \underbrace{e_{0}._ie_{0}}_{0}\nonumber\\
= -\frac{1}{2}\Vert\mathbf{p}-\mathbf{q}\Vert^2 \label{eq:pspp}
\end{gather*}

Cette formule permet de comprendre comment définir une hypersphère de $\mathbb R^n$ comme un vecteur de l'algèbre géométrique conforme et comment tester par un produit intérieur l'appartenance d'un point à cette hypersphère. Soit donc une hypersphère de centre $\mathbf{c}$ et de rayon $\rho$, {\em i.e.} l'ensemble des $\mathbf{x}$ vérifiant~:

\begin{equation*}
	\Vert\mathbf{x}-\mathbf{c}\Vert^2 = \rho^2.
	\label{eq:defhs}
\end{equation*}

Avec les notations introduites précédemment, cette équation peut s'écrire sous la forme :
\begin{equation*}
\tilde{x}._i \tilde{c} = -\frac{1}{2}\Vert\mathbf{x}-\mathbf{c} \Vert^2=-\frac{1}{2}\rho^2.
\end{equation*}

On en déduit qu’en utilisant $\tilde{x}._i e_{\infty}=-1$ :
\begin{gather*}
		\tilde{x}._i \tilde{c} = -\frac{1}{2} \rho^2 \Longleftrightarrow \tilde{x}._i \tilde{c} - \frac{1}{2} \rho^2 = 0 \\
		\Longleftrightarrow\tilde{x}._i \tilde{c} - (-\frac{1}{2}\rho^2)(-1)=0 \Longleftrightarrow \tilde{x}._i \tilde{c} -(\frac{1}{2}\rho^2)(x._i e_{\infty})=0\Longleftrightarrow \\
		\Longleftrightarrow \tilde{x}._i \underbrace{(\tilde{c}-\frac{1}{2}\rho^2e_{\infty})}_{\tilde{s}}=0 \Longleftrightarrow \tilde{x}._i \tilde{s} = 0,
\end{gather*}\\
avec
\begin{equation}
		\tilde{s} = \mathbf{c} + e_0 + \frac{1}{2}\parallel \mathbf{c} \parallel^2 e_{\infty} - \frac{1}{2}\rho^2 e_{\infty}
		\label{eq:tildes}
\end{equation}
		
Par conséquent, dans l'algèbre géométrique conforme, l'hypersphère de centre $\mathbf{c}$ et de rayon $\rho$ correspond au vecteur $\tilde{s}$ de $\mathbb{R}^{n+1,1}$ mentionné précédemment. On constate par ailleurs que deux contraintes de normalisation doivent être satisfaites : 

\begin{equation*}
	\begin{array}{l}
		\tilde{s}^2 = \rho^2 >0,\ \ e_{\infty}._i \tilde{s} = -1.
	\end{array} 
\end{equation*}

Les propriétés que l'on retient pour la suite sont les suivantes : le point ${\bf x}$ est

\begin{itemize}
	\item à l'intérieur de l'hypersphère si $$ 0 < \tilde{x} ._i \tilde{s} \leq \frac{1}{2} \rho^2,$$
	\item sur l'hypersphère si $$\tilde{x}._i \tilde{s} = 0,$$
	\item à l'extérieur de l'hypersphère s'il vérifie $$\tilde{x}._i \tilde{s} < 0.$$
\end{itemize}

En particulier,
\begin{equation*}
\tilde{x}._i \tilde{c}=\frac{1}{2}\rho^2 \quad \mbox{ ssi } \bf x=\bf c .
\end{equation*}


Les figures \ref{fig:ps} et \ref{fig:ds} illustrent les variations du produit $\tilde{x}._i \tilde{s}$ pour une sphère de centre $(0, 0)$, de rayon égal à 5. 

\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}
		\begin{axis}[height=10cm,
			width=10cm,
			grid= major ,
			xlabel = {$x_0$} ,
			ylabel = { $\tilde{x}._i \tilde{s}$} ,
			xmin=-10,xmax=10,
			legend entries={$\frac{1}{2}\rho^2$},
			]
			%\addplot table [x=a, y=c, col sep=comma] {ProduitScalaire.csv};
			%\addplot coordinates {(0,0) (0.1,200) (0.2,300) (0.3,350) (0.4,375) };
			\addplot[mark =*, green] coordinates {(0,12.25)};
			\addplot[mark=none, blue] coordinates {(-10,0) (10,0)};
			\addplot[red, mark=none] table [x=a, y=b,col sep=comma] {sources/ProduitScalaire.csv};
		\end{axis}
	\end{tikzpicture}
	\caption{Profil du produit  $\tilde{x}._i \tilde{s}$}
	\label{fig:ps}
	
\end{figure}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=13cm]{figs/ps2d.pdf}
		\caption{Distance signée $\tilde{x}._i \tilde{s}$}
		\label{fig:ds}
	\end{center}
\end{figure}

Il est possible de même de représenter l'hyperplan de $\mathbb R^n$ de vecteur normal unitaire $\bf n$ et passant par le point ${\bf a}$ par le vecteur $\widetilde h$ de l'algèbre géométrique conforme qui s'écrit
\begin{equation*}
\widetilde h = \bf n+\delta e_\infty,
\end{equation*}
avec $\delta = n\cdot{\bf a}$. On vérifie facilement que 

\begin{equation*}
\widetilde x._i\widetilde h=\bf n\cdot({\bf x}-{\bf a}).
\end{equation*}

Si $x$ appartient à l'hyperplan représenté par $\tilde{h}$ alors $\tilde{x}._i\tilde{h}=0$. 
Grâce à cette description, on peut vérifier que l'hypersphère représentée par le vecteur
\begin{equation*}
\widetilde s=({\bf c}+\alpha {\bf n})+{(\bf c+\alpha \bf n)^2\over 2}e_\infty+e_0-{(\alpha +\rho)^2\over 2}e_\infty
\end{equation*}
{\em i.e.} l'hypersphère de centre ${\bf c}+\alpha {\bf n}$ et de rayon $\alpha +\rho$ tend vers  l'hyperplan $\widetilde h$ précédent lorsque $\alpha>0$ tend vers l'infini. En effet, par la renormalisation 
\begin{equation*}
e_\infty._i \widetilde s=-1/\alpha,
\end{equation*}
l'hypersphère correspond au vecteur
\begin{gather*}
\widetilde s=
{(\bf c+\alpha \bf n)\over \alpha}+{(\bf c+\alpha \bf n)^2\over 2\alpha}e_\infty+{e_0\over\alpha}-{(\alpha +\rho)^2\over 2\alpha}e_\infty\\
=\left({\bf c\over\alpha}+\bf n\right)+\left({{\bf c}^2\over 2\alpha}+{\bf c}\cdot {\bf n}+{\alpha\over 2}-{\alpha\over 2}-\rho-{\rho^2\over 2\alpha}\right)e_\infty+{e_0\over\alpha}.
\end{gather*}
Quand $\alpha>0$ tend vers l'infini, alors ce vecteur tend vers
\begin{equation*}
\widetilde h=\bf n+(\bf c\cdot \bf n-\rho)e_\infty,
\end{equation*}
avec
\begin{equation*}
\bf a=\bf c-\rho\bf n,\ \ \bf n\cdot(\bf c-\rho \bf n)=\bf c\cdot \bf n-\rho.
\end{equation*}

Ce type de calcul, donné en exemple, illustre bien la cohérence des définitions et des opérations du modèle conforme d'Hestenes.


%%%%%%%%%%%%%%%%%%%%%%
\newpage


















\section{Définition des couches hypersphériques}


Les modèles de réseaux à couches hypersphériques se composent d'une ou plusieurs hypersphères, dont les paramètres, représentant le centre et le rayon, sont ajustés au cours de l'entraînement pour optimiser la séparation des données. Sans ajout de fonction d'activation, ces couches permettent de créer des frontières décisionnelles non linéaires, contrastant ainsi avec les couches traditionnelles.


Dans les réseaux de neurones classiques, les couches sont généralement de type dense ou à convolution. Les couches de type dense permettent de réaliser des séparations complexes dans l'espace des caractéristiques, tandis que les couches à convolution appliquent ce principe de manière localisée, ce qui est particulièrement utile pour le traitement d'images. 

Nous montrons ici comment construire l'analogue de ces deux types de couches à partir du modèle conforme décrit précédemment.


\subsection{Couches hypersphériques denses}

Une couche hypersphérique a été développée par \cite{Banarer2003}  en utilisant des hypersphères représentées par des vecteurs de \(\mathbb{R}^{n+1,1}\).

Le schéma \ref{fig:reseau1cc} illustre un réseau de neurones avec une couche cachée utilisant une hypersphère. La première étape, intégrée dans l'implémentation de la couche hypersphérique, consiste à envoyer les données d'entrée dans l'espace de l'algèbre géométrique conforme en utilisant le plongement décrit par \eqref{eq:plongement}. Ensuite, il est nécessaire de prendre en compte la contrainte d'unicité, qui peut être considérée selon plusieurs méthodes décrites dans la section \ref{sec:sec_optimisation}.

La sortie de la couche pour une hypersphère de centre $\mathbf{c}$ et de rayon $\rho$  s'écrit~:


\begin{equation}
	y = -\frac{1}{2} \left[ \| \mathbf{x} - \mathbf{c} \|^2 - R^2 \right]
\end{equation}


Le signe de \(y\) détermine la position relative du point \(\mathbf{x}\) par rapport à l'hypersphère, conformément aux propriétés établies précédemment. Cette quantité est donc calculée par le produit interne $\tilde{s}\cdot_i \tilde{x}$.
\bigskip

\def\layersep{6cm}
\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
		\tikzstyle{every pin edge}=[<-,shorten <=1pt]
		\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
		\tikzstyle{neuron_}=[]
		\tikzstyle{enter neuron}=[neuron, fill=red!100];
		\tikzstyle{input neuron}=[neuron, fill=yellow!120];
		\tikzstyle{output neuron}=[neuron, fill=green!100];
		% Draw neuron_ info
		\foreach \name / \y in {1,...,2}
		\node[enter neuron, left of=J, pin=left:{\tiny $e_{\name}$}] (K-\name) at (0,-\y+1/2) {$x_{\y}$};
		\node[enter neuron, left of=J, pin=left:{\tiny $e_0$}] (K-3) at (0,-3+1/2) {$1$};
		\node[enter neuron, left of=J, pin=left:{\tiny $e_{\infty}$}] (K-4) at (0,-4+1/2) {$\frac{||x||^2}{2}$};
		% Draw the input layer nodes
		\foreach \name / \y in {0}
		\node[input neuron] (J-\name) at (0,-1.75) {$y = \tilde{s}._i\tilde{x}$};
		\foreach \name / \y in {0}
		\path (K-1) edge (J-\name);
		\foreach \name / \y in {0}
		\path (K-2) edge (J-\name);
		% Draw the input layer nodes
		\foreach \name / \y in {0}
		\path (K-3) edge[dashed,->,red] (J-\name);
		\foreach \name / \y in {0}
		\path (K-4) edge[dashed,->,red] (J-\name);
	\end{tikzpicture}
	\caption{Schéma d'un réseau à une couche cachée hypersphérique}
	\label{fig:reseau1cc}
\end{figure}

On peut noter que la définition du produit interne induit les différences suivantes sur le paramétrage des couches hypersphériques~:
\begin{itemize}
	\item  Couche dense ($n+1$ paramètres) : le produit $\mathbf{w} \cdot \mathbf{x} $ est linéaire en $x$ et n'est pas borné
	\item Couche sphérique   ($n+1$ paramètres) : le produit $\tilde{x} ._i \tilde{s}$ est quadratique en les coordonnées de $\mathbf{x}$ et est majoré par $\rho^2/2$
\end{itemize}

\bigskip

\noindent\textbf{Remarque :} Comme expliqué précédemment, les paramètres du vecteur conforme représentant l'hypersphère \(\tilde{s}\) sont ajustés par l'apprentissage. Ses paramètres encodent à la fois le centre (les $n$ premières coordonnées) et une forme implicite du rayon à travers la composante en $e_{\infty}$ qui sont ajustées par le processus d'apprentissage.


\subsection{Couches convolutives}

L'extension du modèle de neurone hypersphérique à un filtre de convolution hypersphérique ne nécessite pas de nouveaux outils mathématiques. Cette extension sera décrite pour un filtre de convolution 2D discret, applicable au traitement des images ; toutefois, l'approche reste valable en toute dimension, indépendamment du mode de \textit{padding} et des valeurs de \textit{strides} choisies.

La convolution standard est bien connue pour être équivalente au produit scalaire entre les versions vectorisées du bloc image et du filtre \( F \) auquel est ajouté un biais~:


\begin{gather*}
	(I * F)[j,j']=  \sum_{d_j} \sum_{d_{j'}} I[j+d_j, j'+d_{j'}] * F[d_j, d_{j'}] + b
	\nonumber \\
	= \mathrm{vec}(I[j-d_j:j+d_j, j'-d_{j'}:j'+d_{j'}]) \cdot
	\mathrm{vec}(F[-d_j:d_j, -d_{j'}:d_{j'}])+ b .   
	%\label{eq:vecps}
\end{gather*}
où  $d_j$ et $d_{j'}$ désignent les indices de parcours du filtre qui dépendent de la taille du filtre $d \times d$.
De façon naturelle, on peut voir vec$(I[j-d_j:j+d_j, j'-d_{j'}:j'+d_{j'}])$ comme un vecteur $x_I(j,j')$ de $\mathbb{R}^{d^2}$ que l'on va plonger dans l'algèbre géométrique conforme $\mathbb{R}^{d^2+1,1}$. Le filtre de convolution est remplacé par une sphère  en dimension $\mathbb{R}^{d^2}$ paramétrée dans cette même algèbre. Si l'on note $\circledast$ cette "nouvelle" convolution, on en définit le résultat comme suit~: 


\begin{equation}
	(I \circledast s)[j,j'] = x_I(j,j') ._i s
	\label{eq:sphericalconv}
\end{equation}

La valeur de sortie de ce filtre possède donc les mêmes propriétés que le produit interne $\tilde{s}._i\tilde{x}$ défini dans la sous-section précédente (voir figure \ref{fig:ps}). L'extension de ce filtre aux images à $c$-canaux ne pose pas de difficultés. Dans cet esprit, on envisagera deux approches, ``Conv2d'' et ``DepthWiseConv2d'' (dans le cas où une hypersphère est définie par canal)~:\\

\begin{itemize}
	\item $(I \circledast s)[j,j'] = \sum_c x_I(j,j',c) ._i s$ ($\sim$ Conv2d)\\
	
	\item $(I \circledast s)[j,j'] = \sum_c x_I(j,j',c) ._i s_c$  ($\sim$ DepthWiseConv2d)
\end{itemize}


\begin{figure}[H]
	\begin{center}		\includegraphics[width=16cm]{figs/dessin_prod_convol_img_spheric2.png}
		\caption{Approche par convolution hypersphérique}
		\label{fig:conv}
	\end{center}
\end{figure}

\subsection{Implémentation des couches hypersphériques}


\subsubsection{Couches denses (formulation matricielle)}



Dans la base  $\{e_1, e_2,\ldots,e_n, e_0, e_{\infty}\}$, le vecteur $\mathbf{x}$ et la sphère $s$  de centre $\mathbf{c}$  et de rayon $\rho$ s'expriment comme des vecteurs à $n+2$ coordonnées~:
\begin{align}
	\mathbf{x}  & \mapsto  \check{x}:= [x_1, x_2, \ldots, x_n, 1, \frac{1}{2} ||\mathbf{x}||^2]^{t}\label{eqcheckx}\\
	(\mathbf{c}, \rho) & \mapsto \check{s} := [c_1, c_2, \ldots, c_n, 1, \frac{1}{2} (||\mathbf{c}||^2-\rho^2)]^{t}
\end{align}
\\


Pour garantir l'efficacité du calcul du produit interne $._i$, celui-ci va être exprimé comme un produit matriciel $\check{x}^{t} M \check{s}$.
On rappelle que les règles de calcul suivantes doivent être vérifiées :
\begin{equation}
	\left\{\ \begin{array}{l r}
		e_j ._i e_{j'} = \delta_{jj'}, & \forall j,j' \in \{1,\ldots,n\}\\
		e_j ._i e_{0} = e_j ._i e_{\infty} = 0\\
		e_{\infty} ._i e_0 = e_0 ._i e_{\infty} = -1
	\end{array}\right.
	\label{eq:metrique}
\end{equation}
avec
\[
\delta_{jj'} = \begin{cases} 
	1 & \text{si } j = j', \\
	0 & \text{sinon}.
\end{cases}
\]


On peut vérifier que la matrice $M$ qui traduit les relations données dans le système  \eqref{eq:metrique} s'écrit~:\\

\begin{equation}
	M =\left( \begin{array}{cccccc}
		
		1 & 0 & \dots & 0 & 0 & 0\\
		
		0 & 1 & \dots & 0 & 0 & 0\\
		
		\vdots & \vdots & \ddots & 0 & 0 & 0\\
		
		0 & 0 & \dots & 1 & 0 & 0\\
		
		0 & 0 & \dots & 0 & 0 & -1\\
		
		0 & 0 & \dots & 0 & -1 & 0\\
		
	\end{array}\right)	
\end{equation}\\


On peut alors vérifier (voir les calculs ci-dessous) les deux propriétés  suivantes pour le produit de deux points ou d'un point et d'une hypersphère~:\\

\begin{equation}
	\check{x}^{t} M \check{s} =  \frac{1}{2}\left(\rho^2 - ||\mathbf{x} - \mathbf{c}||^2\right)
	\mbox{~~et~~}\check{p}^{t} M \check{q} =  -\frac{1}{2}||\mathbf{p} - \mathbf{q}||^2
	\label{eq:ecriturematriciel}
\end{equation}\\


\noindent Vérification : produit $\tilde{p} ._i \tilde{q}$ entre deux points :

$\begin{array}{lcl}
	\check{p}^{t} M \check{q} & = & (\mathbf{p}^t) M \mathbf{q}
	\\
	& = & \left(\mathbf{p}^t, 1 , \frac{1}{2}\parallel \mathbf{p} \parallel^2\right) 
	\left(
	\begin{array}{ccc}
		Id_{n} & 0 & 0 \\
		0 & 0 & -1 \\
		0 & -1 & 0
	\end{array}
	\right)
	\left(\begin{array}{c}
		\mathbf{q}\\
		1\\
		\frac{1}{2}\parallel \mathbf{q} \parallel^2
	\end{array}\right)
		\\
	& = & \left(\mathbf{p}^t, -\frac{1}{2}\parallel \mathbf{p} \parallel^2, -1\right) \left(\begin{array}{c}
		\mathbf{q}\\
		1\\
		\parallel \mathbf{q} \parallel^2
	\end{array}\right)\\
	\\
	& = & \mathbf{p}^t \mathbf{q} - \frac{1}{2} \left(\parallel \mathbf{p} \parallel^2 - \parallel \mathbf{q} \parallel^2\right)\\
	\\
	& = & -\frac{1}{2} \parallel \mathbf{p} - \mathbf{q} \parallel^2
\end{array}$

\vspace{0.75cm}

\noindent Vérification : produit entre un point et une hypersphère :

$\begin{array}{lcl}
	\check{x}^{t} M \check{s} & = & \left(\mathbf{x}^t, 1 , \frac{1}{2}\parallel \mathbf{x} \parallel^2\right) \left(\begin{array}{ccc}
		Id_n & 0 & 0\\
		0 & 0 & -1\\
		0 & -1 & 0
	\end{array}\right)\left(\begin{array}{c}
		\mathbf{c}\\
		1\\
		\frac{\parallel \mathbf{c} \parallel^2 - \rho^2}{2}
	\end{array}\right)
	\\
	& = & \left(\mathbf{x}^t, -\frac{1}{2}\parallel \mathbf{x} \parallel^2, -1\right) \left(\begin{array}{c}
		\mathbf{c}\\
		1\\
		\frac{\parallel \mathbf{c} \parallel^2 - \rho^2}{2}
	\end{array}\right)\\
		\\
	& = & \frac{1}{2}\left(\rho^2 - \parallel \mathbf{x}-\mathbf{c} \parallel^2\right)
	
\end{array}$

\vspace{1cm}

\subsubsection{Couches convolutives}

Pour le filtre hypersphérique, le calcul de $\check{x}^{t} M \check{s}$ peut être divisé en trois étapes distinctes :

\begin{enumerate}
	\item La convolution standard de l'image avec le filtre correspondant aux $n$ premières coordonnées de $\check{s}$ redimensionnées à la taille $d \times d$.	
	\item L'opération $-\frac{1}{2}||x||^2$ revient à faire la convolution de l'image par un filtre constant de valeur $-\frac{1}{2}$ et de taille $d \times d$. Le résultat est multiplié par l'avant-dernière coordonnée de $\check{s}$ qui vaut 1.
	\item Une multiplication terme à terme est effectuée pour obtenir le tenseur contenant les coefficients correspondant au produit entre la composante $e_0$ de $\check{x}$ et la composante  $e_{\infty}$ de $\check{s}$.
\end{enumerate}

Le résultat de la convolution hypersphérique est la somme de ces trois termes.
Par ailleurs, si l'image traitée contient plusieurs canaux, une nouvelle somme selon les canaux de profondeur permet de reproduire le comportement du réseau de type Conv2D.


\subsubsection{Optimisation des poids du réseau de neurones}
\label{sec:sec_optimisation}

Une hypersphère est représentée de manière unique par un vecteur \(\tilde{s}\) dont l'avant-dernière coordonnée est fixée à 1, conformément à la contrainte \(\tilde{s} \cdot e_{\infty} = -1\). Pour optimiser les paramètres de l'hypersphère, c'est-à-dire les coordonnées de son centre \(\mathbf{c}\) et la valeur de son rayon \(\rho\), tout en respectant cette contrainte, plusieurs approches peuvent être envisagées :\\

\begin{itemize}
\item Une première méthode consiste à effectuer la descente du gradient directement sur les $n+2$ coordonnées puis normaliser le vecteur résultat pour garantir \(\tilde{s} \cdot e_{\infty} = -1\). La normalisation consiste à diviser le vecteur dans la base $\{e_1, e_2,\ldots,e_n, e_0, e_{\infty}\}$ par la composante en $e_0$. \\

\item Une autre approche fixe la composante \(e_0\) de \(\tilde{s}\) à 1, et l'optimisation est alors effectuée uniquement sur les \(n+1\) autres éléments du vecteur. Cette méthode simplifie l'optimisation en réduisant le nombre de paramètres à ajuster, tout en maintenant la contrainte nécessaire.\\
C'est la méthode qui a été privilégiée dans les codes utilisés pour cette thèse.\\

\item Enfin, une perspective possible est de développer une méthode de descente de gradient qui opère directement dans l'espace des sphères, en garantissant que le produit \(e_0 \cdot e_{\infty}\) reste égal à -1 tout au long du processus d'optimisation. Cela permettrait de maintenir la contrainte \(s ._i e_{\infty} = -1\) de manière plus intégrée, évitant ainsi la nécessité de re-normalisation après chaque mise à jour.
\end{itemize}


	\section{Lien de parenté avec d'autres réseaux}
	Dans ce paragraphe, nous évoquons deux autres familles de réseaux dont les noms laissent entrevoir une certaine parenté avec ceux manipulés dans cette thèse :  les réseaux de neurones à fonctions à base radiale (RBF) et les réseaux de neurones de Clifford. 
	
	\subsection{Lien avec les réseaux de neurones à fonctions à bases de radiales}
	
	Les fonctions à base radiales (RBF) \cite{buh2003} sont en particulier bien connues pour leur capacité à approcher des fonctions non linéaires. 
	Ce sont des fonctions dont la valeur dépend uniquement de la distance entre un point d'entrée et un centre fixe. L'approximation d'une fonction $f$ donnée par un réseau RBF avec $J$ fonctions radiales prend donc la forme~:
	
	\begin{equation}
		\vspace*{-0.1cm}
		f(x) \approx \sum_{j=1}^J w_j \rho_j(||\mathbf{x} - \mathbf{c}_j||)
	\end{equation}
	avec  $\rho_j : \mathbb{R}^+ \mapsto \mathbb{R}$. Un exemple classique est celui du noyau gaussien, $\rho_j(||\mathbf{x} - \mathbf{c}_j||) = e^{-\lambda_j^2||\mathbf{x} - \mathbf{c}_j||^2}$ où $\lambda_j$ est un paramètre d'échelle  s'apparentant au rayon d'une sphère. 
	Le lien avec les réseaux de neurones à couches hypersphériques sera davantage détaillé dans la section \ref{sec:lienrbf}, en particulier dans le cas $n=1$ où une formulation analytique des sorties d'un réseau à une couche hypersphérique permet de clairement discerner les différences ou les similarités avec celles d'un réseau RBF.

\subsection{Réseaux de neurones Clifford}

L'algèbre géométrique conforme du modèle d'Hestenes \cite{Hestenes2001} étant un cas particulier des algèbres géométriques, tous les travaux portant sur les réseaux de neurones dits de Clifford (initialement introduits par Buchholz et Sommer \cite{articleBuchholz}, \cite{Buchholz_these} pour enrichir les réseaux neuronaux avec des propriétés géométriques) peuvent naturellement être rapprochés (puis exploités) de ce qui est fait ici.

Les neurones de Clifford étendent le modèle du perceptron classique en remplaçant les opérations réelles par des opérations dans l'algèbre de Clifford. Deux types de neurones Clifford sont généralement utilisés~:\\

\begin{itemize}
    \item Neurone de Base Clifford (BCN) : Dans un neurone de Clifford, les entrées et les poids sont les composantes des multivecteurs appartenant à une algèbre de Clifford $C_{p,q}$. La combinaison linéaire classique est remplacée par le produit géométrique noté $\otimes_{p,q}$ (avec $\theta$ un biais). La fonction de propagation d'un neurone Clifford de base est définie par~:
    \begin{itemize}
        \item $f(\mathbf{x}) = \mathbf{w} \otimes_{p,q} \mathbf{x} + \theta$ (multiplication à droite)
        \item $f(\mathbf{x}) = \mathbf{x} \otimes_{p,q} \mathbf{w} + \theta$ (multiplication à gauche)
    \end{itemize}
    où $\mathbf{w}$, $\mathbf{x}$, $\theta \in C_{p,q}$ sont des multivecteurs.

    Comme dans notre cas, le produit scalaire usuel est remplacé par un produit géométrique. On notera que la structure est complexifiée par l'absence de commutativité.\\

    \item Neurone Spineur Clifford (SCN) : le produit géométrique est étendu de manière à inclure des transformations orthogonales (comme les rotations) appliquées aux entrées. Les spineurs, qui sont des éléments de Clifford agissant comme des opérateurs de rotation, sont utilisés pour transformer les vecteurs d’entrée de manière contrôlée. La transformation de $\mathbf{x}$ s'écrit alors~: $y= \mathbf{w} \otimes_{p,q} \mathbf{x} \otimes_{p,q} \mathbf{\phi(w)} + \theta$, où $\mathbf{\phi(w)}$ peut représenter l'inversion, la réversion ou la conjugaison par un neurone spineur Clifford.
\end{itemize}

\medskip

Les réseaux de neurones de Clifford sont construits en connectant plusieurs neurones de Clifford. On trouve dans la littérature trois catégories principales \cite{articleBuchholz}~:
\begin{itemize}
    \item Clifford Multilayer Perceptron (CMLP) : utilise des BCN avec des fonctions d'activation à valeurs réelles.
    \item Spinor Clifford Multilayer Perceptron (SCMLP) : utilise des SCN avec des fonctions d'activation à valeurs réelles.
    \item Clifford MLP avec Fonctions d'Activation à Valeurs Clifford (FCMLP) : utilise des fonctions d'activation à valeurs dans une algèbre de Clifford.
\end{itemize}

\medskip

L'apprentissage des neurones de Clifford s'effectue par une extension de la descente de gradient classique. Il faut noter que le produit de deux éléments non nuls d'une algèbre de Clifford peut être nul, ce qui induit quelques difficultés pour l'étape de rétropropagation du gradient. On utilise donc des involutions (une fonction qui, appliquée deux fois à un multivecteur, retourne le multivecteur d'origine ; cela peut être l'inversion, la réversion ou la conjugaison). Par exemple, l'algorithme de rétropropagation pour les CMLP (Clifford Multilayer Perceptrons) utilise une involution unique déterminée par l'algèbre de Clifford sous-jacente. Les outils développés dans cette thèse ne présentent pas autant de complexité technique pour leur implémentation.

\section{Expérimentations}

Les expérimentations qui suivent ont été mises en place pour valider les modèles de couches hypersphériques construits à partir d'un ensemble de neurones ou de filtres de convolution hypersphériques. Comme nous nous positionnons sur des briques élémentaires de bas niveau, nous avons volontairement choisi des architectures de réseau simples afin de limiter l'écueil du sur-apprentissage et des difficultés d'interprétation.
\\

Nos expérimentations sont présentées en deux parties, sans puis avec convolution.

\subsection{Réseaux à couches hypersphériques {\it vs} couches denses}
\subsubsection{Jeux de données}
Deux jeux de données synthétiques ont été utilisés pour étudier la performance des réseaux à couches hypersphériques. 

Le premier nommé \emph{Easy} est construit de façon à avoir 3 classes quasiment linéairement séparables. Celui-ci contient 350 points en dimension 2, ce qui donne de plus l'opportunité d'illustrer les résultats par différentes images.\\
Le second nommé \emph{Dif} est constitué d'une série de 450 points en dimension 2 répartis en 3 classes non linéairement séparables avec un taux de chevauchement non négligeable. Cette structure nous permettra de visualiser et de comparer la forme des frontières de décision pour différentes architectures.\\

Les paramètres ayant servi à générer ces jeux de données sont donnés dans \cite{saintjean:tel-00145895}. 
Les deux jeux de données ont été séparés en ensemble d'apprentissage (66.66\%) et de validation (33.33\%).

\subsubsection{Types de réseaux et entraînement}
L'architecture de base que nous considérons est un perceptron multi-couches de faible profondeur. 
Dans un premier cas, les couches du réseau (noté PMC) seront des couches denses classiques. Le réseau nommé GeoPMC ne contient quant à lui que des neurones hypersphériques.

On a évalué l'influence de différents paramètres :
\begin{itemize}
	\item Le nombre de couches cachées varie de 0 à 2.
	\item Le nombre de neurones par couche cachée prend les valeurs 2, 3 ou 10. 
	\item La batch normalisation et la fonction d'activation \emph{ReLU} s'appliquent ou non sur toutes les couches cachées.
\end{itemize}

Chaque réseau se termine par une couche de 3 neurones classiques ou hypersphériques avant de passer par la fonction d'activation softmax car il s'agit d'un problème de classification ; celle-ci n'est jamais précédée d'un \emph{ReLU}.

Pour faciliter la lecture des résultats, chaque architecture du réseau est codée par une chaîne de caractères : un chiffre correspond au nombre de neurones par couche, "r" à l'utilisation de la fonction d'activation relu, "b" à une batch normalisation et "sf" à la fonction d'activation softmax.

La taille du batch est de 30, l'algorithme d'optimisation est Adam \cite{Kingma2014AdamAM}.

\subsubsection{Résultats}
Bien que l'ensemble des configurations ci-dessus ait été testé, seul un ensemble de configurations efficaces a été sélectionné pour ce manuscrit.
Les figures \ref{fig:acceasy} et \ref{fig:accdif} reportent le taux de bonne prédiction (\emph{accuracy}) sur les jeux de données de validation au cours des itérations.
Pour des architectures PMC et GeoPMC identiques, on constate que l'\emph{accuracy} augmente plus rapidement pour les modèles GeoPMC.
L'échec du modèle GeoPMC "b 2 r b 2 r b 3 sf" s'explique probablement par son architecture qui contribue à l'inhibition de certains neurones (\emph{dying-ReLU})

.\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\begin{axis}[height=9cm,
			width=9cm,
			grid= major ,
			xlabel = {Epoch} ,
			ylabel = {Accuracy} ,
			xmax=200,
			legend entries={geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, },
			legend style={at={(0.5,0)},anchor=south west,nodes={scale=0.6, transform shape}}],
			]
			\addplot[red,mark=none] table [x=Step, y=Value,col sep=comma]
			{data/easy_geoPMC_3_accuracy.csv};
			\addplot[black,mark=none] table [x=Step, y=Value,col sep=comma]
			{data/easy_geoPMC_10_3_relu_accuracy.csv};
			\addplot[blue, mark=none] table [x=Step, y=Value,col sep=comma]
			{data/easy_geoPMC_2_2_3_bn_relu_accuracy.csv};
			\addplot[red, mark=none,dashed] table [x=Step, y=Value,col sep=comma]
			{data/easy_PMC_3_accuracy.csv};
			\addplot[black,mark=none,dashed] table [x=Step, y=Value,col sep=comma]
			{data/easy_PMC_10_3_relu_accuracy.csv};
			\addplot[blue,mark=none,dashed] table [x=Step, y=Value,col sep=comma]
			{data/easy_PMC_2_2_3_bn_relu_accuracy.csv};
			
		\end{axis}
	\end{tikzpicture}
	\caption{easy: \emph{accuracy} pour les données de validation }
	\label{fig:acceasy}
	\vspace*{-0.5cm}
\end{figure}

\begin{figure}[H]
	\vspace*{-0.15cm}
	\centering
	\begin{tikzpicture}
		\begin{axis}[height=9cm,
			width=9cm,
			grid= major ,
			xlabel = {Epoch} ,
			ylabel = {Accuracy} ,
			xmax=200,
			legend entries={ geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, },
			legend style={at={(0.5,0)},anchor=south west,nodes={scale=0.6, transform shape}}],
			]
			\addplot[red,mark=none] table [x=Step, y=Value,col sep=comma]
			{data/dif_geoPMC_3_accuracy.csv};
			\addplot[black,mark=none] table [x=Step, y=Value,col sep=comma]
			{data/dif_geoPMC_10_3_relu_accuracy.csv};
			\addplot[blue, mark=none] table [x=Step, y=Value,col sep=comma]
			{data/dif_geoPMC_2_2_3_bn_relu_accuracy.csv};
			\addplot[red, mark=none,dashed] table [x=Step, y=Value,col sep=comma]
			{data/dif_PMC_3_accuracy.csv};
			\addplot[black,mark=none,dashed] table [x=Step, y=Value,col sep=comma]
			{data/dif_PMC_10_3_relu_accuracy.csv};
			\addplot[blue,mark=none,dashed] table [x=Step, y=Value,col sep=comma]
			{data/dif_PMC_2_2_3_bn_relu_accuracy.csv};
			
		\end{axis}
	\end{tikzpicture}
	\caption{\emph{Dif}: \emph{accuracy} pour les données de validation }
	\label{fig:accdif}
	\vspace*{-0.15cm}
\end{figure}


Pour aller plus loin, la table \ref{table:acc} reporte l'\emph{accuracy} en fin d'entraînement des 14 réseaux les plus performants. 
Excepté les trois premiers cas, l'ensemble des réseaux efficaces sur \emph{Easy} le sont également sur \emph{Dif}. 
\begin{table}[H]
	\center
	\small
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{}&\multicolumn{2}{|c|}{\emph{Easy}}&\multicolumn{2}{|c|}{\emph{Dif}}\\
		\hline
		\multicolumn{1}{|c|}{Architectures} & GeoPMC & PMC & GeoPMC & PMC\\
		\hline
		10 r : 3 sf  & 92\% & \textbf{99\%} & 94.6\% & 92.6\%\\
		\hline
		b 2 r : b 2 r : b 3 sf  & 41\% & 68\% & 86\% & \textbf{94\%}\\
		\hline
		3 r : 3 sf & 67.2\% & \textbf{95\%} & 93.3\% & 92.6\%\\
		\hline
		3 sf & 96.5\% &\textbf{97.4\%} & 93.3\% & 92\%\\
		\hline
		b 2 : b 2 : b 3 sf & \textbf{98.3\%} & \textbf{98.3\%} & 91.3\% & 92\%\\
		\hline
		10 : 3 sf & \textbf{99\%} & 96\% & 92\% & 93.3\%\\
		\hline
		10 : 10 : 3 sf & \textbf{99\%} & 94\% & 91.3\% & \textbf{93.3\%}\\
		\hline
		2 : 2 : 3 sf & \textbf{98.3\%} & 95.7\% & 91.3\% & 93.3\%\\
		\hline
		3 : 3 : 3 sf & \textbf{98.3\%} & 95.7\% & 90\% & \textbf{93.3\%}\\
		\hline
		b 10 : b 3 sf & \textbf{98.3}\% & 97.4\% & 93\% & 91.3\%\\
		\hline
		b 10 r : b 3 sf & \textbf{98.3\%} & \textbf{98.3\%} & 92\% & 92\%\\
		\hline
		b 3 r : b 3 sf & \textbf{98.3\%} & \textbf{98.3\%} & 92\% & 91.6\%\\
		\hline
		b 3 sf & 97.4\% & \textbf{98.3\%} & 92.4\% & 92\%\\
		\hline
		b 2 : b 3 sf & \textbf{98.3\%} & \textbf{98.3\%} & 91.8\% & 91.6\%\\
		\hline
	\end{tabular}
	\normalsize
	\caption{Accuracy pour différentes architectures}
	\label{table:acc}
\end{table}

	
	Hormis l'architecture "10 r : 3 sf" qui s'avère être plus performante pour les réseaux à couches hypersphériques et le réseau "b 2 r : b 2 r : b 3 sf" qui lui semble être meilleur que le modèle classique sur les données de \emph{Easy}, l'ensemble des résultats d'\emph{accuracy} ne sont pas significativement différents. Étant donnée la taille des jeux de données de validation, un écart de 1\% ne représente en effet que 1 ou 2 points mal classés.\\
	
	Afin de mieux voir l'impact de la fonction d'activation \emph{ReLU}  et de la batch normalisation, les tableaux \ref{table:releasy} et \ref{table:reldif} présentent les résultats de l'apprentissage par architecture. \\
	
	\begin{table}[H]
		\vspace*{-0.25cm}
		\center
		\small
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\multicolumn{5}{|c|}{GeoPMC}\\
			\hline
			Architectures & $\emptyset$ & \emph{ReLU} & bn & bn + \emph{ReLU}\\
			\hline
			10  : 3 sf  &\textbf{99\%} & 92\% & 98.3\% & 98.3\%\\
			\hline
			3 sf & 95\% & ND & \textbf{98.3\%} & ND\\
			\hline
			3  : 3 sf & 94\% & 67.2\% & \textbf{98.3\%} & \textbf{98.3\%}\\
			\hline
			2 : 3 sf & 92.2\% & 97.4\% & \textbf{98.3\%} & \textbf{98.3\%}\\
			\hline
			3 : 3 : 3 sf & 98.3\% & \textbf{99.1\%} & 97.4\% & 68.9\%\\
			\hline
			\multicolumn{5}{|c|}{PMC}\\
			\hline
			Architectures & $\emptyset$ & \emph{ReLU} & bn & bn + \emph{ReLU}\\
			\hline
			10  : 3 sf  & 96\% & 95.6\% & 97\% & \textbf{98.3\%}\\
			\hline
			3 sf & 97.4\% & ND & \textbf{98.3\%} & ND\\
			\hline
			3  : 3 sf & 95.6\% & 94.8\% & \textbf{98.3\%} & \textbf{98.3\%}\\
			\hline
			2 : 3 sf & 96.5\% & 94.8\% & \textbf{98.3\%} & 97.4\%\\
			\hline
			3 : 3 : 3 sf & 95.7\% & 94\% & \textbf{98.3\%} & \textbf{98.3\%}\\
			\hline
		\end{tabular}
		\normalsize
		\caption{\emph{Easy} : Comparaison de l'impact de \emph{ReLU} et de la batch normalisation}
		\label{table:releasy}
	\end{table}
	
	\begin{table}[H]
		\vspace*{-0.7cm}
		\center
		\small
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\multicolumn{5}{|c|}{GeoPMC}\\
			\hline
			Architectures &  $\emptyset$ & \emph{ReLU} & bn & bn + \emph{ReLU}\\
			\hline
			10  : 3 sf  & 92\% & \textbf{94.6\%} & 93\% & 92\%\\
			\hline
			3 sf & \textbf{93.3\%} & ND & 92.4\% & ND\\
			\hline
			3  : 3 sf & 92.6\% & \textbf{93.3\%} & 91.9\% & 92.6\%\\
			\hline
			2 : 3 sf & \textbf{92.6\%} & 89.6\% & 92\% & 92\%\\
			\hline
			3 : 3 : 3 sf & 90\% & 86\% & \textbf{92.6\%} & 90\%\\
			\hline
			\multicolumn{5}{|c|}{PMC}\\
			\hline
			Architectures & $\emptyset$ & \emph{ReLU} & bn & bn + \emph{ReLU}\\
			\hline
			10  : 3 sf  & \textbf{93.3\%} & 92.6\% & 91\% & 92\%\\
			\hline
			3 sf & \textbf{92\%} & ND & \textbf{92\%} & ND\\
			\hline
			3  : 3 sf & \textbf{92.6\%} & \textbf{92.6\%} & 92\% & 92\%\\
			\hline
			2 : 3 sf & 92.6\% & 92.6\% & 92\% & \textbf{93\%}\\
			\hline
			3 : 3 : 3 sf & \textbf{93.3\%} & 48\% & 92.6\% & 92.2\%\\
			\hline
		\end{tabular}
		\normalsize
		\caption{\emph{Dif} : Comparaison de l'impact de \emph{ReLU} et de la batch normalisation}
		\label{table:reldif} 
	\end{table}
	
	De manière générale, la présence du \emph{ReLU} semble dégrader davantage la qualité des prédictions sur les réseaux à couches hypersphériques, d'autant plus si le nombre de couches cachées croît. En conséquence, le \emph{ReLU} ne semble pas être la fonction d'activation la plus adaptée sachant que $\tilde{s}._i\tilde{x}$ peut être un scalaire très négatif (cf. figure \ref{fig:ps}). Une étude supplémentaire s'avère nécessaire pour en trouver une plus appropriée\footnote{Cette question justifie le travail sur les fonctions d'activation qui fait partie du chapitre suivant de la thèse.}. 
	
	En ce qui concerne la batch normalisation, elle permet dans le cas des données de \emph{Easy}, d'améliorer considérablement les résultats. Son utilisation semble moins pertinente pour les données issues du fichier \emph{Dif}.\\
	
	En regardant maintenant les valeurs de la fonction de perte (\emph{loss}) sur le jeu de données de validation (figure \ref{fig:easyloss} et \ref{fig:difloss}), il est possible d'observer que les modèles GeoPMC convergent plus vite vers des valeurs plus petites.
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[height=9cm,
				width=14cm,
				grid= major ,
				xlabel = {Epoch} ,
				ylabel = {Loss} ,
				xmax=150,
				legend entries={geoPMC : 3 sf, geoPMC : 10 r 3 sf,  geoPMC : b 2 r b 2 r b 3 sf,  PMC : 3 sf, PMC : 10 r 3 sf,  PMC : b 2 r b 2 r b 3 sf,  },
				legend style={at={(0.5,0.5)},anchor=south west,nodes={scale=0.6, transform shape}}],
				]
				\addplot[red,mark=none] table [x=Step, y=Value,col sep=comma]
				{data/easy_geoPMC_3_loss.csv};
				\addplot[blue,mark=none] table [x=Step, y=Value,col sep=comma]
				{data/easy_geoPMC_10_3_relu_loss.csv};
				\addplot[black, mark=none] table [x=Step, y=Value,col sep=comma]
				{data/easy_geoPMC_2_2_3_bn_relu_loss.csv};
				\addplot[red, mark=none,dashed] table [x=Step, y=Value,col sep=comma]
				{data/easy_PMC_3_loss.csv};
				\addplot[blue,mark=none,dashed] table [x=Step, y=Value,col sep=comma]
				{data/easy_PMC_10_3_relu_loss.csv};
				\addplot[black,mark=none,dashed] table [x=Step, y=Value,col sep=comma]
				{data/easy_PMC_2_2_3_bn_relu_loss.csv};
				
				{font=\tiny} 		\end{axis}
		\end{tikzpicture}
		\caption{\emph{Easy}: Courbes des fonctions de pertes }
		\label{fig:easyloss}
	\end{figure}
	
	\begin{figure}[H]
		\vspace*{-0.5cm}
		\centering
		\begin{tikzpicture}
			\begin{axis}[height=9cm,
				width=14cm,
				grid= major ,
				xlabel = {Epoch} ,
				ylabel = {Loss } ,
				xmax=150,
				legend entries={ geoPMC : 3 sf, geoPMC : 10 r 3 sf,  geoPMC : b 2 r b 2 r b 3 sf,  PMC : 3 sf, PMC : 10 r 3 sf,  PMC : b 2 r b 2 r b 3 sf,  },
				legend style={at={(0.5,0.5)},anchor=south west,nodes={scale=0.6, transform shape}}],
				]
				\addplot[red,mark=none] table [x=Step, y=Value,col sep=comma]
				{data/dif_geoPMC_3_loss.csv};
				\addplot[blue,mark=none] table [x=Step, y=Value,col sep=comma]
				{data/dif_geoPMC_10_3_relu_loss.csv};
				\addplot[black, mark=none] table [x=Step, y=Value,col sep=comma]
				{data/dif_geoPMC_2_2_3_bn_relu_loss.csv};
				\addplot[red, mark=none,dashed] table [x=Step, y=Value,col sep=comma]
				{data/dif_PMC_3_loss.csv};
				\addplot[blue,mark=none,dashed] table [x=Step, y=Value,col sep=comma]
				{data/dif_PMC_10_3_relu_loss.csv};
				\addplot[,black,mark=none,dashed] table [x=Step, y=Value,col sep=comma]
				{data/dif_PMC_2_2_3_bn_relu_loss.csv};
				
			\end{axis}
		\end{tikzpicture}
		\caption{\emph{Dif}: Courbes des fonctions de pertes}
		\label{fig:difloss}
	\end{figure}
	
		
	
\subsection{Illustration géométrique des résultats}
\label{sec:preimage}
En considérant $\Omega \subset \mathbb{R}^2$ comme le domaine initial sur lequel le modèle est évalué, la pré-image est définie comme un sous-ensemble $\Omega$ associé à un ensemble d'éléments de sortie définis par $\displaystyle \tilde{s_j} ._i \tilde{\Phi}(\mathbf{x})$. En d'autres termes, pour une hypersphère $\tilde{s_j}$, la pré-image est définie par :
\[ \{ \mathbf{x} \in \Omega \subset \mathbb{R}^2 \mid \tilde{s_j} ._i \tilde{\Phi}(\mathbf{x}) = y \} \]
où $y \in \mathbb{R}$ est une constante. Dans ce contexte, les courbes de niveau du produit $\displaystyle \tilde{s_j} ._i \tilde{\Phi}(x_k)$, où $\tilde{s_j}$ est un vecteur associé à une hypersphère et $\tilde{\Phi}(\mathbf{x_k})$ est la représentation de l'exemple $\mathbf{x_k}$ dans un espace transformé (tout le réseau hormis la dernière couche), peuvent être interprétées comme des pré-images. Les lignes correspondant aux frontières de décision correspondent à l'ensemble des points où le produit $\displaystyle \tilde{s_j} ._i \tilde{\Phi}(\mathbf{x_k})$ est nul.

Comme les expérimentations sont effectuées sur des données 2D, il est possible de visualiser la pré-image d'un neurone hypersphérique paramétré par une hypersphère $s$ en affichant 
les bandes de niveau pour les valeurs positives de $\displaystyle \tilde{\Phi (x)}._i \tilde{s}$ (figure \ref{fig:3sfband} et \ref{fig:b3rpre}).
%Pour chaque point $\mathbf{x}=(x_1, x_2)$ de la grille d'affichage, on infère l'entrée $\Phi (x)$ de la couche à afficher pour $\mathbf{x}$ et on calcule la valeur de $\displaystyle \Phi (x)._i s$. 


\begin{figure}[H]
	\begin{subfigure}{.46\linewidth}
		\begin{center}
			\includegraphics[height=5cm]{images/easy_geomPMC_3_sphere.png}
			\caption{$\tilde{s}._i\tilde{x}=0$}
			\label{fig:3sf}
		\end{center}
	\end{subfigure} \hfill
	\begin{subfigure}{.46\linewidth}
		\begin{center}
			\includegraphics[height=5cm]{images/easy_geomPMC_3_frdec.png}
			\caption{\mbox{$x .s > 0$}}
			\label{fig:3sfband}
		\end{center}
	\end{subfigure}
	\caption{\emph{Easy} : \mbox{Bandes de niveaux pour geomPMC "3 sf"}}
\end{figure}	

Si l'hypersphère à afficher est située dans la couche 0 ({\it i.e.} pas de non-linéarité) (cf. figure \ref{fig:b3r}) ou qu'il n'existe pas de couches cachées (voir figure \ref{fig:3sf}), les bandes de niveau sont circulaires concentriques. \\


\begin{figure}[H]
	\begin{subfigure}{.45\linewidth}
		\begin{center}
			\includegraphics[height=5cm]{images/dif_geomPMC_3_3_bn_relu_sphere.png}
			\caption{Sphères de la couche 0}
			\label{fig:b3r}
		\end{center}
	\end{subfigure} \hfill
	\begin{subfigure}{.45\linewidth}
		\begin{center}
			\includegraphics[height=5cm]{images/dif_geomPMC_3_3_bn_relu_pi.png}
			\caption{Pré-images de la couche 2}
			\label{fig:b3rpre}
		\end{center}
	\end{subfigure}
	\caption{\emph{Dif} : Bandes de niveaux pour\\
		geomPMC "b 3 r : b 3 r : b 3 sf"}

\end{figure}

La figure \ref{fig:comp1} permet d'illustrer la proximité entre les pré-images des hypersphères (situées avant la couche softmax) et les frontières de décision. Cela peut paraître évident au premier abord, mais si l'on se réfère à la figure \ref{fig:3sfband}, on sait que la frontière de décision entre deux hypersphères, en l'absence de non-linéarité, est un hyperplan...
Il convient également de noter que les centres des hypersphères (colonne de gauche en jaune) diffèrent des centres des classes (colonne de droite).

\begin{figure}[h]
	\begin{subfigure}{\linewidth}
		\includegraphics[width=8cm]{figs/easy_geomPMC_3_3_3_relu_pi.png}
		\includegraphics[width=8cm]{figs/easy_geomPMC_3_3_3_relu_frdec.png}
		\caption{\emph{Easy} : geoPMC "3 r :  3 r : 3 sf"}
	\end{subfigure}
	\hfill
	\begin{subfigure}{\linewidth}
		\begin{center}
			\includegraphics[width=8cm]{figs/dif_geomPMC_10_3_3_pi.png}\hfill
			\includegraphics[width=8cm]{figs/dif_geomPMC_10_3_3_relu_frdec.png}
		\end{center}
		\caption{\emph{Dif} : geoPMC "10 r :  3 r : 3 sf"}
	\end{subfigure}
	
	\caption{Pré-images couche 2 (gauche) et frontières de décision (à droite)}
	\label{fig:comp1} 
\end{figure}


La différence entre deux modèles GeoPMC et PMC de même architecture est flagrante si l'on compare leurs frontières de décision (cf. figure \ref{compfront}). Si l'on regarde leur forme, PMC correspond à une succession de lignes brisées, alors que GeoPMC produit une succession d'arcs de cercle.

\begin{figure}[h]
	\begin{subfigure}{.45\linewidth}
		\begin{center}
			\includegraphics[width=7cm]{figs/dif_PMC_3_3_relu_frdec.png}
			\caption{PMC "3 r :  3 r : 3 sf"}
		\end{center}
	\end{subfigure} 
	\begin{subfigure}{.45\linewidth}
		\begin{center}
			\includegraphics[width=7cm]{figs/dif_geomPMC_3_3_relu_frdec.png}
			\caption{GeoPMC "3 r :  3 r : 3 sf"}
		\end{center}
	\end{subfigure}
	\caption{\emph{Dif} : Comparaison des frontières de décision}
	\label{compfront}
\end{figure}

\subsection{Réseaux à couches convolutives {\it vs} convolutions hypersphériques}

Cette section propose une comparaison entre les filtres hypersphériques et Conv2d sur le jeu de données MNIST.
Il s'agit d'une base de 70000 mini-images étiquetées, en niveaux de gris, de taille $28 \times 28$ représentant l'écriture manuscrite des chiffres de 0 à 9. C'est donc un problème de reconnaissance à 10 classes.\\

Dans cette expérimentation, une architecture est testée avec la séquence de couches suivante~:
\begin{enumerate}
	\item  une batch normalization ou non
	\item  une couche hypersphérique pour GeoPMC / une couche Dense pour PMC / une couche à filtres hypersphériques pour GeoConv2d /  une couche Conv2d pour Conv2d avec un certain nombre de filtres de taille $ 3 \times 3$
	\item  une fonction \emph{ReLU} ou non
	\item  une batch normalization ou non
	\item  une couche Dense ou hypersphérique avec un certain nombre de neurones
	\item  une activation softmax
\end{enumerate}
Le tableau \ref{table:MNIST} donne les résultats d'\emph{accuracy} pour l'ensemble des configurations testées.

\begin{table}[htbp]
	\vspace*{-0.15cm}
	\center
	\small
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{}&\multicolumn{2}{|c|}{Dense}&\multicolumn{2}{|c|}{Convolution}\\
		\hline
		\multicolumn{1}{|c|}{Architectures} & GeoPMC & PMC & GeoConv2d & Conv2d\\
		\hline
		\hline
		10 sf & \textbf{92.7\%} & \textbf{92.7\%} & \multicolumn{2}{|c|}{ND} \\
		\hline
		\hline
		32 : 10 sf & 93\% & 92.6\% & \textbf{97.5\%} & \textbf{97.5\%}\\
		\hline
		32 r : 10 sf & 10\% & 96.8\% & 98\% & \textbf{98.2\%}\\
		\hline
		b 32 : b 10 sf & 86.4\% & 91.6\% & \textbf{97.2\%} &\textbf{ 97.2\%}\\
		\hline
		b 32 r : b 10 sf & 10\% & 96\% & 97.8\% & \textbf{98.2\%}\\
		\hline
		\hline
		512 : 10 sf & 93\% & 93\% & \textbf{97\%} &\textbf{ 97\%}\\
		\hline
		512 r : 10 sf & 10\% & \textbf{98\%} & \textbf{98\%} & \textbf{98\%}\\
		\hline
		b 512 : b 10 sf & 93\% & 93\% &\textbf{ 97.7\%} & 97.1\%\\
		\hline
		b 512 r : b 10 sf & 10\% & 98\% & 97.7\% & \textbf{98.3\%}\\
		\hline
	\end{tabular}
	\normalsize
	\caption{MNIST : Comparaison de l'\emph{accuracy} pour différents modèles}
	\label{table:MNIST}
\end{table}


Une première observation concerne l’incapacité de GeoPMC à fonctionner en "grande" dimension avec une activation \emph{ReLU}. Le taux d'erreur de 90\% laisse à penser que le classifieur prédit toujours la même classe. Le problème provient, selon nous, d'une mauvaise initialisation des hypersphères qui fait que tous les points sont à l'extérieur de toutes les sphères\footnote{Ce constat justifie le travail d'initialisation fait au paragraphe suivant.}. Par la suite, le \emph{ReLU} annulera tout signal vers les couches suivantes du réseau. Ce problème ne se produit certainement pas pour GeoConv2d
car les hypersphères sont définies en dimension 9 au lieu de 784. Plus généralement, il serait pertinent de se demander si les couches hypersphériques sont sensibles à la malédiction de la dimension.

On constate la même amélioration lors du passage des modèles GeoPMC vers GeoConv2d que celui de PMC vers Conv2d. D’après nos premières expériences, GeoConv2d
et Conv2d produisent des résultats identiques.

\section{Une initialisation appropriée}
\label{sec:pbinitdistrib}

L'initialisation des paramètres dans une couche comportant plusieurs neurones hypersphériques représente un défi complexe, mais qu'il fallait relever au vu des expérimentations menées précédemment\footnote{Il nous faut signaler ici que les résultats présentés dans ce paragraphe ont été obtenus en fin de thèse.}.

Pour se convaincre encore de l'importance de la question de l'initialisation, on peut aussi évoquer dans ce préambule le comportement du volume d'une hypersphère en grande dimension.
	On rappelle en effet que le volume d'une hypersphère en fonction de la dimension $n$ pour un rayon $\rho$ fixé est le suivant~:
	
	$$V(\rho) = \frac{\pi^{\frac{1}{2} n} \rho^n}{\Gamma\left(\frac{1}{2} n + 1\right)} .$$
	On peut constater dans la figure \ref{fig:vol_hs} que le volume d'une hypersphère tend rapidement vers zéro lorsque la dimension $n$ de l'espace des données augmente. Pour un rayon $\rho$ proche de 1, le volume devient quasiment nul dès que la dimension atteint 40. Cela devient problématique pour les données de dimensions encore plus élevées et peut donc compliquer les étapes d'initialisation. En effet, comme le volume décroît fortement avec l'augmentation de la dimension, à la limite, tout point de l'espace se situe en dehors des hypersphères comme l'on peut le voir sur la figure \ref{fig:prod_fct_dim} (le produit $\tilde{s} ._i \tilde{x}$ décroît linéairement vers des valeurs négatives au fur et à mesure que la dimension augmente). 
	
	\begin{figure}[H]
		\includegraphics[keepaspectratio,width=16cm]{figs/volume_hs.pdf}
		\caption{Volume d'une hypersphère en fonction de la dimension de l'espace des données (pour des rayons fixés : ici 0.8, 1 et 1.2)}
		\label{fig:vol_hs}
	\end{figure}
	
	\begin{figure}[H]
		\includegraphics[keepaspectratio,width=16cm]{figs/sx_dim_vol.pdf}
		\caption{Produit $\tilde{s} ._i \tilde{x}$ en fonction de la dimension de l'espace des données}
		\label{fig:prod_fct_dim}
	\end{figure}
	


Le paragraphe est organisé de la façon suivante. \\

Nous commençons au paragraphe \ref{etat} par une présentation des méthodes aujourd'hui les plus utilisées pour l'initialisation des réseaux à couches denses. 
C'est en effet de ces méthodes que nous avons décidé de nous inspirer, en particulier de l'approche heuristique de Glorot et Bengio \cite{glorot}. 

D'autres stratégies d'initialisation auraient pu être envisageables. Par exemple, à l'instar des réseaux utilisant des fonctions radiales, il est possible de déterminer les centres à l'aide d'un algorithme de clustering et d'ajuster les rayons en fonction de la dispersion des données autour de ces centres. Cependant, ce type d'approche souffre de deux défauts. D'abord, il est crucial de ne pas confondre les centres des hypersphères avec ceux des clusters. Ensuite, et surtout, une telle stratégie qui sépare les centres et les rayons des sphères et en plus les traite avec des outils totalement différents, va à l'encontre de la philosophie de cette thèse, basée sur des outils d'algèbre géométrique et un apprentissage le plus direct possible.\\

Nous choisissons donc une stratégie de normalisation à la Glorot. 
Pourtant, et c'est l'objet du paragraphe \ref{commedhab} suivant, nous montrons rapidement que la méthode usuelle n'est pas du tout appropriée. Nous en illustrons les conséquences. Nous identifions également la cause : une distribution normale passée dans un neurone hypersphérique ne suit plus une loi normale.

Le paragraphe \ref{var} présente les calculs de variance qui justifient la pertinence de la méthode d'initialisation que nous allons proposer. La loi de probabilité adaptée à la structure des neurones hypersphériques est la loi Gamma Généralisée. Une difficulté est que sa complexité permet très peu de calculs explicites et que ses propriétés sont mal connues. Nous la contournons en utilisant quelques propriétés asymptotiques des lois Gamma Généralisées.

Le paragraphe \ref{var} est à la fois très calculatoire et très probabiliste. 
Le lecteur peut passer les détails et aller directement en Section \ref{expe_init} pour une version synthétique des règles d’initialisation obtenues et pour des expérimentations qui permettent à la fois de confirmer la pertinence de ces règles et d’illustrer les arguments mathématiques sous-jacents.  
Un test de la méthode d'initialisation proposée est présenté au paragraphe \ref{expe_real}, sur des données synthétiques suivant une distribution normale, composées de points en dimension 4.

\subsection{Initialisation {\it à la Glorot}}\label{etat}

L'initialisation des réseaux de neurones est une question qui a été largement explorée.
Dans ce manuscrit, nous ne présentons pas d'état de l'art mais faisons un focus sur un type d'approche, dont des exemples emblématiques sont les travaux de Glorot \cite{glorot} (adaptés aux fonctions d'activation type tangente hyperbolique) ou de He \cite{KaimingHe} (adaptés aux fonctions d'activation \texttt{\emph{ReLU}}).
Comme déjà mentionné, nous pensons en effet ces stratégies plus adaptables aux réseaux hypersphériques. 
Pour une revue de l'état de l'art, nous mentionnons par exemple les articles \cite{DBLPinit}, \cite{DEPATER2023579}, ainsi que \cite{LIHuimin}.\\

Par initialisation {\it à la Glorot}, nous désignons des stratégies consistant à une forme de normalisation (voir aussi LeCun et al. \cite{LeCun2012}) des points du réseau sensée garantir la propagation d'un signal à travers les couches profondes, en évitant qu'il ne se déforme trop. Typiquement on ne veut pas que l'amplitude du signal explose ou au contraire tende vers zéro.
Simultanément l'approche vise à maintenir les poids dans une plage de valeurs raisonnable.


En pratique, si la distribution des éléments $x_i$, $i \in \{1, \cdots, n\}$, en entrée du réseau suit une loi normale, on choisit la distribution des poids du réseau de telle sorte que les éléments $y_j$, $j \in \{1, \cdots, m\}$, en sortie du réseau suivent la même loi normale (même moyenne, même variance).
Pour que les calculs soient faisables, on suppose que les poids du réseau suivent également une loi normale.\\

Nous allons détailler les approches de Glorot \cite{glorot} et He \cite{KaimingHe} dans les lignes qui suivent.
L'objectif est de comprendre ces méthodes dans le contexte classique des réseaux de neurones avant de les adapter au cas plus complexe des neurones hypersphériques. 


\subsubsection{Glorot et He}

On se place donc ici dans le cas `classique'' où la sortie d'un neurone de type dense est donnée par la forme linéaire suivante~: 

\begin{equation}
	\displaystyle y_j = \sum_{i=1}^{n} w_{ji} x_i
	\label{eq:sortie_classique_he}
\end{equation}
les $w_{ji}$ désignant les poids du réseau dont l'initialisation doit être choisie.

\bigskip

\begin{itemize}
    \item[$\bullet$] On suppose que $x_i$, $1\le i \le n$, sont des variables aléatoires indépendantes et identiquement distribuées, suivant la loi normale de moyenne $\mu_x = 0$ et d'écart-type $\sigma_x$.
    \item[$\bullet$] On veut que les sorties $y_j$, $1\le i \le m$, suivent encore loi normale de moyenne $\mu_x = 0$ et d'écart-type $\sigma_x$.
\end{itemize}

On fait le pari qu'on va arriver à nos fins en faisant des hypothèses relativement simple sur la structuration des poids :
\begin{itemize}
    \item[$\bullet$] On suppose que $w_{ji}$, $1\le j \le m$, $1\le i\le n$, sont des variables aléatoires indépendantes et identiquement distribuées, suivant la loi normale de moyenne $\mu_w = 0$ et d'écart-type $\sigma_w$.
\end{itemize}

\bigskip
On va maintenant calculer la variance des $y_j$, $1\le i \le m$, définis par \eqref{eq:sortie_classique_he} et jouer sur le seul paramètre libre du problème, c'est-à-dire $\sigma_w$ pour arriver à nos fins.

\bigskip

\noindent{\bf Remarque.} 
Pour la suite, au vu de la formule \eqref{eq:sortie_classique_he}, on a besoin de calculer en particulier la variance de produits de variables aléatoires indépendantes. On rappelle donc que si  $X$ et $Y$ sont des variables aléatoires indépendantes, alors\footnote{dans la suite var désigne toujours la variance et $E$ l'espérance}
\begin{equation}
	\text{var}(XY) = \mathbb{E}^2(X)  \text{var}(Y) + \mathbb{E}^2(Y)  \text{var}(X) + \text{var}(X)  \text{var}(Y).
	\label{eq:varianceformuleindependant}
\end{equation}
En effet, on a
\begin{eqnarray*}
    \text{var}(XY)  &=& \mathbb{E}(X^2Y^2)-\mathbb{E}^2(XY)
	 = \underbrace{\text{cov}(X^2,Y^2)}_{0  \text{  car X, Y indépendant. }}+\mathbb{E}(X^2)\mathbb{E}(Y^2)-\mathbb{E}^2(XY)
	 \\
	 &=& \left[\text{var}(X)+\mathbb{E}^2(X)\right] \left[\text{var}(Y)+\mathbb{E}^2(Y)\right] - \mathbb{E}^2(XY)
	\\
	 &=& \text{var}(X) \text{var}(Y)+ \mathbb{E}^2(X) \text{var}(Y)+\mathbb{E}^2(Y) \text{var}(X) + \underbrace{\mathbb{E}^2(X) \mathbb{E}^2(Y)-\mathbb{E}^2(XY)}_{0 \text{  car X, Y sont indépendant. }}
\end{eqnarray*}\\

\newpage
\noindent \underline{Premier cas : sans fonction d'activation.}\\ 


La sortie d'un neurone est exactement donnée par \eqref{eq:sortie_classique_he}.
On a donc 
$$\displaystyle \text{var}\left(y_j\right)= \text{var}\left(\sum_{i=1}^{n}w_{ji} x_i\right)=\sum_{i=1}^{n}\text{var}\left(w_{ji} x_i\right) $$
En appliquant la formule  établie dans la remarque précédente, on obtient
\begin{equation*}
	\operatorname{var}(y_j) = \sum_{i=1}^{n} \operatorname{var}(w_{ji})  \operatorname{var}(x_i)
\end{equation*}
Les distributions étant identiques, on arrive finalement à  : 
$$\displaystyle \text{var}\left(y_j\right)=n \sigma_w^2 \sigma_x^2.$$

Puisqu'on veut $\text{var}\left(y_j\right) = \sigma_x^2$, on conclut que le résultat est obtenu si
$$ n \sigma_w^2 = 1.$$

Le travail n'est pas terminé. En effet pour l'apprentissage il faut aussi envisager l'étape de rétro-propagation.

Notons $L$ la fonction de perte. On veut maintenant que
$$ \text{var}\Bigl( \frac{\partial L}{\partial x } \Bigr) = \text{var} \Bigl(  \frac{\partial L}{\partial y } \Bigr).$$
Intuitivement, les gradients doivent conserver une certaine homogénéité au fil des passages entre couches.

Or $\displaystyle \frac{\partial L}{\partial x } = \frac{\partial L}{\partial y } \frac{\partial y}{\partial x }$ (dérivation en chaîne) où la différentielle $\displaystyle\frac{\partial y}{\partial x }$ se calcule facilement grâce à la formule \eqref{eq:sortie_classique_he} définissant $y$ comme une  fonction  linéaire de $x$. On a :
\begin{equation}
\frac{\partial L}{\partial x_i} = \sum_{j=1}^m   w_{ji} \frac{\partial L}{\partial y_j}.
\label{eq:sortie_classique_backprop}
\end{equation} 

On remarque facilement la similitude des définitions \eqref{eq:sortie_classique_he} et \eqref{eq:sortie_classique_backprop}. Comme la moyenne des $w_{ji}$ est nulle ($\mu_w=0$), les moyennes des autres termes dans \eqref{eq:sortie_classique_backprop} le sont aussi. Pour le calcul des variances, seul le nombre de termes dans la somme change. On arrive donc à la condition
$$ m \sigma_w^2 = 1.$$

Si $m\ne n$, il faut finalement faire une hypothèse sur $\sigma_w$ qui permette de faire un compromis entre les conditions $ n \sigma_w^2 = 1$ et $ m \sigma_w^2 = 1$.
On aboutit ainsi à la condition sur l'initialisation des poids suivante :

\begin{equation}
	\displaystyle \sigma_w^2=\frac{2}{n+m} .
	\label{eq:regle_glorot}
\end{equation}\\

\noindent{\bf Remarque.} On aurait pu aussi supposer que la distribution des poids suit une loi uniforme sur $\left[-r_w,r_w\right]$. On a alors var$\left(w_j\right)=r_w^2/3$, si bien que la condition ci-dessus devient
$r_w = \sqrt{6/(n+m)}$. 


\bigskip
\newpage
\noindent \underline{Deuxième cas : avec fonction d'activation.}\\ 

Dans ce cas, l'équation \eqref{eq:sortie_classique_he} est remplacée par
$$ y_j = f\Bigl( \sum_{i=1}^{n} w_{ji} x_i \Bigr)$$
où $f$ désigne la fonction d'activation.
Les calculs de variances sont alors bien sûr plus techniques. 

Néanmoins, si la fonction $f$ est régulière, on peut l'approcher par une fonction linéaire {\it via} son développement limité en 0 (puisque les entrées sont supposées de moyenne nulle). Les calculs ci-dessus ne sont donc que très peu modifiés. On peut le faire par exemple si la fonction d'activation $f$  est une fonction logistique ou une tangente hyperbolique : au voisinage de 0,  $\tanh{x}\approx x$ (résultat ci-dessus inchangé), $1/1+e^{-x}\approx 1/2+x/4$ (résultat ci-dessus inchangé à la constante près). 

Si la fonction $f$ n'est pas régulière, mais si elle suffisamment simple pour calculer les intégrales correspondant à la définition des variances, on peut également adapter les calculs précédents : c'est le cas par exemple de la fonction \emph{ReLU}.

Les résultats sont résumés ci-dessous.
	


\bigskip


\noindent \underline{Synthèse}\\

Le tableau  \ref{tab:bilan_init_formule} présente une synthèse des résultats concernant les écarts types et les bornes de l'intervalle considéré, en fonction de l'utilisation de la loi normale ou uniforme pour l'initialisation des poids.\\

\begin{table}[h!]
\centering
	\begin{tabular}{|c|c|c|}
		\hline
		activation & Loi normale & Loi uniforme \\ 
		\hline
		- & $\sigma_w = \sqrt{\frac{2}{n + m}}$ (Glorot) & $r_w = \sqrt{\frac{6}{n + m}}$ (Glorot) \\ 
		\hline
		\emph{Tanh} & $\sigma_w = \sqrt{\frac{2}{n + m}}$ (Glorot) & $r_w = \sqrt{\frac{6}{n + m}}$ (Glorot) \\ 
		\hline
		\emph{Sigmoïde} & $\sigma_w = 4  \sqrt{\frac{2}{n + m}}$ (Glorot) & $r_w = 4  \sqrt{\frac{6}{n + m}}$ (Glorot) \\ 
		\hline
		\emph{ReLU} & $\sigma_w = \sqrt{2}  \sqrt{\frac{2}{n + m}}$ (He) & $r_w = \sqrt{6}  \sqrt{\frac{6}{n + m}}$ (He) \\ 
		\hline
	\end{tabular}
	\caption{Bilan des formules pour les paramètres d'initialisation}
	\label{tab:bilan_init_formule}
\end{table}


\subsection{Inefficacité de l'initialisation classique}\label{commedhab}

Notre stratégie va consister à adapter l'approche présentée au paragraphe précédent pour des couches hypersphériques, {\it i.e.} des sorties de la forme
\begin{equation}
	\displaystyle y_{j} = \sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\rho^{2}_{j} 
\label{formulekilmefaut}
\end{equation}
Les calculs sont bien sûr rendus plus techniques par la forme quadratique de la sortie hypersphérique.
Ils sont détaillés au paragraphe suivant. 

\bigskip

Mais on va d'abord voir qu'on ne peut pas se placer dans un cadre aussi simple que celui qui précède. Plus précisément, travailler sur des lois normales et centrées ne permet pas d'obtenir une initialisation pertinente.

Nous allons en donner une illustration pratique.
Puis nous le justifierons par des arguments probabilistes.

\bigskip


\subsubsection{Expérimentation}

Nous considérons des données, les entrées $x_i$, $1\le i \le n$, qui suivent une loi normale et centrée.
Pour l'initialisation des hypersphères, 
les centres sont initialement tirés aléatoirement selon une loi normale centrée, avec un écart-type ajusté en fonction de la dimension $n$ et du nombre de neurones dans la couche\footnote{Bien sûr l'écart-type n'est pas ajusté selon la règle \eqref{eq:regle_glorot} obtenu pour des couches classiques ; ils respectent la règle établie au paragraphe \ref{var} mais dans le cas où d'une loi normale {\bf centrée}.}. 
Les rayons des hypersphères sont initialisés à 1.


On peut observer la propagation du signal à travers les trois premières couches à la Figure \ref{fig:distrib_out_dense_hs}. 


\begin{figure}[H]
	\begin{center}		
		\includegraphics[width=10cm]{figs/out_densesph.png}
		\caption{Distribution de l'entrée et valeurs de sortie des couches hypersphériques}
		\label{fig:distrib_out_dense_hs}
	\end{center}
\end{figure}

Clairement, l'initialisation n'est pas efficace\footnote{On rappelle que le but était de maintenir le signal dans une plage de valeurs contrôlées et éviter des divergences numériques.}.
\\
On observe une explosion de l'écart-type des valeurs dès la sortie des premières couches hypersphériques.


Ce phénomène se manifeste également dans les couches à filtre de convolution hypersphériques. La figure \ref{fig:distrib_out_conv_hs} illustre la distribution des données d'entrée et de sortie un tenseurs de taille $(3,1,28,28)$ initialisés selon une loi normale. Cela correspond, par exemple, à un batch de trois images de taille $28 \times 28$. Chaque colonne présente un tenseur différent, tandis que chaque ligne montre l'évolution des distributions à travers les couches successives, depuis la première couche jusqu'à la quatrième couche à filtre de convolution hypersphérique.

\begin{figure}[H]
	\begin{center}		
		\includegraphics[width=16cm]{figs/out_conv.png}
		\caption{Distribution de l'entrée et valeurs de sortie des couches à filtre hypersphériques}
		\label{fig:distrib_out_conv_hs}
	\end{center}
\end{figure}

	
\bigskip

Dans ces expérimentations, au-delà de l'explosion des écarts type, un élément frappant est le fait qu'on perd très vite la symétrie de la distribution initiale.
En fait on voit dans la figure \ref{fig:distrib_out_dense_hs} que la distribution à la sortie de la première couche ne suit déjà plus une loi normale. 
L'hypothèse de normalité de la distribution au fil des couches étant à la base de tous les calculs précédents, c'est évidemment un souci : on sait conserver la moyenne et la variance d'une distribution normale au travers de la couche hypersphérique mais dès la première sortie la distribution n'est plus normale et plus rien ne sera assuré à partir de la deuxième couche.
	

\bigskip

Ce point peut être confirmé par les éléments de probabilités fondamentales qui suivent. 
Il est la principale difficulté de ce travail d'initialisation.

\bigskip 

\subsubsection{Eléments de probabilités}

La densité de probabilité d'une loi normale d'espérance $\mu$ et d'écart-type $\sigma$ est donnée par
$$ x \mapsto \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\bigl(\frac{x-\mu}{\sigma}\bigr)^2}.$$
Si on compose cette fonction avec une fonction affine $x\mapsto wx+b$, caractéristique d'une couche dense classique, on obtient une densité
$$ x \mapsto \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\bigl(\frac{wx + b -\mu}{\sigma}\bigr)^2}$$
qui correspond toujours à une loi normale. L'approche {\`a la Glorot} a du sens : une distribution normale est transformée par la couche dense en une distribution normale.
Mais si la densité de Gauss est composée avec la fonction carrée, fonction qui apparaît dans l'opérateur hypersphérique ({\it cf. \eqref{formulekilmefaut})}, la structure de la loi normale disparaît :
$$ x \mapsto \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\bigl(\frac{x^2-\mu}{\sigma}\bigr)^2}.$$
La couche hypersphérique ne peut donc transformer une distribution normale en une distribution normale.
En fait (voir par exemple  \cite{Johnson}), si une variable aléatoire réelle $X$ suit une loi normale centrée d'écart-type $\sigma$, noté $X \sim \mathcal{N}(0,\sigma^2)$, alors $X^2$ suit une loi Gamma, $X^2 \sim G(1/2,2\sigma^4)$. Cette dernière n'est pas stable par le passage au carré : si $X$ suit une loi Gamma, alors $X^2$ suit une loi dite {\bf Gamma Généralisée}. 
On a enfin abouti à une loi suffisamment stable pour nos opérations hypersphériques car si $X$ suit une loi Gamma Généralisée alors toute puissance de $X^p$, $p\in \mathbb{N}^*$, suit aussi une loi Gamma Généralisée. 
Ce point est confirmé par la forme de la densité d'une loi Gamma Généralisée, donnée par une fonction
$$ x \mapsto \frac{(p/a^d)x^{d-1}e^{-(x/a)^p}}{\Gamma(d/p)},$$
où $d>0$ et $p>0$ sont des paramètres de forme, $a$ est un paramètre d'échelle et $\Gamma$ désigne la fonction Gamma.

La prépondérance de la distribution Gamma Généralisée aux sorties hypersphériques est aussi confirmée par la forme des sorties observées dans la Figure \ref{fig:distrib_out_dense_hs} (l'impression visuelle peut être confortée par des travaux de fitting tels que \cite{Wagener}).

\bigskip 

Pour les premières propriétés des lois Gamma Généralisées, nous renvoyons le lecteur par exemple à \cite{Morteza} ou \cite{Jiang2021TheGG} où la bibliographie est bien présentée.

Pour reproduire les calculs du paragraphe \ref{etat}, des propriétés plus spécifiques sont nécessaires. La difficulté est qu'il faut calculer la variance et la moyenne de produits et de sommes impliquant au moins une distribution Gamma Généralisée.\\

De ce point de vue :
\begin{itemize} 
    \item La somme de deux variables aléatoires réelles suivant deux lois Gamma Généralisées ne suit pas {\it a priori} une loi Gamma Généralisée (voir \cite{Soury})\footnote{On n'avait pas ce souci au paragraphe \ref{etat} puisque la somme de deux distributions normales suit une loi normale. Voir par exemple \cite{springer1979algebra}}.
    \item Le produit de deux variables aléatoires réelles suivant respectivement une loi Gamma Généralisée et une loi normale ne suit pas une loi Gamma Généralisée (voir \cite{Malik_1967}).
    \item Le même problème se pose en fait pour le produit de deux variables aléatoires réelles suivant une loi normale. Cependant, voir par exemple \cite{SeijasMacias}, les moments caractéristiques de la loi produit sont calculables et on peut donc la rapprocher d'une loi normale.
    \item Pour obtenir des formules explicites pour les calculs de variance, peu de choix s'offrent à nous. \\
    On peut penser à la méthode de Stein pour déterminer les fonctions densités de probabilité au fil des couches. On peut consulter \cite{Gaunt} pour des calculs de ce type dans le contexte des lois Gamma. On voit que c'est extrêmement technique. De plus, le calcul des variances et des moyennes {\it via} l'intégration des densités de probabilité doit ensuite être approché numériquement. Pour les premières briques dans le contexte Gamma Généralisée, on peut consulter \cite{Podolski}, \cite{Marques2012OnTP}, et \cite{MARQUES201655}.
    \\
    La question de l'approximation numérique est d'autant plus sensible que des distributions  Gamma Généralisées de forme très semblables peuvent avoir des paramètres $(a,d,p)$ très différents (voir \cite{lawless2011statistical}).
\end{itemize}

\bigskip

\subsubsection{Stratégie}

Au vu des éléments précédents, nous allons mettre en place la stratégie suivante :\\

\begin{itemize}
    \item[$\bullet$] Une loi  Gamma Généralisée est caractérisée par trois paramètres. La méthode {\it à la Glorot} consiste à en imposer seulement deux.\\
    L'idée est d'influer sur le degré de liberté restant pour forcer les distributions Gamma Généralisées en sortie de chaque couche à s'approcher d'une distribution normale.
    \\
    \item[$\bullet$] Pour parvenir à rapprocher chaque sortie d'une distribution normale, en grande dimension, nous serons aidés par le théorème central limite. Au vu de l'expérimentation précédente ce n'est évidemment pas suffisant.\\
    Afin de ne pas introduire dans le problème de variable supplémentaire, nous allons peser sur les moyennes et les variances.
    %\\
    %\item[$\bullet$] 
\end{itemize}

\bigskip 

Illustrons la méthode avec quelques calculs simples. On considère une variable aléatoire réelle $X$ suivant une loi Gamma Généralisée, $X \sim \mbox{GG}(a,d,p)$. On sait alors que $X^2 \sim \mbox{GG}(a^2,d/2,p/2)$.
\\
Comparons les moyennes :
\begin{eqnarray*}
&& \mathbb{E}(X) = a \frac{\Gamma\bigl(\frac{d+1}{p}\bigr)}{\Gamma\bigl(\frac{d+1}{p}\bigr)},
\\
&& \mathbb{E}(X^2) = a^2 \frac{\Gamma\bigl(\frac{d+2}{p}\bigr)}{\Gamma\bigl(\frac{d}{p}\bigr)} 
=\mathbb{E}(X) \times a \times \frac{\Gamma\bigl(\frac{d+2}{p}\bigr)}{\Gamma\bigl(\frac{d+1}{p}\bigr)} .
\end{eqnarray*}
Dans le cas où $p=1$ (c'est le cas où la loi Gamma Généralisée est une loi Gamma, en sortie de première couche si l'entrée est gaussienne par exemple) et pour un paramètre de forme $d\ge 1$ (ce qui est le cas observé), on voit que
$$ \mathbb{E}(X^2) = a(d+1) \mathbb{E}(X).$$
Ainsi, la moyenne se décale vers la droite. Cet effet indésirable s'observe d'ailleurs dans les sorties hypersphériques ci-dessus (avec un décalage vers la gauche à cause du signe ``$-$'' dans \eqref{formulekilmefaut}).
Un moyen de limiter ce décalage est de réduire la valeur de $a$. Et un moyen direct d'influer sur la valeur de $a$ est de réduire la variance de la distribution $X$.



\noindent Comparons maintenant les variances :
\begin{eqnarray*}
&& \mbox{var}(X) = a^2 \frac{\Gamma\bigl(\frac{d+2}{p}\bigr)\Gamma\bigl(\frac{d}{p}\bigr)-\Gamma\bigl(\frac{d+1}{p}\bigr)^2}{\Gamma\bigl(\frac{d}{p}\bigr)},
\\
&& \mbox{var}(X^2) = a^4 \frac{\Gamma\bigl(\frac{d+4}{p}\bigr)\Gamma\bigl(\frac{d}{p}\bigr)-\Gamma\bigl(\frac{d+2}{p}\bigr)^2}{\Gamma\bigl(\frac{d}{p}\bigr)}  .
\end{eqnarray*}
Une fois encore, réduire la valeur de $a$ permet de contrôler l'explosion de la variance.

Ces considérations sur la partie carrée de la sortie hypersphérique \eqref{formulekilmefaut} sont confortées par les travaux de Aroian et al. \cite{Aroian1978MathematicalFO} (voir aussi \cite{AntonioSeijas2012} pour des expérimentations numériques) qui établissent que le produit de $X \sim \mathcal{N}(\mu_x,\sigma_x^2)$ par $S \sim \mathcal{N}(\mu_s,\sigma_s^2)$ tend vers une distribution normale si
\begin{equation}
    \frac{\mu_x}{\sigma_x} + \frac{\mu_s}{\sigma_s} \gg 1 .
    \label{maline}
\end{equation}

\bigskip

\noindent{\bf En bref :} la stratégie consiste à assurer des sorties normales 
\begin{itemize}
    \item[$\bullet$] en diminuant la variance des données en entrée
    \item[$\bullet$] en initialisant les poids avec une moyenne non nulle
\end{itemize}
On reprend donc les calculs {\it à la Glorot} dans le paragraphe suivant, en les adaptant aux sorties hypersphériques, mais en conservant l'hypothèse de normalité.

	
\subsection{Initialisation hypersphérique par le contrôle des moyennes et variances}\label{var} 

Cette partie est très calculatoire. Tous les détails sont fournis pour assurer la reproductibilité des résultats. Nous invitons le lecteur qui veut aller directement aux résultats à consulter le paragraphe \ref{subsec:resume_init} suivant.

\bigskip

Comme dans l’approche classique, le calcul des variances entre les entrées et les sorties vise à établir une condition pour maintenir une variance constante du signal à travers les couches du réseau. En imposant l’égalité entre la variance des entrées et celle des sorties, on peut relier les  paramètres de l’hypersphère aux caractéristiques des données d’entrée. Cela permet d’imposer des contraintes sur les paramètres du modèle afin d’assurer une propagation adéquate du signal. L'objectif principal est de déterminer les valeurs d'initialisation des poids du réseau pour minimiser les variations indésirables du signal durant l'apprentissage. Ici, on fixe le rayon des hypersphères et on joue sur la distribution des centres.

\subsubsection{Contrôle pendant la propagation (contrôle {\it forward})}% : couches denses hypersphériques}

Pour l'initialisation, on introduit un paramètre supplémentaire $\delta >0$ dans le réseau. Ce paramètre nous permettra de gagner suffisamment de latitude pour parvenir à nos fins. 
En particulier\footnote{mais pas seulement...}, il va jouer le rôle de facteur d'échelle pour ramener les données à des quantités à variance petite (conformément à la stratégie construite ci-dessus).
On considère donc que les  éléments $y_j$ du vecteur de sortie $Y$, sont donnés par $y_{j}= \frac{\widetilde{ X}._i\widetilde{S}_j}{\delta}$, c'est-à-dire :
\begin{equation}
	\displaystyle y_{j} = \frac{1}{2\delta}\left[\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\rho^{2}_{j} \right] \qquad \forall 1 \le j \le m,
\end{equation}
le nombre $m$ étant le nombre de sphères (neurones hypersphériques) dans la couche.

\bigskip

On suppose les données identiquement distribuées suivant une loi normale : pour tout $i \in \{1, \ldots, n\}$,  $x_i \sim \mathcal{N}(\mu_x, \sigma_x^2)$.

On fait une hypothèse similaire pour les sorties $y_j$ et les poids du réseau $s_{ji}$ : pour tout $i \in \{1, \ldots, n\}$, tout $j \in \{1, \ldots, m\}$, $y_j \sim \mathcal{N}(\mu_y, \sigma_y^2)$, $s_{ij} \sim \mathcal{N}(\mu_s, \sigma_s^2)$.

Le rayon  $\rho$ est supposé fixe.



\bigskip 



\noindent{\bf Remarque.} 
Pour faciliter la lecture, on rappelle le résultat suivant. Soit $x$ une variable aléatoire  telle que $X \sim \mathcal{N}(\mu, \sigma^2)$. Les moments centrés peuvent être calculés en fonction des moments d'ordre $p$ en développant l'expression $(x-\mu)^p$ puis en appliquant les propriété de linéarité de l'espérance \cite{bogaert2020probabilites}. On a donc
$$
	\begin{array}{ll}
		\mathbb{E} \left[ (X-\mu)^{2} \right] & = \mathbb{E} \left[ X^2 - 2 \mu X + \mu^2 \right]\\
		\sigma^2 & =  \mathbb{E} \left[ X^2 \right] - 2 \mu \textcolor{red}{ \mathbb{E} \left[ X \right]} +    \mu^2 \\
		\sigma^2 & =  \mathbb{E} \left[ X^2 \right] - 2 \mu \textcolor{red}{\mu} +    \mu^2 \\
		
	\end{array}
$$
$$
	\begin{array}{ll}
		\mathbb{E} \left[ (X-\mu)^{3} \right]& = \mathbb{E} \left[ X^3 - 3 X^2 \mu + 3 X \mu^2 -3\mu^3 \right] \\
	0	& = \mathbb{E} \left[ X^3 \right] - 3  \mu \textcolor{red}{ \mathbb{E} \left[ X^2 \right]} + 3 \mu^2\textcolor{red}{ \mathbb{E} \left[ X \right]}  -3 \mu^3 \\
	0	& = \mathbb{E} \left[ X^3 \right] - 3  \mu \textcolor{red}{(\mu^2 + \sigma^2)} + 3 \mu^2\textcolor{red}{\mu}  -3 \mu^3 \\
	\end{array}
$$
$$
	\begin{array}{ll}
		\mathbb{E} \left[ (X-\mu)^{4} \right]& = \mathbb{E} \left[ X^4 - 4 \mu X^3 + 6 \mu^2 X^2 -4 \mu^3 X + \mu^4 \right] \\
	3 \sigma^4	& = \mathbb{E} \left[ X^4 \right] - 4  \mu \textcolor{red}{\mathbb{E} \left[ X^3 \right]} + 6 \mu^2\textcolor{red}{\mathbb{E} \left[ X^2 \right]}  -4 \mu^3 \textcolor{red}{\mathbb{E} \left[ X \right]} + \mu^4\\
	3 \sigma^4	& = \mathbb{E} \left[ X^4 \right] - 4  \mu \textcolor{red}{(3\mu\sigma^2+\mu^3)} + 6 \mu^2\textcolor{red}{(\sigma^2+\mu^2)}  -4 \mu^3 \textcolor{red}{\mu} + \mu^4\\
	\end{array}
$$
Le calcul des quatre premiers moments donne en particulier les résultats suivants~:
$$\begin{array}{l l }
    \mathbb{E}\left[X\right]= \mu &\mathbb{E}\left[X^2\right]= \sigma^2+\mu^2\\
    \\
    \mathbb{E}\left[X^3\right]= 3\mu\sigma^2+\mu^3 &  \mathbb{E}\left[X^4\right]= 3\sigma^4+6\sigma^2\mu^2+\mu^4.
\end{array}$$

\vspace{1cm}



Le calcul de l'espérance de $y_j$ donne, pour tout $1 \le j \le m$ :\\
$\begin{array}{ll}
	\displaystyle \mathbb{E}(y_{j}) &\displaystyle =\mathbb{E}\left(\frac{1}{2\delta}\left[\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\rho^{2}_{j} \right]\right)\\
	\\
	&\displaystyle =\mathbb{E}\left(\frac{1}{2\delta}\left[\sum_{k=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji}) \right]\right)+\mathbb{E}\left(\frac{1}{2\delta}\left[\rho^{2}_{j} \right]\right)\\
	\\
	&\displaystyle =\frac{n}{\delta}\mathbb{E}\left(x_{i} s_{ji} \right)-\frac{n}{2\delta}\mathbb{E}\left(x^{2}_{i} \right)-\frac{n}{2\delta}\mathbb{E}\left(s^{2}_{ji} \right)+\frac{1}{2\delta}\mathbb{E}\left(\rho^{2}_{j} \right)\\
	\\
	&\displaystyle =\frac{n}{\delta}\mathbb{E}\left(x_{i}\right)\mathbb{E}\left(s_{ji}\right)-\frac{n}{2\delta}\mathbb{E}\left(x^{2}_{i} \right)-\frac{n}{2\delta}\mathbb{E}\left(s^{2}_{ji} \right)+\frac{\rho^2_j}{2\delta}
\end{array}$

\medskip

\noindent Pour assurer que $\mathbb{E}(y_{j})= \mu_x$, il faut donc :		 

\begin{equation*}
	\begin{array}{ll}
		\displaystyle \mu_x &= \displaystyle\frac{n}{\delta}\mu_x \mu_s-\frac{n}{2\delta}\left(\sigma_x^2+\mu_x^2\right)-\frac{n}{2\delta}\left(\sigma_s^2+\mu_s^2\right)+\frac{\rho^2_j}{2\delta}\\
		\\
		& \displaystyle= \frac{n}{2\delta}\left[2 \mu_x \mu_s - \mu_x^2 -\mu_s^2 -\sigma_x^2-\sigma_s^2 +\frac{\rho^2}{n}\right]\\
		\\
		&\displaystyle = \frac{n}{2\delta}\left[ (\mu_x-\mu_s)^2 + \sigma_x^2 + \sigma_s^2 +\frac{\rho^2}{n}\right]\\
	\end{array}
	\label{eq:vareq}
\end{equation*}
égalité qui revient à~:\\
\begin{equation*}
	\begin{array}{ll}
		\displaystyle \rho^2 & = 2\delta \mu_x + n \left[ (\mu_x-\mu_s)^2 + \sigma_x^2 + \sigma_s^2 \right]
	\end{array}
	\label{eq:ecriturho2}
\end{equation*}
ou~:\\

\begin{equation}
	\begin{array}{ll}
		\displaystyle \rho & =\sqrt{ 2\delta \mu_x + n \left[ (\mu_x-\mu_s)^2 + \sigma_x^2 + \sigma_s^2 \right]}\\
	\end{array}
	\label{eq:ecriturho}
\end{equation}\\

Passons à la recherche d'une condition induite par l'égalité des variances en entrée et en sortie. 
On rappelle que si $X \sim \mathcal{N}(\mu,\sigma^2)$, 
alors var$(X^2) = \mathbb{E}\left[X^4_{}\right]-\mathbb{E}^{2}\left[X^2_{}\right] =  2 \sigma^2 \left(\sigma^2 + 2 \mu^2 \right)$.
En utilisant la formule  \eqref{eq:varianceformuleindependant},  on  calcule la variance de $2x_i s_{ji}$ de la façon suivante~:
\begin{equation}
	\begin{array}{ll}
		\displaystyle \mbox{var}(2x_i s_{ji}) & =4 \mbox{var}(s_{ji}) \mbox{var}(x_i) + 4 \mathbb{E}^2\left[s_{ji}^2\right] \mbox{var}(x_i) +4 \mathbb{E}^2\left[x_{i}^2\right] \mbox{var}(s_{ji})\\
		\\
		&=4 \sigma_s^2 \sigma_x^2 + 4 \mu_x^2 \sigma_s^2 + 4 \mu_s^2 \sigma_x^2
	\end{array}
	\label{eq:ecriturvar2xs}
\end{equation}
Or on a~:
\begin{equation}
\begin{array}{l l }
    \mbox{var}(x_i)= \sigma_x^2  & \mbox{var}(x_i^2)=  2 \sigma_x^2 \left(\sigma_x^2 + 2 \mu_x^2 \right)\\
\end{array}
\label{eq:ecriturvarx}
\end{equation}
\begin{equation}
\begin{array}{l l }
    \mbox{var}(s_{ji})= \sigma_s^2   & \mbox{var}(s_{ji}^2)=  2 \sigma_s^2 \left(\sigma_s^2 + 2 \mu_s^2 \right)
\end{array}
\label{eq:ecriturvars}
\end{equation}
\begin{equation}
\begin{array}{l l }
    \mbox{cov}\left( 2x_i s_{ji}-x_i^2, -s_{ji}^2\right) & = -4 \mu_s \mu_x \sigma_s^2\\
    \mbox{cov}\left( 2x_i s_{ji},-x_i^2 \right) & = -4 \mu_s \mu_x \sigma_x^2\
\end{array}
\label{eq:ecriturdescovariances}
\end{equation}
L'annexe \ref{annex_covariance} détaille les calculs des termes de covariance. 
\[
\begin{array}{ll}
	\displaystyle \operatorname{var}(y_{j}) &\displaystyle = \operatorname{var}\left(\frac{1}{2\delta}\left[\sum_{i=1}^{n} (2\cdot x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\rho^{2}_{j} \right]\right)\\
	\\
	
	&\displaystyle = \frac{1}{4\delta^2}\operatorname{var}\left(\left[\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})\right]\right) {\footnotesize \text{ hypothèse de distributions identiques}} \\
	\\
	
	&\displaystyle = \frac{n}{4\delta^2} \operatorname{var}\left(2  x_{i}  s_{ji} - x^{2}_{i} - s^{2}_{ji}\right)\\
	\\
	
	&\displaystyle = \frac{n}{\delta^2} \operatorname{var}\left(x_{i}  s_{ji}\right) + \frac{n}{4\delta^2} \operatorname{var}\left(x^{2}_{i}\right) + \frac{n}{4\delta^2} \operatorname{var}\left(s^{2}_{ji}\right)\\
	\\
	& + \dfrac{1}{2\delta^2}\mbox{cov}(2x_i s_{ji}-x_i^2, -s_{ji}^2) + \dfrac{1}{2\delta^2}\mbox{cov}(2x_i s_{ji},-x_i^2) \\
\end{array}
\]

\newpage

En utilisant les relations \eqref{eq:ecriturvar2xs}--\eqref{eq:ecriturdescovariances},
on obtient  l'expression suivante~:

\begin{align*}
\mbox{var}(y_j) &=\frac{n}{4 \delta^2} (4\mu_s^2\sigma_s^2 + 4\mu_s^2\sigma_x^2 - 8\mu_s\mu_x\sigma_s^2 - 8\mu_s\mu_x\sigma_x^2 + 4\mu_x^2\sigma_s^2 + 4\mu_x^2\sigma_x^2 + 2\sigma_s^4 + 4\sigma_s^2\sigma_x^2 + 2\sigma_x^4 )\\
\\
 &= \frac{n}{2 \delta^2} (\sigma_s^2 + \sigma_x^2) \left( 2(\mu_x  - \mu_s)^2  +\sigma_s^2 + \sigma_x^2 \right)
\end{align*}


On veut que var$(y_j)=\sigma_x^2$ pour tous $i \in \{1, \ldots, n\}$ et $j\in \{1, \ldots, m\}$. Cela se traduit par l'equation~:

\begin{equation}
\begin{array}{l }
    2\mu_s^2\textcolor{red}{\sigma_s^2} + 2\mu_s^2\sigma_x^2 - 4\mu_s\mu_x\textcolor{red}{\sigma_s^2} - 4\mu_s\mu_x\sigma_x^2 + 2\mu_x^2\textcolor{red}{\sigma_s^2} + 2\mu_x^2\sigma_x^2 + \textcolor{blue}{\sigma_s^4} + 2\textcolor{red}{\sigma_s^2}\sigma_x^2 + \sigma_x^4 - \frac{2\delta^2}{n}\sigma_x^2=0
\end{array}
\label{eq:eqvariance}
\end{equation}\\

L'équation \eqref{eq:eqvariance} est une équation bicarrée en $\sigma^2_s$. On peut donc la résoudre explicitement avec la méthode du discriminant. 
En ne conservant que la solution qui peut être positive, on arrive à

\begin{equation}
\displaystyle \sigma_s^2 = -\left(\mu_s- \mu_x \right)^2- \sigma_x^2 + \sqrt{\left(\mu_s^2 - \mu_x^2\right)^4 + \frac{2\sigma_x^2\delta^2}{n}}
\label{eq:solution_1_sigma_s}
\end{equation}\\

\noindent  Garantir la positivité de $\sigma^2_s$ revient à s'assurer que son numérateur est positif étant donné que le coefficient devant $\sigma^4_s$  est positif, c'est à dire la condition suivante~:

\begin{equation}
 \sigma_x^2 \left(-2\mu_s^2 + 4\mu_s\mu_x - 2\mu_x^2 - \sigma_x^2 + \frac{2\delta^2}{n}\right) >0
\label{eq:deltatcond}
\end{equation}

\noindent Pour s'en assurer, il faut cette fois résoudre une autre équation du second degré en $\mu_s$. Il existe une solution réelle si son discriminant satisfait  $\displaystyle \Delta_{\mu} =  - 8 \sigma_x^2 + \frac{16}{n}\delta^2 > 0$. On en tire la condition suivante sur $\delta$~:
\begin{equation*}
	\displaystyle   16 \left( \frac{\delta^2}{n} - \frac{\sigma_x^2}{2}\right) >0
\end{equation*}\\
c'est-à-dire~: \\


\begin{equation}
	\displaystyle
	\displaystyle   \delta  > \sqrt{ \frac{n}{2} \sigma_x^2 }
	\label{eq:cond_delta_mu}
\end{equation}\\
	
On vérifie alors que la solution donnée par \eqref{eq:deltatcond} est bien définie si  $\mu_s$ satisfait~:\\


\begin{equation}
	\displaystyle   \mu_s \in \left[\textcolor{red}{ \mu_x - \sqrt{  \frac{\delta^2}{n} - \frac{\sigma_x^2}{2} }}, \textcolor{blue}{ \mu_x + \sqrt{ \frac{\delta^2}{n} - \frac{\sigma_x^2}{2}} } \right]
	\label{eq:cond_mu_s}
\end{equation}\\

\newpage
	
Pour résumer, les calculs précédents aboutissent aux conditions suivantes~:\\
	
	\begin{equation}
		\displaystyle\left\{\begin{array}{ll}
			\displaystyle \rho^2 & = 2\delta \mu_x + n \left[ (\mu_x-\mu_s)^2 + \sigma_x^2 + \sigma_s^2 \right]\\
			\\
			\displaystyle \sigma_s^2 & =  \sqrt{ \left(\mu_x - \mu_s \right)^4 +  \frac{2 \delta^2}{n}\sigma_x^2 }- \left[ \left( \mu_s - \mu_x \right)^2 + \sigma_x^2\right]
		\end{array}\right.
		\label{eq:sysvariance}
	\end{equation}
pour $\delta$ vérifiant \eqref{eq:cond_delta_mu}.

\bigskip

\noindent{\bf Remarque.} Nous nous sommes également intéressés au cas des couches de neurones à filtres convolutifs ({\it cf} Annexe \ref{annexcasconvolutive}), afin de déterminer comment la covariance entre deux éléments de la sortie est reliée aux contraintes du système d’équations \eqref{eq:sysvariance} établi précédemment.
Des travaux comme \cite{trockman2022} ont de plus montré comment exploiter la structure de covariance d'un filtre convolutif peut permettre d'améliorer l'initialisation.

	
	\bigskip 
	
	\subsubsection{Contrôle pendant la rétro-propagation (contrôle backward)}% : couches denses hypersphériques}
	
	
	
	Dans cette partie, l'impact de la variance de la fonction de coût sur les poids est étudié. 
	%Afin d'éviter toute confusion avec les notations utilisées dans la section sur l'algèbre conforme, les notations sont modifiées par rapport à l'article de Glorot.
	La sortie d'un neurone de la couche $l$ est désormais notée $\mathbf{z}^l$, 
	définie par
	$$\begin{array}{ll}
		\mathbf{z}^l_j = \displaystyle \frac{1}{\delta}\widetilde{\mathbf{z}^l}._i\widetilde{s^l_j} & = \displaystyle\frac{1}{\delta}\left[\sum_{i=1}^{n}x^l_{i}s^l_{ji}-\frac{1}{2}{x^l_{i}}^2 - \displaystyle \frac{1}{2}{s^l_{ji}}^2+\frac{\rho^2_j}{2}\right]\\
		\\
		& = \displaystyle\frac{1}{\delta}\left[\mathbf{x}^l_{} \mathbf{s}^l_{j}-\frac{1}{2}{\mathbf{x}^l_{} \mathbf{x}^l} - \frac{1}{2}{\mathbf{x}^l_{j}\cdot \mathbf{s}^l_{j}}+\frac{\rho^2_j}{2}\right]
	\end{array}$$\\
	
	\noindent où $\mathbf{s}^l_j$ correspond à la partie euclidienne du vecteur $S^l$, soit les coordonnées du centre de l'hypersphère.	On a ainsi, \\
	
	$$\begin{array}{ll}
		\mathbf{z}^{l+1}_{j'}  = & \displaystyle\frac{1}{\delta}\left[\mathbf{x}^{l+1}_{} \mathbf{s}^{l+1}_{j'}-\frac{1}{2}{\mathbf{x}^{l+1}_{}}^2 - \frac{1}{2}{\mathbf{s}^{l+1}_{j'} }^2+\frac{\rho^2_{j'}}{2}\right]
	\end{array}$$\\
	
\noindent Si $f$ représente la fonction d'activation telle que $f'(0) = 1$,  la sortie de la couche est  définie par \hbox{$\mathbf{x}^{l+1} = f(\mathbf{z}^l)$}.\\
	
	
	
	On note $L$ la fonction de coût.
	
	
	
	
	Pour adapter au cas hypersphérique le raisonnement effectué dans le cas classique pour obtenir l'équation \eqref{eq:sortie_classique_backprop} \footnote{Si l'on considère $S^{l+1}_{}$ la matrice de poids des centres des hypersphères, alors on note $\mathbf{s}^{l+1}_{j\bullet}$, la $j$ ième ligne de $S^{l+1}_{}$ qui correspond au vecteur du centre pour un $j$ donné.}, on calcule la dérivée partielle de $\mathbf{z}^{l+1}$ par rapport à $\mathbf{x}^{l+1}_{j}$~: \\
	
	\begin{equation*}\label{eq:d}
		\dfrac{\partial \mathbf{z}^{l+1}_{}}{\partial \mathbf{x}^{l+1}_{j}}=\frac{1}{\delta}\left[\mathbf{s}^{l+1}_{j\bullet}- \mathbf{x}^{l+1}_{j}\right]
	\end{equation*}\\
	\newpage
	
	\noindent ce qui conduit à l'équation suivante~:\\
	
	\begin{equation*}\label{eq:e}
		\dfrac{\partial L}{\partial \mathbf{z}^{l}_{j}} = \frac{1}{\delta}\left(\mathbf{s}^{l+1}_{j\bullet}- \mathbf{x}^{l+1}_{j}\right)\dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{}}
	\end{equation*}
	
	\noindent soit~:
	
	\begin{equation}\label{eq:f}
		\dfrac{\partial L}{\partial \mathbf{z}^{l}_{}} = \frac{1}{\delta}\left(S^{l+1}_{}- \mathbf{x}^{l+1}_{}\right)\dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{}}
	\end{equation}\\
	
	\noindent avec $S^{l+1}$, la matrice dont les coefficients sont les centres des hypersphères. L'opération $\left(S^{l+1}_{}- \mathbf{x}^{l+1}_{}\right)$ consiste donc à soustraire le vecteur $\mathbf{x}^{l+1}$ à chaque colonne.\\
	
	De \eqref{eq:f}, on tire d'abord un résultat sur les moyennes :\\
	
	$$\mathbb{E}\Bigl( \dfrac{\partial L}{\partial \mathbf{z}^{l}_{j}} \Bigr) 
	= \frac{1}{\delta} \mathbb{E}(S^{l+1}_{j}- \mathbf{x}^{l+1}_{j}) \mathbb{E}\Bigl( \dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{j}} \Bigr)
	= \frac{m}{\delta} (\mu_s^{l+1} - \mu_x^{l+1}) \mathbb{E}\Bigl( \dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{j}} \Bigr).$$
	On a assuré au paragraphe précédent que la moyenne des distributions de sortie reste constante au cours du processus : $\mu_x^{l+1} = \mu_x$.
	Ainsi\\
	
	\begin{equation}
	    \mathbb{E}\Bigl( \dfrac{\partial L}{\partial \mathbf{z}^{l}_{j}} \Bigr) 
	= \frac{m}{\delta} (\mu_s^{l+1} - \mu_x) \mathbb{E}\Bigl( \dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{j}} \Bigr).
	\label{eq:moy_retro_hyper}
	\end{equation}
	
	\bigskip 
	
	On peut aussi calculer\footnote{Les étapes du calculs sont présentées en Annexe \ref{annex_variance}.} les variances dans \eqref{eq:f} :
	
	\begin{equation}
	\begin{array}{ll}
		\displaystyle \mbox{var}\left(\dfrac{\partial L}{\partial \mathbf{z}^{l}_{}} \right) 
		&  = \dfrac{m}{\delta^2}\left[\left(\sigma_s^2 + \sigma_x^2\right)\mbox{var}\left(\dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{j'}}\right) + \left(\mu_x - \mu_s \right)^2\mbox{var}\left(\dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{j'}}\right) \right.\\
			\\
			\\
			&\left. +\mathbb{E}^2\left(\dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{j'}}\right)\left(\sigma_s^2 + \sigma_x^2 \right) \right]\\
	\end{array}
	\label{eq:var_retro_hyper2}
	\end{equation}
	
	On cherche une initialisation permettant de limiter les écarts entre les moyennes et les variances des gradients successifs  $\dfrac{\partial L}{\partial \mathbf{z}^{l}_{j}} $ et  $\dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{j}} $ au cours de la rétro-propagation.
	
	En gardant à l'esprit que $\mu_s $ est seulement défini par l'intervalle donné dans \eqref{eq:cond_mu_s}, le choix de $\mu_s$ et $\delta$ tels que $m \vert \mu_s - \mu_x \vert / \delta \ll 1$ permet de limiter les variations de moyenne dans \eqref{eq:moy_retro_hyper}, en gardant à l'esprit que l'équation \eqref{eq:var_retro_hyper2} limite la borne supérieure de $\delta$ pour que $m(\sigma_x^2+\sigma_s^2)/\delta^2$, c'est-à-dire $m\sigma_x/\delta$, reste d'un ordre non négligeable. Ceci dit, d'autres choix sont possibles. On voit par exemple dans \eqref{eq:var_retro_hyper2}  que si on accepte un peu de variations dans l'espérance des gradients, cela évite la dégénérescence de leur variance... C'est le principe que nous utilisons dans la suite.\\
	

	On réunit maintenant les conditions établies de \eqref{eq:solution_1_sigma_s} à \eqref{eq:var_retro_hyper2}. D'abord, on introduit un paramètre $\delta'$ (produit de $\delta$ par l'accroissement de moyenne qu'on s'autorise pendant la rétro-propagation) et on choisit pour $\mu_s$ la valeur maximale autorisée par l'intervalle dans \eqref{eq:cond_mu_s}.
	On obtient ainsi un système qui donne deux valeurs possibles pour $\mu_s-\mu_x$~:\\
	\begin{equation}
		\displaystyle\left\{\begin{array}{ll}
			 \mu_s - \mu_x & = \displaystyle \frac{\delta'}{m}\\
			\\
			\displaystyle\mu_s - \mu_x & =  \displaystyle\sqrt{\frac{\delta^2}{n}-\frac{\sigma^2_x}{2}} 
		\end{array}\right.
		\label{eq:sys_musmux}
	\end{equation}
	

 Dans la première équation de \eqref{eq:sys_musmux}, on a  $\delta' = C \delta$ où $C$ représente le taux de croissance des moyennes :
\begin{equation} 
C = \frac{\mathbb{E}\Bigl( \dfrac{\partial L}{\partial \mathbf{z}^{l}_{j}} \Bigr)}{\mathbb{E}\Bigl( \dfrac{\partial L}{\partial \mathbf{z}^{l+1}_{j}} \Bigr)}.
\label{defC}
\end{equation}
Par exemple si $C=1$, les moyennes ne changent pas. Si $C=2$, à chaque fois qu'on traverse une couche la moyenne est doublée etc. \'Evidemment on cherche une stratégie qui n'augmente pas les moyennes de façon trop significative. D'autant que la croissance des moyennes conduit à une croissance des variances  à cause de \eqref{eq:var_retro_hyper2}. 

Dans la deuxième équation de \eqref{eq:sys_musmux}, on a $\delta'= m \displaystyle \sqrt{\frac{\delta^2}{n}-\frac{\sigma^2_x}{2}}$. Comme, d'après \eqref{eq:cond_delta_mu}, il faut $\delta'=C\delta > C \sqrt{(n/2)\sigma^2_x}$, on a forcément $\displaystyle\sqrt{\frac{\delta^2}{n}-\frac{\sigma^2_x}{2}} >  (C/m)\sqrt{\frac{n}{2}\sigma^2_x}$. En passant au carré la dernière égalité et la dernière inégalité, on en tire les conditions suivantes sur $\delta$ : 
\begin{eqnarray}
&& C^2 \delta^2 = m^2( \frac{\delta^2}{n}-\frac{\sigma^2_x}{2}),
\label{cond111}
\\
&& \delta^2 > \frac{n}{2} \Bigl(  \frac{C^2n}{m^2} +1 \Bigr)\sigma_x^2.
\label{cond222}
\end{eqnarray}
La condition \eqref{cond222} va se substituer à la condition \eqref{eq:cond_delta_mu} (on note en particulier que \eqref{cond222} implique \eqref{eq:cond_delta_mu}).
De la condition \eqref{cond111} on peut tirer une expression de $\delta$ :
\begin{equation}
\delta = \sqrt{\frac{m^2n\sigma_x^2}{2(m^2-C^2n)} } 
    \label{conddelta000}
\end{equation}  
qui n'a de sens que si la variation $C$ de la moyenne des gradients est contrôlée par
\begin{equation}
    C \le \frac{m}{\sqrt{n}} .
    \label{eq:condC000}
\end{equation}  
\\


\bigskip 

{\bf Récapitulons : }
\\
\begin{itemize} 
\item On choisit $C$ vérifiant \eqref{eq:condC000} et d'un ordre de grandeur raisonnable pour que la relation \eqref{defC} ne conduise pas à l'explosion de la moyenne des gradients. Nous considérons la variable $\epsilon$ petite pour calculer $C = \frac{m}{\sqrt{n}}-\epsilon$. La soustraction de la valeur $\epsilon$ permet de garantir que l'inégalité \ref{eq:condC000} est maintenue.
\\
\item On définit ensuite $\delta$ par \eqref{conddelta000}.
\\
\item On remarque alors que 
$$\frac{m}{\delta^2} = \displaystyle \frac{2(m^2-C^2n)}{m \, n\, \sigma_x}.$$ 
Il ne reste plus qu'à contrôler cette dernière quantité, en utilisant si besoin la marge de manoeuvre qu'on a sur $C$, pour que l'équation \eqref{eq:var_retro_hyper2} assure que la variance des gradients n'explose pas et ne dégénère pas non plus.\\
\\
\item Une fois l'étape précédente terminée, on a une valeur définitivement établie pour $\delta$. Celle ci est fixé pour l'entraînement. 
\\
\item On peut alors choisir $\mu_s$ et $\sigma_s$ par \eqref{eq:sysvariance}.
\end{itemize}


\vspace{1cm} 


Comme mentionné précédemment, nous avons deux expressions pour définir $\mu_s$. Afin de garantir la condition établie par \ref{eq:cond_mu_s}, on propose donc considérer le min entre les deux expressions obtenues pour $\mu_s$. Ainsi on pose~:
\begin{equation}
    \mu_s =\displaystyle \min \left(\mu_x + \frac{\delta}{m}; \mu_x + \sqrt{\frac{\delta^2}{n}-\frac{\sigma^2_x}{2}} \right)
    \label{eq:defmin_mus}
\end{equation}


	
	


\vspace{1cm} 

	
	Dans les paragraphes suivants sont présentées les expérimentations numériques correspondant à l'ensemble des règles d'initialisation que nous venons de développer.
	
	
	

	
	
	%$\left(\frac{c\cdot d}{2}\sigma^4\right)$
	%$\left(\frac{c\cdot d}{2}\sigma^4+\frac{c\cdot\left(d-\epsilon\right)}{2}\sigma^{4}_{x}\right)$
	%$\frac{n}{2}\left(\sigma^{2}_{x}+\sigma^{2}\right)^{2}$
	
	
	

    \section{L'initialisation hypersphérique en pratique}\label{expe_init} 
    
    On rappelle ici de façon synthétique les règles d'initialisation issues des calculs des paragraphes précédents. Nous présentons ensuite un certain nombre d'expérimentations qui permettent à la fois de confirmer la pertinence de ces règles et d'illustrer les arguments mathématiques précédents. Un test sur un cas plus réaliste est présenté au paragraphe \ref{expe_real}.
    
    
    \subsection{Résumé des règles d'initialisation}\label{subsec:resume_init}
    
    Dans le tenseur de sortie, la composante $y_{j}= \frac{\tilde{ x}._i\tilde{s}_j}{\delta}$ associée à l'hypersphère $\displaystyle \tilde{s}_j$ correspondant à la sortie d'un neurone s'écrit~:

    \begin{equation}
    	\displaystyle y_{j} = \frac{1}{2\delta}\left[\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\rho^{2}_{j} \right]
    \end{equation}
    
    \bigskip
    
    %On rappelle les hypothèses suivantes~:\\
    
    %\begin{itemize}
        %\item Pour tout $i \in \{1, \ldots, n\}$, les $x_i$ sont des variables aléatoires indépendantes identiquement distribué, et suivent une loi normal de moyenne $\mu_x$ et de variance $\sigma^2_x$.   Soit $x_i \sim \mathcal{N}(\mu_x, \sigma_x^2)$.\\
        %\item Pour l'initialisation les paramètre $\rho_j$ des rayons sont constants.\\
        %\item Les paramètres $s_{ji}$ des centres des hypersphères sont également des variables aléatoires indépendantes et identiquement distribué. Ils suivent une loi normal de moyenne $\mu_s$ et de variance $\sigma^2_s$.   Soit $s_{ji} \sim \mathcal{N}(\mu_s, \sigma_s^2)$.\\
        
        %\item Pour tout $i\in \{1, \ldots, n\}$ et $j \in  \{1, \ldots, m\}$, on a $var(y_j)=var(x_i)$
        
    %\end{itemize}
    
    %\bigskip
    
    %Le développement des calcules des variances de sortie, nous permet d'aboutir à une expression pour les rayons $\rho_j$ et les variances $\sigma^2_s$ des centres afin d'initialiser les paramètres des hypersphères et de maintenir une distribution stable entre chaque sorties de couches cachées.  La résolution de l'équation \ref{eq:eqvariance} nous montre également qu'il y a des conditions sur le choix des moyennes $\mu_s$ pour les paramètres de centres, et également une condition nécessaire a respecter sur la valeur $\delta$ tel que~:\\
    
    
    \newpage
    
    
    Les conditions \eqref{maline}, \eqref{eq:cond_delta_mu} et \eqref{eq:sysvariance}
	  nous permettent  de proposer l'algorithme d'initialisation des paramètres des hypersphère suivant pour une couche hypersphérique~:\\
    
    
    \begin{tcolorbox}[
	colback=gray!5,
	colframe=blue!75!black,
	fonttitle=\bfseries,
	%boxrule=0mm,
	%notitle
	title={Initialisation des paramètres des hypersphères}
]	
	\begin{algorithm}[H]
	    \label{algo:initialisationsph}
		\Donnees{Ensemble de données d'entrée de la couche hypersphérique $X = \{\mathbf{x}_1, \dots, \mathbf{x}_K\}_k$ tel que $\mathbf{x}_k \in \mathbb{R}^n$, le nombre d'hypersphères $m$, $\epsilon=\SI{1e-3}{}$. }
		 \BlankLine
		 
		\Res{Pour tout $j \in \{1, \dots, m\}$, les centres $\mathbf{c}_j = \left(s_{j1}, \dots, s_{jn}\right)$ et les rayons $\rho_j$ des hypersphères}
		
		
		\BlankLine
		\textbf{Étape 1~: Déterminer la distribution des données d'entrée (ou un de ses sous-ensemble)}\\
		
		$$\begin{array}{cc}
		  \displaystyle \mu_x  = &\displaystyle   \frac{1}{Kn} \sum_{k,i} x_{ki} \\
		   \displaystyle \sigma^2_x = & \displaystyle\frac{1}{Kn} \sum_{k,i} x_{ki}^2 - \mu_x^2 
		\end{array}$$
		
		\BlankLine
		\textbf{Étape 2~: Fixer $C$ puis calculer le facteur d'échelle $\delta$}\\
		
		$$C = \frac{m}{\sqrt{n}}-\epsilon \quad \textit{et} \quad \displaystyle   \delta  := \sqrt{\frac{m^2n\sigma_x^2}{2(m^2-C^2n)} }  $$
		
		\BlankLine
		\textbf{Étape 3~: Calcul de $\mu_s$} \\
		$$\mu_s :=\displaystyle \min \left(\mu_x + \frac{\delta}{m}; \mu_x + \sqrt{\frac{\delta^2}{n}-\frac{\sigma^2_x}{2}} \right)$$
		
		\BlankLine
		\textbf{Étape 4~: Calcul de $\rho$}\\
		$$\displaystyle \rho^2 := 2\delta \mu_x + n \left[ (\mu_x-\mu_s)^2 + \sigma_x^2 + \sigma_s^2 \right]$$
		
		\BlankLine
		\textbf{Étape 5~: Calcul de $\sigma^2_s$}\\
		$$\displaystyle \sigma_s^2 :=  \sqrt{ \left(\mu_x - \mu_s \right)^4 +  \frac{2 \delta^2}{n}\sigma_x^2 }- \left[ \left( \mu_s - \mu_x \right)^2 + \sigma_x^2\right]$$
		
	    \BlankLine
		\textbf{Étape 6 : Initialisation des hypersphères}\\
		\PourCh{$j \in \{1, \dots, m\}$} {
		\PourCh{$i \in \{1, \dots, n\}$} {
		Initialiser les $s_{ji}$ aléatoirement tels que $s_{ji} \sim \mathcal{N}(\mu_s, \sigma_s^2)$
		}
		\BlankLine
	      Les paramètres de l'hypersphère $\tilde{s_j}$  sont définis par le centre $\textbf{c}_j =\left(s_{j1}, \dots, s_{jn}\right)$ et le rayon $\rho$  (cf équation \ref{eq:tildes}).
		}
		
		
        
        
		
	\end{algorithm}
\end{tcolorbox}
    \noindent{\bf Remarque. }  
	La méthodologie d’initialisation proposée nécessite une inférence préalable couche par couche du modèle. En effet, pour pouvoir appliquer l’algorithme d’initialisation à une couche hypersphérique donnée, il est indispensable de disposer des tenseurs d’entrée associés à cette couche. Cela implique qu'il faut exécuter une passe avant (forward pass) du réseau couche par couche.


	\newpage
	
	Dans les sous-paragraphes suivants, on illustre la pertinence des règles d'initialisation proposées en observant la façon dont un ''signal'' est transformé au fil de ses passages dans des neurones hypersphériques. Pour des raisons de place, nous ne montrons que la distribution de données initiales et la distribution à la sortie des deux premières couches. Il faut savoir que lorsque nous annonçons que les résultats sont mauvais, ils le sont dès la sortie de la quatrième couche. Lorsque nous les annonçons bons, c'est parce que nous les avons testé sur une centaine de couches.
	La figure \ref{fig:ma_enchainement} donne par exemple les résultats à la sortie de 50 couches pour~: \\
	
	\begin{itemize}
	    \item Une distribution normale avec des points de dimension 4\\
	    \item Des données extraites du jeu IRIS \cite{IRISdataset}.\\
	    \item Une distribution composée de trois clusters de points, chacun suivant une loi normale multivariée.
	\end{itemize}
 
\begin{figure}[htbp]
    \centering
    % Deux premières figures côte à côte avec sous-titres
    \begin{subfigure}[t]{0.45\textwidth} % alignement en haut
        \centering
        \includegraphics[scale=0.075]{images/plot_distrib_50_layers_2d_test_normal_data.png}
        \caption{Jeu de données synthétique 4D distribué selon une loi normale multivariée}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth} % alignement en haut
        \centering
        \includegraphics[scale=0.075]{images/plot_distrib_50_layers_iris_data.png}
        \caption{Jeu de données Iris}
    \end{subfigure}

    \caption{Enchaînement de couches hypersphériques}
    \label{fig:ma_enchainement}
\end{figure}

\bigskip

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.08]{images/plot_distrib_50_layers_normal_data.png}
    \caption{Enchaînement de couches hypersphériques pour un jeu de données synthétique de points 4D suivant une loi normale centrée}
    \label{fig:enchainement_4d_normal}
\end{figure}
	
\bigskip
	
    
    \subsection{Illustrations expérimentales}
    \label{sec:manip_init_chap1}
    Pour les expériences qui suivent, nous utilisons un réseau de neurones à deux couches cachées hypersphériques, contenant chacune $N$ hypersphères. Lorsque $N > 1$, la sortie du neurone est obtenue en moyennant l'ensemble des produits $\tilde{s}._i \tilde{x}$ calculés. Il n'y a pas d'activation : nous observons simplement l'enchaînement successif des couches. Le jeu de données utilisé est un ensemble de points synthétiques de dimension 10\,000, distribués selon une loi normale de moyenne $\mu_x$ et de variance $\sigma_x^2$. Les valeurs utilisées pour les expérimentations sont indiquées au-dessus des figures correspondantes.\\
    
    Dans cette section, la valeur $\delta$ est définie par $ \sqrt{\frac{n}{2}\sigma^2_x} + \epsilon_{\delta}$ car les conditions données par l'étude du gradient de la fonction de coût ne sont pas prises en compte. 
   
        \subsubsection{Expérience 1  : Montrer que $\mu_s$ doit être différent de zéro}
        
        \begin{figure}[htbp]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_zero/fig10.png}
            \caption{$\mu_s = 0$, $\sigma_x = 0.1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 1$: 
            \\
            $\sigma_{x0} = 0.099984$, $\sigma_{x1} = 0.080297$, $\sigma_{x2} = 0.036692$,
            \\
            $\mu_{x0} = 1.000006$, $\mu_{x1} = 1.075929$, $\mu_{x2} = 1.177815$,
            \\
            $\sigma_{s1} = 0.005229$, $\sigma_{s2} = 0.014190$,
            \\
            $\mu_{s1} = 0.000000$, $\mu_{s2} = 0.000000$.}
            \label{fig:ma_figure10}
        \end{figure}
        
        
        \begin{itemize}
            \item \textbf{Observations} :
            \begin{itemize}
                \item Les écarts-types \(\sigma_{x1}\) et \(\sigma_{x2}\) diminuent significativement (0.080297 et 0.036692), ce qui indique une perte de variance.
                \item Les moyennes \(\mu_{x1}\) et \(\mu_{x2}\) augmentent légèrement par rapport à \(\mu_{x0}\), ce qui montre une dérive du signal.
            \end{itemize}
        \end{itemize}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig11.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 0.1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 1$: 
            \\
            $\sigma_{x0} = 0.099984$, $\sigma_{x1} = 0.031161$, $\sigma_{x2} = 0.019281$,
            \\
            $\mu_{x0} = 1.000006$, $\mu_{x1} = 0.998604$, $\mu_{x2} = 0.997637$,
            \\
            $\sigma_{s1} = 0.000011$, $\sigma_{s2} = 0.000001$,
            \\
            $\mu_{s1} = 1.382000$, $\mu_{s2} = 2.150632$.}
            \label{fig:ma_figure11}
        \end{figure}
        
        \begin{itemize}
            \item \textbf{Observations} :
            \begin{itemize}
                \item Les écarts-types \(\sigma_{x1}\) et \(\sigma_{x2}\) restent proches de \(\sigma_{x0}\), ce qui indique une bonne préservation de la variance.
                \item Les moyennes \(\mu_{x1}\) et \(\mu_{x2}\) restent proches de \(\mu_{x0}\), ce qui suggère une propagation plus fidèle du signal.
            \end{itemize}
        \end{itemize}
        
        
        \begin{itemize}
            \item \textbf{Analyse} :
            \begin{itemize}
                \item Lorsque \(\mu_s = 0\), les écarts-types et les moyennes se dégradent plus rapidement, que dans le cas où \(\mu_s \neq 0\), les écarts-types diminuent, mais les moyennes restent très proches de la valeur initiale, ce qui montre une meilleure stabilité. Il est plus intéressant de fixer les moyennes $\mu_s$ proche de la valeur de la borne supérieur. (le cas proche de la valeur de la borne inf donne des résultats similaires).
            \end{itemize}
        \end{itemize}
        \subsubsection{Expérience 2 : Montrer que si $\epsilon_{\delta} = 0$ et $\sigma_x = 1$ (non petit), cela ne fonctionne pas}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig12.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 1$: 
            \\
            $\sigma_{x0} = 0.999842$, $\sigma_{x1} = 0.593819$, $\sigma_{x2} = 0.400592$,
            \\
            $\mu_{x0} = 1.000060$, $\mu_{x1} = 0.985483$, $\mu_{x2} = 0.967814$,
            \\
            $\sigma_{s1} = 0.000747$, $\sigma_{s2} = 0.000302$,
            \\
            $\mu_{s1} = 1.631132$, $\mu_{s2} = 2.195358$.}
            \label{fig:ma_figure12}
        \end{figure}
        
        \begin{itemize}
            \item \textbf{Observations} :
            \begin{itemize}
                \item Les écarts-types \(\sigma_{x1}\) et \(\sigma_{x2}\) diminuent trop fortement (0.593819 et 0.400592), ce qui indique une perte d'information excessive.
                \item Malgré une moyenne relativement stable, la propagation devient moins efficace en raison de la réduction de la variance.
            \end{itemize}
        \end{itemize}
        
        \begin{itemize}
            \item \textbf{Analyse} :
            \begin{itemize}
                \item Lorsque \(\sigma_x\) est grand, les écarts-types diminuent fortement, et le signal tend vers une loi gamma de signe inverse, ce qui montre une instabilité dans la propagation du signal. 
            \end{itemize}
        \end{itemize}
        
        
        \subsubsection{Expérience 3 : Influence de $N$ grand}
        
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig13.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 0.1$, $\mu_x = 1$, $N = 10$, $\epsilon_{\delta} = 1$: 
            \\
            $\sigma_{x0} = 0.099984$, $\sigma_{x1} = 0.031154$, $\sigma_{x2} = 0.019243$,
            \\
            $\mu_{x0} = 1.000006$, $\mu_{x1} = 0.999603$, $\mu_{x2} = 0.999568$,
            \\
            $\sigma_{s1} = 0.000011$, $\sigma_{s2} = 0.000001$,
            \\
            $\mu_{s1} = 1.382000$, $\mu_{s2} = 2.151655$.}
            \label{fig:ma_figure13}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig14.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 0.1$, $\mu_x = 1$, $N = 1000$, $\epsilon_{\delta} = 1$: 
            \\
            $\sigma_{x0} = 0.099984$, $\sigma_{x1} = 0.031153$, $\sigma_{x2} = 0.019238$,
            \\
            $\mu_{x0} = 1.000006$, $\mu_{x1} = 0.999987$, $\mu_{x2} = 0.999986$,
            \\
            $\sigma_{s1} = 0.000011$, $\sigma_{s2} = 0.000001$,
            \\
            $\mu_{s1} = 1.382000$, $\mu_{s2} = 2.152048$.}
            \label{fig:ma_figure14}
        \end{figure}
        
        
        \begin{itemize}
            \item \textbf{Observations} :
            \begin{itemize}
                \item Lorsque \(N\) augmente (\(N=10\) et \(N=1000\)), la variance \(\sigma_x\) reste bien préservée.
                \item La moyenne \(\mu_x\) est presque inchangée, suggérant une stabilité accrue avec un grand \(N\).
                \item Une meilleure propagation du signal est observée avec un grand \(N\), validant son rôle bénéfique.
            \end{itemize}
        \end{itemize}
        
        \begin{itemize}
            \item \textbf{Analyse} :
            \begin{itemize}
                \item Lorsque \(N\) est grand, les écarts-types diminuent, mais les moyennes restent extrêmement proches de la valeur initiale, ce qui montre une excellente stabilité. On observe une conséquence du théorème central limite.
            \end{itemize}
        \end{itemize}
        
        \subsubsection{Expérience 4 : Influence de $\epsilon_{\delta}$ sur $\mu_s$}
        
        \textbf{Cas $\sigma_x$ petit}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig15.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 0.1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 10$: 
            \\
            $\sigma_{x0} = 0.099984$, $\sigma_{x1} = 0.038070$, $\sigma_{x2} = 0.036201$,
            \\
            $\mu_{x0} = 1.000006$, $\mu_{x1} = 0.998295$, $\mu_{x2} = 0.997497$,
            \\
            $\sigma_{s1} = 0.000011$, $\sigma_{s2} = 0.000000$,
            \\
            $\mu_{s1} = 1.504375$, $\mu_{s2} = 11.328631$.}
            \label{fig:ma_figure15}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig16.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 0.1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 1000$: 
            \\
            $\sigma_{x0} = 0.099984$, $\sigma_{x1} = 0.094492$, $\sigma_{x2} = 0.094444$,
            \\
            $\mu_{x0} = 1.000006$, $\mu_{x1} = 0.998323$, $\mu_{x2} = 0.998099$,
            \\
            $\sigma_{s1} = 0.000002$, $\sigma_{s2} = 0.000000$,
            \\
            $\mu_{s1} = 11.694160$, $\mu_{s2} = 1001.364937$.}
            \label{fig:ma_figure16}
        \end{figure}
        
        \textbf{Cas $\sigma_x$ grand}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig17.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 10$: 
            \\
            $\sigma_{x0} = 0.999842$, $\sigma_{x1} = 0.613114$, $\sigma_{x2} = 0.585020$,
            \\
            $\mu_{x0} = 1.000060$, $\mu_{x1} = 0.983484$, $\mu_{x2} = 0.970680$,
            \\
            $\sigma_{s1} = 0.000755$, $\sigma_{s2} = 0.000068$,
            \\
            $\mu_{s1} = 1.754479$, $\mu_{s2} = 11.404689$.}
            \label{fig:ma_figure17}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig18.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 1000$: 
            \\
            $\sigma_{x0} = 0.999842$, $\sigma_{x1} = 0.946954$, $\sigma_{x2} = 0.946493$,
            \\
            $\mu_{x0} = 1.000060$, $\mu_{x1} = 0.983402$, $\mu_{x2} = 0.981176$,
            \\
            $\sigma_{s1} = 0.000171$, $\sigma_{s2} = 0.000002$,
            \\
            $\mu_{s1} = 12.026035$, $\mu_{s2} = 1001.573476$.}
            \label{fig:ma_figure18}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig19.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 10000$: 
            \\
            $\sigma_{x0} = 0.999842$, $\sigma_{x1} = 0.987210$, $\sigma_{x2} = 0.987161$,
            \\
            $\mu_{x0} = 1.000060$, $\mu_{x1} = 0.994143$, $\mu_{x2} = 0.993295$,
            \\
            $\sigma_{s1} = 0.000020$, $\sigma_{s2} = 0.000000$,
            \\
            $\mu_{s1} = 102.073974$, $\mu_{s2} = 10001.604402$.}
            \label{fig:ma_figure19}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.0\textwidth]{cas_mu_x_un/fig20.png}
            \caption{$\mu_s = \mu_s^\star - \eta$, $\eta \ll 1$, $\sigma_x = 1$, $\mu_x = 1$, $N = 1$, $\epsilon_{\delta} = 100000$: 
            \\
            $\sigma_{x0} = 0.999842$, $\sigma_{x1} = 0.991597$, $\sigma_{x2} = 0.000009$,
            \\
            $\mu_{x0} = 1.000060$, $\mu_{x1} = 0.999441$, $\mu_{x2} = 0.999449$,
            \\
            $\sigma_{s1} = 0.000002$, $\sigma_{s2} = 1.000000$,
            \\
            $\mu_{s1} = 1002.079502$, $\mu_{s2} = 0.000000$.}
            \label{fig:ma_figure20}
        \end{figure}
        
        \begin{itemize}
            \item \textbf{Analyse} :
            \begin{itemize}
                \item Lorsque \(\delta\) est très grand (\(\epsilon_\delta = 10000\) ou \(100000\)), les écarts-types sont bien préservés, mais les moyennes dérivent légèrement. Cependant, dans le cas de \(\epsilon_\delta = 100000\), on observe un effondrement de l'écart-type dans la deuxième couche, ce qui indique une perte totale de variance. Cela montre que il y a une valeur limite pour \(\delta\) très grand, afin de garantir la propagation du signal. 
                \item Pour une propagation efficace du signal dans un réseau de neurones, il est essentiel de :
                \begin{itemize}
                    \item Initialiser \(\mu_s\) à une valeur non nulle pour éviter la dégradation de la variance et de la moyenne.
                    \item Maintenir \(\sigma_x\) petit permet d'assurer une propagation stable du signal.
                    \item Augmenter \(N\) pour améliorer la robustesse et la stabilité du réseau.
                    \item Contrôler \(\delta\) pour éviter l'instabilité, surtout lorsque \(\sigma_x\) est grand (on revient sur ce point au paragraphe suivant qui concerne le paramètre $q_i$).
                \end{itemize}
                \item Lorsque \(\delta\) est grand, cela améliore la propagation du signal jusqu'à une valeur limite qui  peut entraîner une instabilité dans le réseau, surtout lorsque \(\sigma_x\) est petit. Il est donc crucial de trouver un équilibre entre \(\delta\) et \(\sigma_x\) pour maintenir une propagation stable du signal.
                
            \end{itemize}
        \end{itemize}
        
        
        \subsubsection{Le paramètre $q_i$}
        
        Dans chaque légende de figure (en haut) apparaît un paramètre $q_i$, l'indice $i$ désignant le numéro de la couche. Celui-ci est calculé en fonction des paramètres d'entrée de la couche $i$ et des paramètres de poids calculés pour cette couche :
        $$ q_i = \frac{\mu_x}{\sigma_x} + \frac{\mu_s}{\sigma_s} .$$
        Le paramètre $q_i$ permet de vérifier si le critère \eqref{maline} est satisfait.
        Les figures ci-dessus, en particulier la dernière image de la figure \ref{fig:ma_figure20} permettent de vérifier que l'utilisation du critère \eqref{maline} peut être un garde-fou très efficace pour éviter un effondrement de la variance du signal de sortie de la couche.\\
        
        
    \noindent{\bf Remarque.} Afin de mieux maîtriser la valeur de $\delta$ et de tenir compte du nombre d'hypersphères utilisées (paramètre $m$ de l'algorithme proposé), il est insuffisant de se limiter aux conditions données par \eqref{eq:cond_delta_mu} et \eqref{eq:sysvariance}. C’est pourquoi nous avons également étudié l’impact de la variance de la fonction de coût, en suivant un raisonnement analogue à celui présenté dans \cite{glorot}. Cette démarche a permis de redéfinir le paramètre $\delta$ (cf. équation \eqref{conddelta000}) et d’assurer la propagation du signal à travers les couches, quel que soit le nombre de neurones choisis dans la couche hypersphérique.


            	
	
    \section{Validation de l'initialisation sur le jeu de données  Iris }\label{expe_real} 
    
    Le but de cette expérience est de vérifier la propagation du signal dans un réseau, en testant un jeu de données réel, afin d’évaluer si l’algorithme proposé pour l’initialisation fonctionne correctement (voir \ref{algo:initialisationsph}). Pour cela, nous avons construit un réseau de neurones comportant 10 couches cachées hypersphériques.\\
    
    Trois configurations ont été testées, avec respectivement 8, 64 et 4096 hypersphères par couches cachées (la valeur est indiquée dans la légende). Les modèles, partagent la même architecture générale, sans fonction d’activation ni normalisation par lot (BatchNorm). \\
    Nous avons pris $\epsilon = \SI{1e-3}{}$ pour calculer $C = \frac{m}{\sqrt{n}}-\epsilon$.\\ 
    
    Les figures \ref{fig:figaventrainnementsph8}, \ref{fig:figaventrainnementsph64} et \ref{fig:figaventrainnementsph4096} présentent la distribution des sorties de chaque couche cachées à l’initialisation, c’est-à-dire avant l’entraînement. Les figures \ref{fig:figapentrainnementsph8}, \ref{fig:figapentrainnementsph64} et \ref{fig:figaventrainnementsph4096} montrent quant à elles les distributions des sorties après entraînement.\\
    
    \begin{figure}[H]
	\begin{center}		
		\includegraphics[width=16cm]{figs/fig av_entrainnement sph8.png}
		\caption{Distribution des valeurs de sortie des couches hypersphériques \textcolor{blue}{avant} l'entraînement (cas de 8 hypersphères par couche cachée, pour un réseau à 10 couches cachées}
		\label{fig:figaventrainnementsph8}
	\end{center}
\end{figure}


    \begin{figure}[H]
	\begin{center}		
		\includegraphics[width=16cm]{figs/fig ap_entrainnement sph8.png}
		\caption{Distribution des valeurs de sortie des couches hypersphériques \textcolor{red}{après} l'entraînement (cas de 8 hypersphères par couche cachée, pour un réseau à 10 couches cachées}
		\label{fig:figapentrainnementsph8}
	\end{center}
\end{figure}


     \begin{figure}[H]
	\begin{center}		
		\includegraphics[width=16cm]{figs/fig av_entrainnement sph64.png}
		\caption{Distribution des valeurs de sortie des couches hypersphériques \textcolor{blue}{avant} l'entraînement (cas de 64 hypersphères par couche cachée, pour un réseau à 10 couches cachées}
		\label{fig:figaventrainnementsph64}
	\end{center}
\end{figure}


    \begin{figure}[H]
	\begin{center}		
		\includegraphics[width=16cm]{figs/fig ap_entrainnement sph64.png}
		\caption{Distribution des valeurs de sortie des couches hypersphériques \textcolor{red}{après} l'entraînement (cas de 64 hypersphères par couche cachée, pour un réseau à 10 couches cachées}
		\label{fig:figapentrainnementsph64}
	\end{center}
\end{figure}

    \begin{figure}[H]
	\begin{center}		
		\includegraphics[width=16cm]{figs/fig av_entrainnement sph4096.png}
		\caption{Distribution des valeurs de sortie des couches hypersphériques \textcolor{blue}{avant} l'entraînement (cas de 4096 hypersphères par couche cachée, pour un réseau à 10 couches cachées}
		\label{fig:figaventrainnementsph4096}
	\end{center}
\end{figure}


    \begin{figure}[H]
	\begin{center}		
		\includegraphics[width=16cm]{figs/fig ap_entrainnement sph4096.png}
		\caption{Distribution des valeurs de sortie des couches hypersphériques \textcolor{red}{après} l'entraînement (cas de 4096 hypersphères par couche cachée, pour un réseau à 10 couches cachées}
		\label{fig:figapentrainnementsph4096}
	\end{center}
\end{figure}

On peut observer que à l’initialisation, les moyennes des distributions de sorties restent stables au fur et à mesure que l’on empile les couches, et que la variance n’explose pas. Cela permet de préserver le signal à la sortie du réseau.\\

Pour les trois configurations qui ont été testées (avec 8, 64 ou 4096 neurones par couche cachée), les taux d’accuracy obtenus sont respectivement de 98.3\% pour 8 hypersphères, 98.3\% pour 64, et 96.7\% pour 4096. Ces résultats indiquent que l’augmentation excessive du nombre d’hypersphères nuit à la performance du modèle, probablement en raison d’un surapprentissage. On en déduit qu’il n’est pas nécessaire, voire contre-productif, d’utiliser un nombre trop important de paramètres. Cependant  les résultats restent pertinents\footnote{Il faut garder à l'esprit qu'ici on a ajouté un nombre démesuré de paramètres pour tester les performances du processus d'initialisation...}. \\
Il faut considérer un nombre suffisamment grand d'hypersphères (4096) pour observer que la méthode d'initialisation proposée contraint la distribution des sorties à tendre vers une loi normale. 

    
    
	\section{Conclusion}
	
	Le modèle de neurone hypersphérique de Banarer \textit{et al.} \cite{Banarer2003} a été étendu pour définir un filtre de convolution hypersphérique adapté aux tenseurs d'ordre $l$ à plusieurs canaux. Cette extension consiste à transformer les coefficients du filtre ainsi que le biais en coordonnées sur une hypersphère, permettant ainsi l'optimisation de ces paramètres.
	
	\bigskip
	
	
	Les expérimentations réalisées sur des cas simples visaient à valider et comparer ces modèles sur des architectures simples, telles que les couches denses ou Conv2d. Les résultats montrent que les taux de prédiction restent globalement similaires entre les réseaux classiques et les réseaux à couches hypersphériques pour une architecture de type Perceptron Multicouches. Il a été observé que les fonctions de perte convergent plus rapidement et atteignent des valeurs plus faibles avec les couches hypersphériques. De plus, en adaptant des réseaux convolutifs avec des filtres hypersphériques, les résultats de prédiction demeurent comparables à ceux obtenus avec des filtres classiques. Cependant, lors de la transition d'un réseau dense hypersphérique vers un réseau convolutif avec filtres hypersphériques, l'augmentation du taux de bonne prédiction est similaire à celle observée avec les réseaux classiques. 
	
	\bigskip
	
	
La question de l'initialisation des poids dans les couches hypersphériques a été explorée afin de garantir la possibilité d'enchaîner plusieurs couches successives. La sortie d'une couche hypersphérique dépend de plusieurs paramètres, dont le facteur d'échelle $\delta$ que nous avons introduit. Ce dernier joue un rôle essentiel dans la distribution des sorties, et une fois fixé, il permet de déterminer à la fois les valeurs des paramètres hypersphériques et les conditions d'initialisation : en particulier, l'intervalle auquel doit appartenir la moyenne des centres, ainsi que la variance de ces derniers lorsqu'ils sont initialisés selon une loi normale.\\

La méthode proposée permet non seulement de propager efficacement le signal à travers des couches hypersphériques successives, mais aussi d’augmenter le nombre de neurones par couche. Cela ouvre des perspectives intéressantes pour l'intégration des couches hypersphériques dans des architectures plus complexes. Néanmoins, l'ajustement des paramètres d'initialisation reste à affiner en fonction des différentes fonctions d'activation utilisées.
	