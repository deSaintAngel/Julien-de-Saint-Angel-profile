Ce chapitre est consacré à la construction de réseaux de neurones à couches hypersphériques. Leur définition est inspirée du modèle conforme défini par Hestenes et al. . Les neurones définissent des hypersphères en dimension \(n\), paramétrées par un vecteur de l’algèbre géométrique conforme \(R^{n+1,1}\) (les poids des neurones correspondent aux paramètres de l’hypersphère). Une approche similaire pour les couches denses a déjà été proposée par Banarer et al. . Ici, nous allons plus loin en introduisant également un modèle convolutif de couches hypersphériques.\\ L’implémentation de ces nouveaux types de couches, en utilisant des opérateurs d’algèbre linéaire ou de convolution classiques, est décrite en détail, ainsi que les contraintes pour la mise à jour des poids. Les premières expérimentations sur des données synthétiques et des bases d’images montrent que les variantes hypersphériques des couches denses et convolutives améliorent le comportement de la fonction de coût et permettent une convergence plus rapide, à nombre de paramètres égal. Cependant, ces couches se révèlent parfois plus sensibles à l’initialisation, ce qui peut entraîner une certaine instabilité. Nous nous penchons donc sur la question de l'initialisation. On montre d'abord pourquoi l'approche heuristique de Glorot et Bengio et donc la normalisation classique associée s'avèrent inefficaces dans le cadre des couches hypersphériques. Il faut donc comprendre comment une distribution suivant une loi de probabilité complexe se propage dans le réseau hypersphérique. Nous mettons alors en place une stratégie d'initialisation basée sur des propriétés asymptotiques des lois de type Gamma généralisées. La pertinence de la méthode est confirmée par les expérimentations numériques. Les réseaux de neurones "classiques" reposent sur un produit scalaire $ $ (appliqué localement dans le cas convolutif), où $$ représente l'entrée et $$ le vecteur de poids$ est augmenté pour intégrer le biais dans le vecteur des poids: $ +b = (,b) (,1)$.} (ou le filtre de convolution). Pour un vecteur $$ de taille $n$, le nombre de paramètres de la couche est $n+1$. Une fonction d'activation peut ensuite être ajoutée. Si on utilise une activation "", la sortie de chaque couche partitionne alors $^n$ en deux sous-espaces, séparés par un hyperplan (plus généralement, selon le choix de la fonction d'activation, les valeurs positives du produit scalaire sont conservées, tandis que les valeurs négatives sont atténuées ou annulées). \\ L'idée développée dans cette thèse est de faire une partition de $^n$ par des hypersphères plutôt que par des hyperplans. On note que la partition induite dans ce cas est faite entre deux sous-espaces dont l’un est compact. \\ Les hypersphères pourraient être paramétrées de manière classique, si bien qu'à l'étape d'apprentissage, l’optimisation des centres et des rayons serait distincts. Cette méthode serait donc similaire à ce qui est classiquement utilisé pour les fonctions à base radiale (réseaux dits RBF). Bien que cette approche soit valide, elle occulte le lien géométrique entre hyperplan et hypersphère : un hyperplan peut être considéré comme une hypersphère dont le centre est situé à l'infini et dont le rayon est infini. Pour répondre à ce besoin d'unification, nous adoptons dans la suite le formalisme des algèbres géométriques conformes, dans lequel hyperplans comme hypersphères sont représentés par un vecteur de dimension $n+2$. Dans ce cadre mathématique, Banarer et al. ont déjà défini un modèle de neurone hypersphérique qui constitue la base de ce travail. Nous allons l'incorporer dans des réseaux à couches denses ou convolutives. Mais nous commençons par quelques rappels sur le formalisme de l'algèbre géométrique conforme. On décrit ici des éléments concernant les algèbres géométriques et le modèle conforme. Le vocabulaire est mathématique mais, il faut garder à l'esprit que l'idée derrière la mobilisation des outils algébriques qui suivent est de fournir une représentation uniforme et ''simple'' des hyperplans et des hypersphères. Nous considérons l'espace euclidien $^n$, l'espace vectoriel sous-jacent $ R^n$ étant muni d'une base orthonormale $(e_1,,e_n)$. La construction suivante explique en particulier par l'utilisation de la projection stéréographique pourquoi le modèle d'espace auquel on aboutit est qualifié de conforme. On commence par plonger l'espace euclidien dans un espace de dimension $n+1$ au moyen de l'inverse d'une projection stéréographique : { x}=x_1 e_1 + + x_n e_n {2(x_1 e_1 + + x_n e_n) x_1^2 + + x_n^2 + 1}+ {x_1^2 + + x_n^2 - 1 x_1^2 + + x_n^2 + 1} e_+, où $e_+$ désigne le point à l'infini. Ensuite on homogénéise le résultat obtenu en ajoutant le vecteur $e_-$. Ceci donne le plongement de $^n$ défini par : { x}=x_1 e_1 + + x_n e_n x_1 e_1 + + x_n e_n+{(x_1^2 + + x_n^2) 2}(e_++e_-)+{(e_--e_+) 2}\\ { x} { x}+{(x_1^2 + + x_n^2) 2}(e_++e_-)+{(e_--e_+) 2}= x L'intérêt de ce plongement est qu'il va permettre de considérer les objets de base de la géométrie euclidienne comme des vecteurs. Pour rendre compte de cette idée, il est nécessaire d'introduire un peu de vocabulaire mathématique. Tout d'abord, on considère sur l'espace $^{n+1,1}$ muni de la base formée des vecteurs $e_1$,$$, $e_n$, $e_+$, et $e_-$ la forme quadratique Q_{n+1,1}=({ccccc}1 & 0 & & 0 & 0 \\0 & 1 & & 0 & 0 \\ & & & & \\0 & 0 & & 1 & 0 \\0 & 0 & & 0 & -1). Au couple $(^{n+1,1}, Q_{n+1,1})$, on associe l'algèbre géométrique conforme qui, formellement, est le quotient de l'algèbre tensorielle de $^{n+1,1}$ par l'idéal bilatère engendré par les éléments du type $a a-Q_{n+1,1}(a)$. On dispose alors d'un produit, dit géométrique, tel que pour tout vecteur $a$ de l'algèbre géométrique conforme, on a aa=a^2=Q_{n+1,1}(a). Plus généralement, le produit géométrique de deux vecteurs $a$ et $b$ s'écrit ab = {2}(ab + ba)}_{}+ {2}(ab - ba)}_{} avec {{2}(ab + ba)}=a._i b=b._i a= B_{n+1,1}(a,b) $B_{n+1,1}$ étant la forme bilinéaire symétrique associée à $Q_{n+1,1}$, et {{2}(ab - ba)}=a b=-b a. Traditionnellement, le produit $a._i b$ est appelé produit interne, il s'agit d'un réel. Le produit extérieur $a b$ est un bivecteur qui peut s'interpréter comme l'analogue en dimension deux d'un vecteur, { i.e.} ``un morceau'' de plan vectoriel muni d'une orientation. Le bivecteur E=e_+e_-=e_+ e_- détermine une décomposition dite conforme ^{n+1,1}=^{n}^{1,1} où $$ désigne la somme directe et $^{1,1}$ le plan de Minkowski. Plus précisément, si nous notons $e_=e_++e_-$, $e_0=(e_--e_+)/2$ (voir Li { et al} ), et a= a_1 e_1 + + a_n e_n + a_ e_ + a_0e_0, un vecteur générique de $^{n+1,1}$, on peut décomposer $a$ en a=_E(a)+_E^{}(a), où $_E$ est la projection définie par _E(a)=(a._i E)E=a_ e_+a_0e_0, et $_E^{}$ est la réjection définie par _E^{}(a)=(a E)E=a_1 e_1 + + a_n e_n permet de retrouver la partie euclidienne de $a$. \\ Pour définir la projection $_E$ et la réjection $_E^{}$, il faut utiliser le produit interne et le produit extérieur entre un vecteur et un bivecteur. Comme nous ne les utiliserons pas par la suite, nous omettons ici les détails. On vérifie facilement que e_{0}^2 = e_{}^2 =0,~~e_{}._i e_0 =-1 Désignons par $ P^{n+1}(e_,e_0)$ l'hyperplan de $ R^{n+1,1}$ normal à $e_$ et contenant $e_0$, donné par l'équation e_._i(a-e_0)=0, et par $ N^{n+1}$ le cône nul de $ R^{n+1,1}$ donné par l'équation a^2=a._i a=0. La représentation conforme de l'espace euclidien $^n$ associée à la décomposition conforme donnée par $E=e_+ e_-=e_ e_0$, est l'horosphère H^n(e_, e_0)= N^{n+1} P^{n+1}(e_,e_0)=\{a R^{n+1,1},\ a^2=0,\ e_(a-e_0)=0\}. Dans la suite, nous noterons $ x$ le vecteur de $ R^{n+1,1}$ image du plongement du point ${ x}$ de l'espace euclidien $ R^n$ dans cette horosphère, { i.e.} x={ x}+{{ x}^2 2}e_+e_0. En notant $$ et $$ deux points de l'espace euclidien $ R^n$, on a en particulier, ._i =(+{2}^2 e_{} + e_0)._i(+{2}^2 e_{} + e_0)= + {2}^2 ._ie_{}}_{0}+ ._ie_0}_{0}\\ +{2}||||^2 (._ie_{}}_{0} + {2}^2 ._ie_{}}_{0} + ._ie_{0}}_{-1}) + ._i}_{0} + {2}^2 ._ie_{}}_{-1} + ._ie_{0}}_{0}\\ = -{2}-^2 Cette formule permet de comprendre comment définir une hypersphère de $ R^n$ comme un vecteur de l'algèbre géométrique conforme et comment tester par un produit intérieur l'appartenance d'un point à cette hypersphère. Soit donc une hypersphère de centre $$ et de rayon $$, { i.e.} l'ensemble des $$ vérifiant~: -^2 = ^2. Avec les notations introduites précédemment, cette équation peut s'écrire sous la forme : ._i = -{2}- ^2=-{2}^2. On en déduit qu’en utilisant $._i e_{}=-1$ : ._i = -{2} ^2 ._i - {2} ^2 = 0 \\ ._i - (-{2}^2)(-1)=0 ._i -({2}^2)(x._i e_{})=0 \\ ._i -{2}^2e_{})}_{}=0 ._i = 0, \\ avec = + e_0 + {2} ^2 e_{} - {2}^2 e_{} Par conséquent, dans l'algèbre géométrique conforme, l'hypersphère de centre $$ et de rayon $$ correspond au vecteur $$ de $^{n+1,1}$ mentionné précédemment. On constate par ailleurs que deux contraintes de normalisation doivent être satisfaites : {l} ^2 = ^2 >0,\ \ e_{}._i = -1. Les propriétés que l'on retient pour la suite sont les suivantes : le point ${ x}$ est à l'intérieur de l'hypersphère si $$ 0 < ._i {2} ^2,$$ sur l'hypersphère si $$._i = 0,$$ à l'extérieur de l'hypersphère s'il vérifie $$._i < 0.$$ En particulier, ._i ={2}^2 x= c . Les figures et illustrent les variations du produit $._i $ pour une sphère de centre $(0, 0)$, de rayon égal à 5. [htbp] [height=10cm, width=10cm, grid= major , xlabel = {$x_0$} , ylabel = { $._i $} , xmin=-10,xmax=10, legend entries={${2}^2$}, ] coordinates {(0,12.25)}; coordinates {(-10,0) (10,0)}; table [x=a, y=b,col sep=comma] {sources/ProduitScalaire.csv}; ._i $} [H] ._i $} Il est possible de même de représenter l'hyperplan de $ R^n$ de vecteur normal unitaire $ n$ et passant par le point ${ a}$ par le vecteur $ h$ de l'algèbre géométrique conforme qui s'écrit h = n+ e_, avec $ = n$. On vérifie facilement que x._i h= n({ x}-{ a}). Si $x$ appartient à l'hyperplan représenté par $$ alors $._i=0$. Grâce à cette description, on peut vérifier que l'hypersphère représentée par le vecteur s=({ c}+ { n})+{( c+ n)^2 2}e_+e_0-{( +)^2 2}e_ { i.e.} l'hypersphère de centre ${ c}+ { n}$ et de rayon $ +$ tend vers l'hyperplan $ h$ précédent lorsque $>0$ tend vers l'infini. En effet, par la renormalisation e_._i s=-1/, l'hypersphère correspond au vecteur s= {( c+ n) }+{( c+ n)^2 2}e_+{e_0}-{( +)^2 2}e_\\ =({ c}+ n)+({{ c}^2 2}+{ c} { n}+{ 2}-{ 2}--{^2 2})e_+{e_0}. Quand $>0$ tend vers l'infini, alors ce vecteur tend vers h= n+( c n-)e_, avec a= c- n,\ \ n( c- n)= c n-. Ce type de calcul, donné en exemple, illustre bien la cohérence des définitions et des opérations du modèle conforme d'Hestenes. Les modèles de réseaux à couches hypersphériques se composent d'une ou plusieurs hypersphères, dont les paramètres, représentant le centre et le rayon, sont ajustés au cours de l'entraînement pour optimiser la séparation des données. Sans ajout de fonction d'activation, ces couches permettent de créer des frontières décisionnelles non linéaires, contrastant ainsi avec les couches traditionnelles. Dans les réseaux de neurones classiques, les couches sont généralement de type dense ou à convolution. Les couches de type dense permettent de réaliser des séparations complexes dans l'espace des caractéristiques, tandis que les couches à convolution appliquent ce principe de manière localisée, ce qui est particulièrement utile pour le traitement d'images. Nous montrons ici comment construire l'analogue de ces deux types de couches à partir du modèle conforme décrit précédemment. Une couche hypersphérique a été développée par en utilisant des hypersphères représentées par des vecteurs de \(^{n+1,1}\). Le schéma illustre un réseau de neurones avec une couche cachée utilisant une hypersphère. La première étape, intégrée dans l'implémentation de la couche hypersphérique, consiste à envoyer les données d'entrée dans l'espace de l'algèbre géométrique conforme en utilisant le plongement décrit par . Ensuite, il est nécessaire de prendre en compte la contrainte d'unicité, qui peut être considérée selon plusieurs méthodes décrites dans la section . La sortie de la couche pour une hypersphère de centre $$ et de rayon $$ s'écrit~: y = -{2} Le signe de \(y\) détermine la position relative du point \(\) par rapport à l'hypersphère, conformément aux propriétés établies précédemment. Cette quantité est donc calculée par le produit interne $_i $. [htbp] [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!120]; =[neuron, fill=green!100]; / in {1,...,2} (K-) at (0,-+1/2) {$x_{}$}; (K-3) at (0,-3+1/2) {$1$}; (K-4) at (0,-4+1/2) {${2}$}; / in {0} (J-) at (0,-1.75) {$y = ._i$}; / in {0} (K-1) edge (J-); / in {0} (K-2) edge (J-); / in {0} (K-3) edge[dashed,->,red] (J-); / in {0} (K-4) edge[dashed,->,red] (J-); On peut noter que la définition du produit interne induit les différences suivantes sur le paramétrage des couches hypersphériques~: Couche dense ($n+1$ paramètres) : le produit $ $ est linéaire en $x$ et n'est pas borné Couche sphérique ($n+1$ paramètres) : le produit $ ._i $ est quadratique en les coordonnées de $$ et est majoré par $^2/2$ Comme expliqué précédemment, les paramètres du vecteur conforme représentant l'hypersphère \(\) sont ajustés par l'apprentissage. Ses paramètres encodent à la fois le centre (les $n$ premières coordonnées) et une forme implicite du rayon à travers la composante en $e_{}$ qui sont ajustées par le processus d'apprentissage. L'extension du modèle de neurone hypersphérique à un filtre de convolution hypersphérique ne nécessite pas de nouveaux outils mathématiques. Cette extension sera décrite pour un filtre de convolution 2D discret, applicable au traitement des images ; toutefois, l'approche reste valable en toute dimension, indépendamment du mode de et des valeurs de choisies. La convolution standard est bien connue pour être équivalente au produit scalaire entre les versions vectorisées du bloc image et du filtre \( F \) auquel est ajouté un biais~: (I * F)[j,j']= _{d_j} _{d_{j'}} I[j+d_j, j'+d_{j'}] * F[d_j, d_{j'}] + b \\ = (I[j-d_j:j+d_j, j'-d_{j'}:j'+d_{j'}]) (F[-d_j:d_j, -d_{j'}:d_{j'}])+ b . où $d_j$ et $d_{j'}$ désignent les indices de parcours du filtre qui dépendent de la taille du filtre $d d$. De façon naturelle, on peut voir vec$(I[j-d_j:j+d_j, j'-d_{j'}:j'+d_{j'}])$ comme un vecteur $x_I(j,j')$ de $^{d^2}$ que l'on va plonger dans l'algèbre géométrique conforme $^{d^2+1,1}$. Le filtre de convolution est remplacé par une sphère en dimension $^{d^2}$ paramétrée dans cette même algèbre. Si l'on note $$ cette "nouvelle" convolution, on en définit le résultat comme suit~: (I s)[j,j'] = x_I(j,j') ._i s La valeur de sortie de ce filtre possède donc les mêmes propriétés que le produit interne $._i$ défini dans la sous-section précédente (voir figure ). L'extension de ce filtre aux images à $c$-canaux ne pose pas de difficultés. Dans cet esprit, on envisagera deux approches, ``Conv2d'' et ``DepthWiseConv2d'' (dans le cas où une hypersphère est définie par canal)~:\\ $(I s)[j,j'] = _c x_I(j,j',c) ._i s$ ($$ Conv2d)\\ $(I s)[j,j'] = _c x_I(j,j',c) ._i s_c$ ($$ DepthWiseConv2d) [H] Dans la base $\{e_1, e_2,,e_n, e_0, e_{}\}$, le vecteur $$ et la sphère $s$ de centre $$ et de rayon $$ s'expriment comme des vecteurs à $n+2$ coordonnées~: & := [x_1, x_2, , x_n, 1, {2} ||||^2]^{t}\\ (, ) & := [c_1, c_2, , c_n, 1, {2} (||||^2-^2)]^{t} \\ Pour garantir l'efficacité du calcul du produit interne $._i$, celui-ci va être exprimé comme un produit matriciel $^{t} M $. On rappelle que les règles de calcul suivantes doivent être vérifiées : \{\ {l r} e_j ._i e_{j'} = _{jj'}, & j,j' \{1,,n\}\\ e_j ._i e_{0} = e_j ._i e_{} = 0\\ e_{} ._i e_0 = e_0 ._i e_{} = -1 . avec \[ _{jj'} = 1 & j = j', \\ 0 & . \] On peut vérifier que la matrice $M$ qui traduit les relations données dans le système s'écrit~:\\ M =( {cccccc} 1 & 0 & & 0 & 0 & 0\\ 0 & 1 & & 0 & 0 & 0\\ & & & 0 & 0 & 0\\ 0 & 0 & & 1 & 0 & 0\\ 0 & 0 & & 0 & 0 & -1\\ 0 & 0 & & 0 & -1 & 0\\ ) \\ On peut alors vérifier (voir les calculs ci-dessous) les deux propriétés suivantes pour le produit de deux points ou d'un point et d'une hypersphère~:\\ ^{t} M = {2}(^2 - || - ||^2) ^{t} M = -{2}|| - ||^2 \\ Vérification : produit $ ._i $ entre deux points : ${lcl} ^{t} M & = & (^t) M \\ & = & (^t, 1 , {2} ^2) ( {ccc} Id_{n} & 0 & 0 \\ 0 & 0 & -1 \\ 0 & -1 & 0 ) ({c} \\ 1\\ {2} ^2 ) \\ & = & (^t, -{2} ^2, -1) ({c} \\ 1\\ ^2 )\\ \\ & = & ^t - {2} ( ^2 - ^2)\\ \\ & = & -{2} - ^2 $ Vérification : produit entre un point et une hypersphère : ${lcl} ^{t} M & = & (^t, 1 , {2} ^2) ({ccc} Id_n & 0 & 0\\ 0 & 0 & -1\\ 0 & -1 & 0 )({c} \\ 1\\ ^2 - ^2}{2} ) \\ & = & (^t, -{2} ^2, -1) ({c} \\ 1\\ ^2 - ^2}{2} )\\ \\ & = & {2}(^2 - - ^2) $ Pour le filtre hypersphérique, le calcul de $^{t} M $ peut être divisé en trois étapes distinctes : La convolution standard de l'image avec le filtre correspondant aux $n$ premières coordonnées de $$ redimensionnées à la taille $d d$. L'opération $-{2}||x||^2$ revient à faire la convolution de l'image par un filtre constant de valeur $-{2}$ et de taille $d d$. Le résultat est multiplié par l'avant-dernière coordonnée de $$ qui vaut 1. Une multiplication terme à terme est effectuée pour obtenir le tenseur contenant les coefficients correspondant au produit entre la composante $e_0$ de $$ et la composante $e_{}$ de $$. Le résultat de la convolution hypersphérique est la somme de ces trois termes. Par ailleurs, si l'image traitée contient plusieurs canaux, une nouvelle somme selon les canaux de profondeur permet de reproduire le comportement du réseau de type Conv2D. Une hypersphère est représentée de manière unique par un vecteur \(\) dont l'avant-dernière coordonnée est fixée à 1, conformément à la contrainte \( e_{} = -1\). Pour optimiser les paramètres de l'hypersphère, c'est-à-dire les coordonnées de son centre \(\) et la valeur de son rayon \(\), tout en respectant cette contrainte, plusieurs approches peuvent être envisagées :\\ Une première méthode consiste à effectuer la descente du gradient directement sur les $n+2$ coordonnées puis normaliser le vecteur résultat pour garantir \( e_{} = -1\). La normalisation consiste à diviser le vecteur dans la base $\{e_1, e_2,,e_n, e_0, e_{}\}$ par la composante en $e_0$. \\ Une autre approche fixe la composante \(e_0\) de \(\) à 1, et l'optimisation est alors effectuée uniquement sur les \(n+1\) autres éléments du vecteur. Cette méthode simplifie l'optimisation en réduisant le nombre de paramètres à ajuster, tout en maintenant la contrainte nécessaire.\\ C'est la méthode qui a été privilégiée dans les codes utilisés pour cette thèse.\\ Enfin, une perspective possible est de développer une méthode de descente de gradient qui opère directement dans l'espace des sphères, en garantissant que le produit \(e_0 e_{}\) reste égal à -1 tout au long du processus d'optimisation. Cela permettrait de maintenir la contrainte \(s ._i e_{} = -1\) de manière plus intégrée, évitant ainsi la nécessité de re-normalisation après chaque mise à jour. Dans ce paragraphe, nous évoquons deux autres familles de réseaux dont les noms laissent entrevoir une certaine parenté avec ceux manipulés dans cette thèse : les réseaux de neurones à fonctions à base radiale (RBF) et les réseaux de neurones de Clifford. Les fonctions à base radiales (RBF) sont en particulier bien connues pour leur capacité à approcher des fonctions non linéaires. Ce sont des fonctions dont la valeur dépend uniquement de la distance entre un point d'entrée et un centre fixe. L'approximation d'une fonction $f$ donnée par un réseau RBF avec $J$ fonctions radiales prend donc la forme~: f(x) _{j=1}^J w_j _j(|| - _j||) avec $_j : ^+ $. Un exemple classique est celui du noyau gaussien, $_j(|| - _j||) = e^{-_j^2|| - _j||^2}$ où $_j$ est un paramètre d'échelle s'apparentant au rayon d'une sphère. Le lien avec les réseaux de neurones à couches hypersphériques sera davantage détaillé dans la section , en particulier dans le cas $n=1$ où une formulation analytique des sorties d'un réseau à une couche hypersphérique permet de clairement discerner les différences ou les similarités avec celles d'un réseau RBF. L'algèbre géométrique conforme du modèle d'Hestenes étant un cas particulier des algèbres géométriques, tous les travaux portant sur les réseaux de neurones dits de Clifford (initialement introduits par Buchholz et Sommer , pour enrichir les réseaux neuronaux avec des propriétés géométriques) peuvent naturellement être rapprochés (puis exploités) de ce qui est fait ici. Les neurones de Clifford étendent le modèle du perceptron classique en remplaçant les opérations réelles par des opérations dans l'algèbre de Clifford. Deux types de neurones Clifford sont généralement utilisés~:\\ Neurone de Base Clifford (BCN) : Dans un neurone de Clifford, les entrées et les poids sont les composantes des multivecteurs appartenant à une algèbre de Clifford $C_{p,q}$. La combinaison linéaire classique est remplacée par le produit géométrique noté $_{p,q}$ (avec $$ un biais). La fonction de propagation d'un neurone Clifford de base est définie par~: $f() = _{p,q} + $ (multiplication à droite) $f() = _{p,q} + $ (multiplication à gauche) où $$, $$, $ C_{p,q}$ sont des multivecteurs. Comme dans notre cas, le produit scalaire usuel est remplacé par un produit géométrique. On notera que la structure est complexifiée par l'absence de commutativité.\\ Neurone Spineur Clifford (SCN) : le produit géométrique est étendu de manière à inclure des transformations orthogonales (comme les rotations) appliquées aux entrées. Les spineurs, qui sont des éléments de Clifford agissant comme des opérateurs de rotation, sont utilisés pour transformer les vecteurs d’entrée de manière contrôlée. La transformation de $$ s'écrit alors~: $y= _{p,q} _{p,q} + $, où $$ peut représenter l'inversion, la réversion ou la conjugaison par un neurone spineur Clifford. Les réseaux de neurones de Clifford sont construits en connectant plusieurs neurones de Clifford. On trouve dans la littérature trois catégories principales ~: Clifford Multilayer Perceptron (CMLP) : utilise des BCN avec des fonctions d'activation à valeurs réelles. Spinor Clifford Multilayer Perceptron (SCMLP) : utilise des SCN avec des fonctions d'activation à valeurs réelles. Clifford MLP avec Fonctions d'Activation à Valeurs Clifford (FCMLP) : utilise des fonctions d'activation à valeurs dans une algèbre de Clifford. L'apprentissage des neurones de Clifford s'effectue par une extension de la descente de gradient classique. Il faut noter que le produit de deux éléments non nuls d'une algèbre de Clifford peut être nul, ce qui induit quelques difficultés pour l'étape de rétropropagation du gradient. On utilise donc des involutions (une fonction qui, appliquée deux fois à un multivecteur, retourne le multivecteur d'origine ; cela peut être l'inversion, la réversion ou la conjugaison). Par exemple, l'algorithme de rétropropagation pour les CMLP (Clifford Multilayer Perceptrons) utilise une involution unique déterminée par l'algèbre de Clifford sous-jacente. Les outils développés dans cette thèse ne présentent pas autant de complexité technique pour leur implémentation. Les expérimentations qui suivent ont été mises en place pour valider les modèles de couches hypersphériques construits à partir d'un ensemble de neurones ou de filtres de convolution hypersphériques. Comme nous nous positionnons sur des briques élémentaires de bas niveau, nous avons volontairement choisi des architectures de réseau simples afin de limiter l'écueil du sur-apprentissage et des difficultés d'interprétation. \\ Nos expérimentations sont présentées en deux parties, sans puis avec convolution. couches denses} Deux jeux de données synthétiques ont été utilisés pour étudier la performance des réseaux à couches hypersphériques. Le premier nommé est construit de façon à avoir 3 classes quasiment linéairement séparables. Celui-ci contient 350 points en dimension 2, ce qui donne de plus l'opportunité d'illustrer les résultats par différentes images.\\ Le second nommé est constitué d'une série de 450 points en dimension 2 répartis en 3 classes non linéairement séparables avec un taux de chevauchement non négligeable. Cette structure nous permettra de visualiser et de comparer la forme des frontières de décision pour différentes architectures.\\ Les paramètres ayant servi à générer ces jeux de données sont donnés dans . Les deux jeux de données ont été séparés en ensemble d'apprentissage (66.66\ L'architecture de base que nous considérons est un perceptron multi-couches de faible profondeur. Dans un premier cas, les couches du réseau (noté PMC) seront des couches denses classiques. Le réseau nommé GeoPMC ne contient quant à lui que des neurones hypersphériques. On a évalué l'influence de différents paramètres : Le nombre de couches cachées varie de 0 à 2. Le nombre de neurones par couche cachée prend les valeurs 2, 3 ou 10. La batch normalisation et la fonction d'activation s'appliquent ou non sur toutes les couches cachées. Chaque réseau se termine par une couche de 3 neurones classiques ou hypersphériques avant de passer par la fonction d'activation softmax car il s'agit d'un problème de classification ; celle-ci n'est jamais précédée d'un . Pour faciliter la lecture des résultats, chaque architecture du réseau est codée par une chaîne de caractères : un chiffre correspond au nombre de neurones par couche, "r" à l'utilisation de la fonction d'activation relu, "b" à une batch normalisation et "sf" à la fonction d'activation softmax. La taille du batch est de 30, l'algorithme d'optimisation est Adam . Bien que l'ensemble des configurations ci-dessus ait été testé, seul un ensemble de configurations efficaces a été sélectionné pour ce manuscrit. Les figures et reportent le taux de bonne prédiction () sur les jeux de données de validation au cours des itérations. Pour des architectures PMC et GeoPMC identiques, on constate que l' augmente plus rapidement pour les modèles GeoPMC. L'échec du modèle GeoPMC "b 2 r b 2 r b 3 sf" s'explique probablement par son architecture qui contribue à l'inhibition de certains neurones () .[H] [height=9cm, width=9cm, grid= major , xlabel = {Epoch} , ylabel = {Accuracy} , xmax=200, legend entries={geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, }, legend style={at={(0.5,0)},anchor=south west,nodes={scale=0.6, transform shape}}], ] table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_3_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_10_3_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_2_2_3_bn_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_3_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_10_3_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_2_2_3_bn_relu_accuracy.csv}; pour les données de validation } [H] [height=9cm, width=9cm, grid= major , xlabel = {Epoch} , ylabel = {Accuracy} , xmax=200, legend entries={ geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, }, legend style={at={(0.5,0)},anchor=south west,nodes={scale=0.6, transform shape}}], ] table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_3_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_10_3_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_2_2_3_bn_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_3_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_10_3_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_2_2_3_bn_relu_accuracy.csv}; : pour les données de validation } Pour aller plus loin, la table reporte l' en fin d'entraînement des 14 réseaux les plus performants. Excepté les trois premiers cas, l'ensemble des réseaux efficaces sur le sont également sur . [H] {|c|c|c|c|c|} {|c|}{}&{|c|}{}&{|c|}{}\\ {|c|}{Architectures} & GeoPMC & PMC & GeoPMC & PMC\\ 10 r : 3 sf & 92\ b 2 r : b 2 r : b 3 sf & 41\ 3 r : 3 sf & 67.2\ 3 sf & 96.5\ b 2 : b 2 : b 3 sf & & & 91.3\ 10 : 3 sf & & 96\ 10 : 10 : 3 sf & & 94\ 2 : 2 : 3 sf & & 95.7\ 3 : 3 : 3 sf & & 95.7\ b 10 : b 3 sf & \ b 10 r : b 3 sf & & & 92\ b 3 r : b 3 sf & & & 92\ b 3 sf & 97.4\ b 2 : b 3 sf & & & 91.8\ Hormis l'architecture "10 r : 3 sf" qui s'avère être plus performante pour les réseaux à couches hypersphériques et le réseau "b 2 r : b 2 r : b 3 sf" qui lui semble être meilleur que le modèle classique sur les données de , l'ensemble des résultats d' ne sont pas significativement différents. Étant donnée la taille des jeux de données de validation, un écart de 1\ Afin de mieux voir l'impact de la fonction d'activation et de la batch normalisation, les tableaux et présentent les résultats de l'apprentissage par architecture. \\ [H] {|c|c|c|c|c|} {|c|}{GeoPMC}\\ Architectures & $$ & & bn & bn + \\ 10 : 3 sf & & 92\ 3 sf & 95\ 3 : 3 sf & 94\ 2 : 3 sf & 92.2\ 3 : 3 : 3 sf & 98.3\ {|c|}{PMC}\\ Architectures & $$ & & bn & bn + \\ 10 : 3 sf & 96\ 3 sf & 97.4\ 3 : 3 sf & 95.6\ 2 : 3 sf & 96.5\ 3 : 3 : 3 sf & 95.7\ : Comparaison de l'impact de et de la batch normalisation} [H] {|c|c|c|c|c|} {|c|}{GeoPMC}\\ Architectures & $$ & & bn & bn + \\ 10 : 3 sf & 92\ 3 sf & & ND & 92.4\ 3 : 3 sf & 92.6\ 2 : 3 sf & & 89.6\ 3 : 3 : 3 sf & 90\ {|c|}{PMC}\\ Architectures & $$ & & bn & bn + \\ 10 : 3 sf & & 92.6\ 3 sf & & ND & & ND\\ 3 : 3 sf & & & 92\ 2 : 3 sf & 92.6\ 3 : 3 : 3 sf & & 48\ : Comparaison de l'impact de et de la batch normalisation} De manière générale, la présence du semble dégrader davantage la qualité des prédictions sur les réseaux à couches hypersphériques, d'autant plus si le nombre de couches cachées croît. En conséquence, le ne semble pas être la fonction d'activation la plus adaptée sachant que $._i$ peut être un scalaire très négatif (cf. figure ). Une étude supplémentaire s'avère nécessaire pour en trouver une plus appropriée. En ce qui concerne la batch normalisation, elle permet dans le cas des données de , d'améliorer considérablement les résultats. Son utilisation semble moins pertinente pour les données issues du fichier .\\ En regardant maintenant les valeurs de la fonction de perte () sur le jeu de données de validation (figure et ), il est possible d'observer que les modèles GeoPMC convergent plus vite vers des valeurs plus petites. [H] [height=9cm, width=14cm, grid= major , xlabel = {Epoch} , ylabel = {Loss} , xmax=150, legend entries={geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, }, legend style={at={(0.5,0.5)},anchor=south west,nodes={scale=0.6, transform shape}}], ] table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_3_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_10_3_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_2_2_3_bn_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_3_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_10_3_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_2_2_3_bn_relu_loss.csv}; {font=} : Courbes des fonctions de pertes } [H] [height=9cm, width=14cm, grid= major , xlabel = {Epoch} , ylabel = {Loss } , xmax=150, legend entries={ geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, }, legend style={at={(0.5,0.5)},anchor=south west,nodes={scale=0.6, transform shape}}], ] table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_3_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_10_3_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_2_2_3_bn_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_3_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_10_3_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_2_2_3_bn_relu_loss.csv}; : Courbes des fonctions de pertes} En considérant $ ^2$ comme le domaine initial sur lequel le modèle est évalué, la pré-image est définie comme un sous-ensemble $$ associé à un ensemble d'éléments de sortie définis par $ ._i ()$. En d'autres termes, pour une hypersphère $$, la pré-image est définie par : \[ \{ ^2 ._i () = y \} \] où $y $ est une constante. Dans ce contexte, les courbes de niveau du produit $ ._i (x_k)$, où $$ est un vecteur associé à une hypersphère et $()$ est la représentation de l'exemple $$ dans un espace transformé (tout le réseau hormis la dernière couche), peuvent être interprétées comme des pré-images. Les lignes correspondant aux frontières de décision correspondent à l'ensemble des points où le produit $ ._i ()$ est nul. Comme les expérimentations sont effectuées sur des données 2D, il est possible de visualiser la pré-image d'un neurone hypersphérique paramétré par une hypersphère $s$ en affichant les bandes de niveau pour les valeurs positives de $ ._i $ (figure et ). [H] {.46} ._i=0$} {.46} } : } Si l'hypersphère à afficher est située dans la couche 0 ({ i.e.} pas de non-linéarité) (cf. figure ) ou qu'il n'existe pas de couches cachées (voir figure ), les bandes de niveau sont circulaires concentriques. \\ [H] {.45} {.45} : Bandes de niveaux pour\\ geomPMC "b 3 r : b 3 r : b 3 sf"} La figure permet d'illustrer la proximité entre les pré-images des hypersphères (situées avant la couche softmax) et les frontières de décision. Cela peut paraître évident au premier abord, mais si l'on se réfère à la figure , on sait que la frontière de décision entre deux hypersphères, en l'absence de non-linéarité, est un hyperplan... Il convient également de noter que les centres des hypersphères (colonne de gauche en jaune) diffèrent des centres des classes (colonne de droite). [h] {} : geoPMC "3 r : 3 r : 3 sf"} {} : geoPMC "10 r : 3 r : 3 sf"} La différence entre deux modèles GeoPMC et PMC de même architecture est flagrante si l'on compare leurs frontières de décision (cf. figure ). Si l'on regarde leur forme, PMC correspond à une succession de lignes brisées, alors que GeoPMC produit une succession d'arcs de cercle. [h] {.45} {.45} : Comparaison des frontières de décision} convolutions hypersphériques} Cette section propose une comparaison entre les filtres hypersphériques et Conv2d sur le jeu de données MNIST. Il s'agit d'une base de 70000 mini-images étiquetées, en niveaux de gris, de taille $28 28$ représentant l'écriture manuscrite des chiffres de 0 à 9. C'est donc un problème de reconnaissance à 10 classes.\\ Dans cette expérimentation, une architecture est testée avec la séquence de couches suivante~: une batch normalization ou non une couche hypersphérique pour GeoPMC / une couche Dense pour PMC / une couche à filtres hypersphériques pour GeoConv2d / une couche Conv2d pour Conv2d avec un certain nombre de filtres de taille $ 3 3$ une fonction ou non une batch normalization ou non une couche Dense ou hypersphérique avec un certain nombre de neurones une activation softmax Le tableau donne les résultats d' pour l'ensemble des configurations testées. [htbp] {|c|c|c|c|c|} {|c|}{}&{|c|}{Dense}&{|c|}{Convolution}\\ {|c|}{Architectures} & GeoPMC & PMC & GeoConv2d & Conv2d\\ 10 sf & & & {|c|}{ND} \\ 32 : 10 sf & 93\ 32 r : 10 sf & 10\ b 32 : b 10 sf & 86.4\ b 32 r : b 10 sf & 10\ 512 : 10 sf & 93\ 512 r : 10 sf & 10\ b 512 : b 10 sf & 93\ b 512 r : b 10 sf & 10\ pour différents modèles} Une première observation concerne l’incapacité de GeoPMC à fonctionner en "grande" dimension avec une activation . Le taux d'erreur de 90\ car les hypersphères sont définies en dimension 9 au lieu de 784. Plus généralement, il serait pertinent de se demander si les couches hypersphériques sont sensibles à la malédiction de la dimension. On constate la même amélioration lors du passage des modèles GeoPMC vers GeoConv2d que celui de PMC vers Conv2d. D’après nos premières expériences, GeoConv2d et Conv2d produisent des résultats identiques. L'initialisation des paramètres dans une couche comportant plusieurs neurones hypersphériques représente un défi complexe, mais qu'il fallait relever au vu des expérimentations menées précédemment. Pour se convaincre encore de l'importance de la question de l'initialisation, on peut aussi évoquer dans ce préambule le comportement du volume d'une hypersphère en grande dimension. On rappelle en effet que le volume d'une hypersphère en fonction de la dimension $n$ pour un rayon $$ fixé est le suivant~: $$V() = {2} n} ^n}{({2} n + 1)} .$$ On peut constater dans la figure que le volume d'une hypersphère tend rapidement vers zéro lorsque la dimension $n$ de l'espace des données augmente. Pour un rayon $$ proche de 1, le volume devient quasiment nul dès que la dimension atteint 40. Cela devient problématique pour les données de dimensions encore plus élevées et peut donc compliquer les étapes d'initialisation. En effet, comme le volume décroît fortement avec l'augmentation de la dimension, à la limite, tout point de l'espace se situe en dehors des hypersphères comme l'on peut le voir sur la figure (le produit $ ._i $ décroît linéairement vers des valeurs négatives au fur et à mesure que la dimension augmente). [H] [H] ._i $ en fonction de la dimension de l'espace des données} Le paragraphe est organisé de la façon suivante. \\ Nous commençons au paragraphe par une présentation des méthodes aujourd'hui les plus utilisées pour l'initialisation des réseaux à couches denses. C'est en effet de ces méthodes que nous avons décidé de nous inspirer, en particulier de l'approche heuristique de Glorot et Bengio . D'autres stratégies d'initialisation auraient pu être envisageables. Par exemple, à l'instar des réseaux utilisant des fonctions radiales, il est possible de déterminer les centres à l'aide d'un algorithme de clustering et d'ajuster les rayons en fonction de la dispersion des données autour de ces centres. Cependant, ce type d'approche souffre de deux défauts. D'abord, il est crucial de ne pas confondre les centres des hypersphères avec ceux des clusters. Ensuite, et surtout, une telle stratégie qui sépare les centres et les rayons des sphères et en plus les traite avec des outils totalement différents, va à l'encontre de la philosophie de cette thèse, basée sur des outils d'algèbre géométrique et un apprentissage le plus direct possible.\\ Nous choisissons donc une stratégie de normalisation à la Glorot. Pourtant, et c'est l'objet du paragraphe suivant, nous montrons rapidement que la méthode usuelle n'est pas du tout appropriée. Nous en illustrons les conséquences. Nous identifions également la cause : une distribution normale passée dans un neurone hypersphérique ne suit plus une loi normale. Le paragraphe présente les calculs de variance qui justifient la pertinence de la méthode d'initialisation que nous allons proposer. La loi de probabilité adaptée à la structure des neurones hypersphériques est la loi Gamma Généralisée. Une difficulté est que sa complexité permet très peu de calculs explicites et que ses propriétés sont mal connues. Nous la contournons en utilisant quelques propriétés asymptotiques des lois Gamma Généralisées. Le paragraphe est à la fois très calculatoire et très probabiliste. Le lecteur peut passer les détails et aller directement en Section pour une version synthétique des règles d’initialisation obtenues et pour des expérimentations qui permettent à la fois de confirmer la pertinence de ces règles et d’illustrer les arguments mathématiques sous-jacents. Un test de la méthode d'initialisation proposée est présenté au paragraphe , sur des données synthétiques suivant une distribution normale, composées de points en dimension 4. } L'initialisation des réseaux de neurones est une question qui a été largement explorée. Dans ce manuscrit, nous ne présentons pas d'état de l'art mais faisons un focus sur un type d'approche, dont des exemples emblématiques sont les travaux de Glorot (adaptés aux fonctions d'activation type tangente hyperbolique) ou de He (adaptés aux fonctions d'activation }). Comme déjà mentionné, nous pensons en effet ces stratégies plus adaptables aux réseaux hypersphériques. Pour une revue de l'état de l'art, nous mentionnons par exemple les articles , , ainsi que .\\ Par initialisation { à la Glorot}, nous désignons des stratégies consistant à une forme de normalisation (voir aussi LeCun et al. ) des points du réseau sensée garantir la propagation d'un signal à travers les couches profondes, en évitant qu'il ne se déforme trop. Typiquement on ne veut pas que l'amplitude du signal explose ou au contraire tende vers zéro. Simultanément l'approche vise à maintenir les poids dans une plage de valeurs raisonnable. En pratique, si la distribution des éléments $x_i$, $i \{1, , n\}$, en entrée du réseau suit une loi normale, on choisit la distribution des poids du réseau de telle sorte que les éléments $y_j$, $j \{1, , m\}$, en sortie du réseau suivent la même loi normale (même moyenne, même variance). Pour que les calculs soient faisables, on suppose que les poids du réseau suivent également une loi normale.\\ Nous allons détailler les approches de Glorot et He dans les lignes qui suivent. L'objectif est de comprendre ces méthodes dans le contexte classique des réseaux de neurones avant de les adapter au cas plus complexe des neurones hypersphériques. On se place donc ici dans le cas `classique'' où la sortie d'un neurone de type dense est donnée par la forme linéaire suivante~: y_j = _{i=1}^{n} w_{ji} x_i les $w_{ji}$ désignant les poids du réseau dont l'initialisation doit être choisie. On suppose que $x_i$, $1 i n$, sont des variables aléatoires indépendantes et identiquement distribuées, suivant la loi normale de moyenne $_x = 0$ et d'écart-type $_x$. On veut que les sorties $y_j$, $1 i m$, suivent encore loi normale de moyenne $_x = 0$ et d'écart-type $_x$. On fait le pari qu'on va arriver à nos fins en faisant des hypothèses relativement simple sur la structuration des poids : On suppose que $w_{ji}$, $1 j m$, $1 i n$, sont des variables aléatoires indépendantes et identiquement distribuées, suivant la loi normale de moyenne $_w = 0$ et d'écart-type $_w$. On va maintenant calculer la variance des $y_j$, $1 i m$, définis par et jouer sur le seul paramètre libre du problème, c'est-à-dire $_w$ pour arriver à nos fins. Pour la suite, au vu de la formule , on a besoin de calculer en particulier la variance de produits de variables aléatoires indépendantes. On rappelle donc que si $X$ et $Y$ sont des variables aléatoires indépendantes, alors (XY) = ^2(X) (Y) + ^2(Y) (X) + (X) (Y). En effet, on a (XY) &=& (X^2Y^2)-^2(XY) = (X^2,Y^2)}_{0 }+(X^2)(Y^2)-^2(XY) \\ &=& - ^2(XY) \\ &=& (X) (Y)+ ^2(X) (Y)+^2(Y) (X) + ^2(X) ^2(Y)-^2(XY)}_{0 } \\ \\ La sortie d'un neurone est exactement donnée par . On a donc $$ (y_j)= (_{i=1}^{n}w_{ji} x_i)=_{i=1}^{n}(w_{ji} x_i) $$ En appliquant la formule établie dans la remarque précédente, on obtient (y_j) = _{i=1}^{n} (w_{ji}) (x_i) Les distributions étant identiques, on arrive finalement à : $$ (y_j)=n _w^2 _x^2.$$ Puisqu'on veut $(y_j) = _x^2$, on conclut que le résultat est obtenu si $$ n _w^2 = 1.$$ Le travail n'est pas terminé. En effet pour l'apprentissage il faut aussi envisager l'étape de rétro-propagation. Notons $L$ la fonction de perte. On veut maintenant que $$ ( { x } ) = ( { y } ).$$ Intuitivement, les gradients doivent conserver une certaine homogénéité au fil des passages entre couches. Or $ { x } = { y } { x }$ (dérivation en chaîne) où la différentielle ${ x }$ se calcule facilement grâce à la formule définissant $y$ comme une fonction linéaire de $x$. On a : { x_i} = _{j=1}^m w_{ji} { y_j}. On remarque facilement la similitude des définitions et . Comme la moyenne des $w_{ji}$ est nulle ($_w=0$), les moyennes des autres termes dans le sont aussi. Pour le calcul des variances, seul le nombre de termes dans la somme change. On arrive donc à la condition $$ m _w^2 = 1.$$ Si $m n$, il faut finalement faire une hypothèse sur $_w$ qui permette de faire un compromis entre les conditions $ n _w^2 = 1$ et $ m _w^2 = 1$. On aboutit ainsi à la condition sur l'initialisation des poids suivante : _w^2={n+m} . \\ On aurait pu aussi supposer que la distribution des poids suit une loi uniforme sur $$. On a alors var$(w_j)=r_w^2/3$, si bien que la condition ci-dessus devient $r_w = $. \\ Dans ce cas, l'équation est remplacée par $$ y_j = f( _{i=1}^{n} w_{ji} x_i )$$ où $f$ désigne la fonction d'activation. Les calculs de variances sont alors bien sûr plus techniques. Néanmoins, si la fonction $f$ est régulière, on peut l'approcher par une fonction linéaire { via} son développement limité en 0 (puisque les entrées sont supposées de moyenne nulle). Les calculs ci-dessus ne sont donc que très peu modifiés. On peut le faire par exemple si la fonction d'activation $f$ est une fonction logistique ou une tangente hyperbolique : au voisinage de 0, $ x$ (résultat ci-dessus inchangé), $1/1+e^{-x} 1/2+x/4$ (résultat ci-dessus inchangé à la constante près). Si la fonction $f$ n'est pas régulière, mais si elle suffisamment simple pour calculer les intégrales correspondant à la définition des variances, on peut également adapter les calculs précédents : c'est le cas par exemple de la fonction . Les résultats sont résumés ci-dessous. \\ Le tableau présente une synthèse des résultats concernant les écarts types et les bornes de l'intervalle considéré, en fonction de l'utilisation de la loi normale ou uniforme pour l'initialisation des poids.\\ [h!] {|c|c|c|} activation & Loi normale & Loi uniforme \\ - & $_w = {n + m}}$ (Glorot) & $r_w = {n + m}}$ (Glorot) \\ & $_w = {n + m}}$ (Glorot) & $r_w = {n + m}}$ (Glorot) \\ & $_w = 4 {n + m}}$ (Glorot) & $r_w = 4 {n + m}}$ (Glorot) \\ & $_w = {n + m}}$ (He) & $r_w = {n + m}}$ (He) \\ Notre stratégie va consister à adapter l'approche présentée au paragraphe précédent pour des couches hypersphériques, { i.e.} des sorties de la forme y_{j} = _{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+^{2}_{j} Les calculs sont bien sûr rendus plus techniques par la forme quadratique de la sortie hypersphérique. Ils sont détaillés au paragraphe suivant. Mais on va d'abord voir qu'on ne peut pas se placer dans un cadre aussi simple que celui qui précède. Plus précisément, travailler sur des lois normales et centrées ne permet pas d'obtenir une initialisation pertinente. Nous allons en donner une illustration pratique. Puis nous le justifierons par des arguments probabilistes. Nous considérons des données, les entrées $x_i$, $1 i n$, qui suivent une loi normale et centrée. Pour l'initialisation des hypersphères, les centres sont initialement tirés aléatoirement selon une loi normale centrée, avec un écart-type ajusté en fonction de la dimension $n$ et du nombre de neurones dans la couche obtenu pour des couches classiques ; ils respectent la règle établie au paragraphe mais dans le cas où d'une loi normale { centrée}.}. Les rayons des hypersphères sont initialisés à 1. On peut observer la propagation du signal à travers les trois premières couches à la Figure . [H] Clairement, l'initialisation n'est pas efficace. \\ On observe une explosion de l'écart-type des valeurs dès la sortie des premières couches hypersphériques. Ce phénomène se manifeste également dans les couches à filtre de convolution hypersphériques. La figure illustre la distribution des données d'entrée et de sortie un tenseurs de taille $(3,1,28,28)$ initialisés selon une loi normale. Cela correspond, par exemple, à un batch de trois images de taille $28 28$. Chaque colonne présente un tenseur différent, tandis que chaque ligne montre l'évolution des distributions à travers les couches successives, depuis la première couche jusqu'à la quatrième couche à filtre de convolution hypersphérique. [H] Dans ces expérimentations, au-delà de l'explosion des écarts type, un élément frappant est le fait qu'on perd très vite la symétrie de la distribution initiale. En fait on voit dans la figure que la distribution à la sortie de la première couche ne suit déjà plus une loi normale. L'hypothèse de normalité de la distribution au fil des couches étant à la base de tous les calculs précédents, c'est évidemment un souci : on sait conserver la moyenne et la variance d'une distribution normale au travers de la couche hypersphérique mais dès la première sortie la distribution n'est plus normale et plus rien ne sera assuré à partir de la deuxième couche. Ce point peut être confirmé par les éléments de probabilités fondamentales qui suivent. Il est la principale difficulté de ce travail d'initialisation. La densité de probabilité d'une loi normale d'espérance $$ et d'écart-type $$ est donnée par $$ x {} e^{-{2}({})^2}.$$ Si on compose cette fonction avec une fonction affine $x wx+b$, caractéristique d'une couche dense classique, on obtient une densité $$ x {} e^{-{2}({})^2}$$ qui correspond toujours à une loi normale. L'approche {\`a la Glorot} a du sens : une distribution normale est transformée par la couche dense en une distribution normale. Mais si la densité de Gauss est composée avec la fonction carrée, fonction qui apparaît dans l'opérateur hypersphérique ({ cf. )}, la structure de la loi normale disparaît : $$ x {} e^{-{2}({})^2}.$$ La couche hypersphérique ne peut donc transformer une distribution normale en une distribution normale. En fait (voir par exemple ), si une variable aléatoire réelle $X$ suit une loi normale centrée d'écart-type $$, noté $X (0,^2)$, alors $X^2$ suit une loi Gamma, $X^2 G(1/2,2^4)$. Cette dernière n'est pas stable par le passage au carré : si $X$ suit une loi Gamma, alors $X^2$ suit une loi dite { Gamma Généralisée}. On a enfin abouti à une loi suffisamment stable pour nos opérations hypersphériques car si $X$ suit une loi Gamma Généralisée alors toute puissance de $X^p$, $p ^*$, suit aussi une loi Gamma Généralisée. Ce point est confirmé par la forme de la densité d'une loi Gamma Généralisée, donnée par une fonction $$ x e^{-(x/a)^p}}{(d/p)},$$ où $d>0$ et $p>0$ sont des paramètres de forme, $a$ est un paramètre d'échelle et $$ désigne la fonction Gamma. La prépondérance de la distribution Gamma Généralisée aux sorties hypersphériques est aussi confirmée par la forme des sorties observées dans la Figure (l'impression visuelle peut être confortée par des travaux de fitting tels que ). Pour les premières propriétés des lois Gamma Généralisées, nous renvoyons le lecteur par exemple à ou où la bibliographie est bien présentée. Pour reproduire les calculs du paragraphe , des propriétés plus spécifiques sont nécessaires. La difficulté est qu'il faut calculer la variance et la moyenne de produits et de sommes impliquant au moins une distribution Gamma Généralisée.\\ De ce point de vue : La somme de deux variables aléatoires réelles suivant deux lois Gamma Généralisées ne suit pas { a priori} une loi Gamma Généralisée (voir ) puisque la somme de deux distributions normales suit une loi normale. Voir par exemple }. Le produit de deux variables aléatoires réelles suivant respectivement une loi Gamma Généralisée et une loi normale ne suit pas une loi Gamma Généralisée (voir ). Le même problème se pose en fait pour le produit de deux variables aléatoires réelles suivant une loi normale. Cependant, voir par exemple , les moments caractéristiques de la loi produit sont calculables et on peut donc la rapprocher d'une loi normale. Pour obtenir des formules explicites pour les calculs de variance, peu de choix s'offrent à nous. \\ On peut penser à la méthode de Stein pour déterminer les fonctions densités de probabilité au fil des couches. On peut consulter pour des calculs de ce type dans le contexte des lois Gamma. On voit que c'est extrêmement technique. De plus, le calcul des variances et des moyennes { via} l'intégration des densités de probabilité doit ensuite être approché numériquement. Pour les premières briques dans le contexte Gamma Généralisée, on peut consulter , , et . \\ La question de l'approximation numérique est d'autant plus sensible que des distributions Gamma Généralisées de forme très semblables peuvent avoir des paramètres $(a,d,p)$ très différents (voir ). Au vu des éléments précédents, nous allons mettre en place la stratégie suivante :\\ Une loi Gamma Généralisée est caractérisée par trois paramètres. La méthode { à la Glorot} consiste à en imposer seulement deux.\\ L'idée est d'influer sur le degré de liberté restant pour forcer les distributions Gamma Généralisées en sortie de chaque couche à s'approcher d'une distribution normale. \\ Pour parvenir à rapprocher chaque sortie d'une distribution normale, en grande dimension, nous serons aidés par le théorème central limite. Au vu de l'expérimentation précédente ce n'est évidemment pas suffisant.\\ Afin de ne pas introduire dans le problème de variable supplémentaire, nous allons peser sur les moyennes et les variances. Illustrons la méthode avec quelques calculs simples. On considère une variable aléatoire réelle $X$ suivant une loi Gamma Généralisée, $X (a,d,p)$. On sait alors que $X^2 (a^2,d/2,p/2)$. \\ Comparons les moyennes : && (X) = a {p})}{({p})}, \\ && (X^2) = a^2 {p})}{({p})} =(X) a {p})}{({p})} . Dans le cas où $p=1$ (c'est le cas où la loi Gamma Généralisée est une loi Gamma, en sortie de première couche si l'entrée est gaussienne par exemple) et pour un paramètre de forme $d 1$ (ce qui est le cas observé), on voit que $$ (X^2) = a(d+1) (X).$$ Ainsi, la moyenne se décale vers la droite. Cet effet indésirable s'observe d'ailleurs dans les sorties hypersphériques ci-dessus (avec un décalage vers la gauche à cause du signe ``$-$'' dans ). Un moyen de limiter ce décalage est de réduire la valeur de $a$. Et un moyen direct d'influer sur la valeur de $a$ est de réduire la variance de la distribution $X$. Comparons maintenant les variances : && (X) = a^2 {p})({p})-({p})^2}{({p})}, \\ && (X^2) = a^4 {p})({p})-({p})^2}{({p})} . Une fois encore, réduire la valeur de $a$ permet de contrôler l'explosion de la variance. Ces considérations sur la partie carrée de la sortie hypersphérique sont confortées par les travaux de Aroian et al. (voir aussi pour des expérimentations numériques) qui établissent que le produit de $X (_x,_x^2)$ par $S (_s,_s^2)$ tend vers une distribution normale si {_x} + {_s} 1 . la stratégie consiste à assurer des sorties normales en diminuant la variance des données en entrée en initialisant les poids avec une moyenne non nulle On reprend donc les calculs { à la Glorot} dans le paragraphe suivant, en les adaptant aux sorties hypersphériques, mais en conservant l'hypothèse de normalité. Cette partie est très calculatoire. Tous les détails sont fournis pour assurer la reproductibilité des résultats. Nous invitons le lecteur qui veut aller directement aux résultats à consulter le paragraphe suivant. Comme dans l’approche classique, le calcul des variances entre les entrées et les sorties vise à établir une condition pour maintenir une variance constante du signal à travers les couches du réseau. En imposant l’égalité entre la variance des entrées et celle des sorties, on peut relier les paramètres de l’hypersphère aux caractéristiques des données d’entrée. Cela permet d’imposer des contraintes sur les paramètres du modèle afin d’assurer une propagation adéquate du signal. L'objectif principal est de déterminer les valeurs d'initialisation des poids du réseau pour minimiser les variations indésirables du signal durant l'apprentissage. Ici, on fixe le rayon des hypersphères et on joue sur la distribution des centres. )} Pour l'initialisation, on introduit un paramètre supplémentaire $ >0$ dans le réseau. Ce paramètre nous permettra de gagner suffisamment de latitude pour parvenir à nos fins. En particulier, il va jouer le rôle de facteur d'échelle pour ramener les données à des quantités à variance petite (conformément à la stratégie construite ci-dessus). On considère donc que les éléments $y_j$ du vecteur de sortie $Y$, sont donnés par $y_{j}= ._i_j}{}$, c'est-à-dire : y_{j} = {2} 1 j m, le nombre $m$ étant le nombre de sphères (neurones hypersphériques) dans la couche. On suppose les données identiquement distribuées suivant une loi normale : pour tout $i \{1, , n\}$, $x_i (_x, _x^2)$. On fait une hypothèse similaire pour les sorties $y_j$ et les poids du réseau $s_{ji}$ : pour tout $i \{1, , n\}$, tout $j \{1, , m\}$, $y_j (_y, _y^2)$, $s_{ij} (_s, _s^2)$. Le rayon $$ est supposé fixe. Pour faciliter la lecture, on rappelle le résultat suivant. Soit $x$ une variable aléatoire telle que $X (, ^2)$. Les moments centrés peuvent être calculés en fonction des moments d'ordre $p$ en développant l'expression $(x-)^p$ puis en appliquant les propriété de linéarité de l'espérance . On a donc $$ {ll} & = \\ ^2 & = - 2 { } + ^2 \\ ^2 & = - 2 {} + ^2 \\ $$ $$ {ll} & = \\ 0 & = - 3 { } + 3 ^2{ } -3 ^3 \\ 0 & = - 3 {(^2 + ^2)} + 3 ^2{} -3 ^3 \\ $$ $$ {ll} & = \\ 3 ^4 & = - 4 { } + 6 ^2{ } -4 ^3 { } + ^4\\ 3 ^4 & = - 4 {(3^2+^3)} + 6 ^2{(^2+^2)} -4 ^3 {} + ^4\\ $$ Le calcul des quatre premiers moments donne en particulier les résultats suivants~: $${l l } = &= ^2+^2\\ \\ = 3^2+^3 & = 3^4+6^2^2+^4. $$ Le calcul de l'espérance de $y_j$ donne, pour tout $1 j m$ :\\ ${ll} (y_{j}) & =({2})\\ \\ & =({2})+({2})\\ \\ & ={}(x_{i} s_{ji} )-{2}(x^{2}_{i} )-{2}(s^{2}_{ji} )+{2}(^{2}_{j} )\\ \\ & ={}(x_{i})(s_{ji})-{2}(x^{2}_{i} )-{2}(s^{2}_{ji} )+{2} $ Pour assurer que $(y_{j})= _x$, il faut donc : {ll} _x &= {}_x _s-{2}(_x^2+_x^2)-{2}(_s^2+_s^2)+{2}\\ \\ & = {2}\\ \\ & = {2}\\ égalité qui revient à~:\\ {ll} ^2 & = 2 _x + n ou~:\\ {ll} & =\\ \\ Passons à la recherche d'une condition induite par l'égalité des variances en entrée et en sortie. On rappelle que si $X (,^2)$, alors var$(X^2) = -^{2} = 2 ^2 (^2 + 2 ^2 )$. En utilisant la formule , on calcule la variance de $2x_i s_{ji}$ de la façon suivante~: {ll} (2x_i s_{ji}) & =4 (s_{ji}) (x_i) + 4 ^2 (x_i) +4 ^2 (s_{ji})\\ \\ &=4 _s^2 _x^2 + 4 _x^2 _s^2 + 4 _s^2 _x^2 Or on a~: {l l } (x_i)= _x^2 & (x_i^2)= 2 _x^2 (_x^2 + 2 _x^2 )\\ {l l } (s_{ji})= _s^2 & (s_{ji}^2)= 2 _s^2 (_s^2 + 2 _s^2 ) {l l } ( 2x_i s_{ji}-x_i^2, -s_{ji}^2) & = -4 _s _x _s^2\\ ( 2x_i s_{ji},-x_i^2 ) & = -4 _s _x _x^2\ L'annexe détaille les calculs des termes de covariance. \[ {ll} (y_{j}) & = ({2})\\ \\ & = {4^2}() { } \\ \\ & = {4^2} (2 x_{i} s_{ji} - x^{2}_{i} - s^{2}_{ji})\\ \\ & = {^2} (x_{i} s_{ji}) + {4^2} (x^{2}_{i}) + {4^2} (s^{2}_{ji})\\ \\ & + {2^2}(2x_i s_{ji}-x_i^2, -s_{ji}^2) + {2^2}(2x_i s_{ji},-x_i^2) \\ \] En utilisant les relations --, on obtient l'expression suivante~: (y_j) &={4 ^2} (4_s^2_s^2 + 4_s^2_x^2 - 8_s_x_s^2 - 8_s_x_x^2 + 4_x^2_s^2 + 4_x^2_x^2 + 2_s^4 + 4_s^2_x^2 + 2_x^4 )\\ \\ &= {2 ^2} (_s^2 + _x^2) ( 2(_x - _s)^2 +_s^2 + _x^2 ) On veut que var$(y_j)=_x^2$ pour tous $i \{1, , n\}$ et $j \{1, , m\}$. Cela se traduit par l'equation~: {l } 2_s^2{_s^2} + 2_s^2_x^2 - 4_s_x{_s^2} - 4_s_x_x^2 + 2_x^2{_s^2} + 2_x^2_x^2 + {_s^4} + 2{_s^2}_x^2 + _x^4 - {n}_x^2=0 \\ L'équation est une équation bicarrée en $^2_s$. On peut donc la résoudre explicitement avec la méthode du discriminant. En ne conservant que la solution qui peut être positive, on arrive à _s^2 = -(_s- _x )^2- _x^2 + {n}} \\ Garantir la positivité de $^2_s$ revient à s'assurer que son numérateur est positif étant donné que le coefficient devant $^4_s$ est positif, c'est à dire la condition suivante~: _x^2 (-2_s^2 + 4_s_x - 2_x^2 - _x^2 + {n}) >0 Pour s'en assurer, il faut cette fois résoudre une autre équation du second degré en $_s$. Il existe une solution réelle si son discriminant satisfait $ _{} = - 8 _x^2 + {n}^2 > 0$. On en tire la condition suivante sur $$~: 16 ( {n} - {2}) >0 \\ c'est-à-dire~: \\ > {2} _x^2 } \\ On vérifie alors que la solution donnée par est bien définie si $_s$ satisfait~:\\ _s \\ Pour résumer, les calculs précédents aboutissent aux conditions suivantes~:\\ \{{ll} ^2 & = 2 _x + n \\ \\ _s^2 & = {n}_x^2 }- . pour $$ vérifiant . Nous nous sommes également intéressés au cas des couches de neurones à filtres convolutifs ({ cf} Annexe ), afin de déterminer comment la covariance entre deux éléments de la sortie est reliée aux contraintes du système d’équations établi précédemment. Des travaux comme ont de plus montré comment exploiter la structure de covariance d'un filtre convolutif peut permettre d'améliorer l'initialisation. Dans cette partie, l'impact de la variance de la fonction de coût sur les poids est étudié. La sortie d'un neurone de la couche $l$ est désormais notée $^l$, définie par $${ll} ^l_j = {}^l}._i & = {}\\ \\ & = {} $$\\ où $^l_j$ correspond à la partie euclidienne du vecteur $S^l$, soit les coordonnées du centre de l'hypersphère. On a ainsi, \\ $${ll} ^{l+1}_{j'} = & {} $$\\ Si $f$ représente la fonction d'activation telle que $f'(0) = 1$, la sortie de la couche est définie par ^{l+1} = f(^l)$}.\\ On note $L$ la fonction de coût. Pour adapter au cas hypersphérique le raisonnement effectué dans le cas classique pour obtenir l'équation _{}$ la matrice de poids des centres des hypersphères, alors on note $^{l+1}_{j}$, la $j$ ième ligne de $S^{l+1}_{}$ qui correspond au vecteur du centre pour un $j$ donné.}, on calcule la dérivée partielle de $^{l+1}$ par rapport à $^{l+1}_{j}$~: \\ ^{l+1}_{}}{ ^{l+1}_{j}}={} \\ ce qui conduit à l'équation suivante~:\\ { ^{l}_{j}} = {}(^{l+1}_{j}- ^{l+1}_{j}){ ^{l+1}_{}} soit~: { ^{l}_{}} = {}(S^{l+1}_{}- ^{l+1}_{}){ ^{l+1}_{}} \\ avec $S^{l+1}$, la matrice dont les coefficients sont les centres des hypersphères. L'opération $(S^{l+1}_{}- ^{l+1}_{})$ consiste donc à soustraire le vecteur $^{l+1}$ à chaque colonne.\\ De , on tire d'abord un résultat sur les moyennes :\\ $$( { ^{l}_{j}} ) = {} (S^{l+1}_{j}- ^{l+1}_{j}) ( { ^{l+1}_{j}} ) = {} (_s^{l+1} - _x^{l+1}) ( { ^{l+1}_{j}} ).$$ On a assuré au paragraphe précédent que la moyenne des distributions de sortie reste constante au cours du processus : $_x^{l+1} = _x$. Ainsi\\ ( { ^{l}_{j}} ) = {} (_s^{l+1} - _x) ( { ^{l+1}_{j}} ). On peut aussi calculer.} les variances dans : {ll} ({ ^{l}_{}} ) & = {^2}\\ On cherche une initialisation permettant de limiter les écarts entre les moyennes et les variances des gradients successifs ${ ^{l}_{j}} $ et ${ ^{l+1}_{j}} $ au cours de la rétro-propagation. En gardant à l'esprit que $_s $ est seulement défini par l'intervalle donné dans , le choix de $_s$ et $$ tels que $m _s - _x / 1$ permet de limiter les variations de moyenne dans , en gardant à l'esprit que l'équation limite la borne supérieure de $$ pour que $m(_x^2+_s^2)/^2$, c'est-à-dire $m_x/$, reste d'un ordre non négligeable. Ceci dit, d'autres choix sont possibles. On voit par exemple dans que si on accepte un peu de variations dans l'espérance des gradients, cela évite la dégénérescence de leur variance... C'est le principe que nous utilisons dans la suite.\\ On réunit maintenant les conditions établies de à . D'abord, on introduit un paramètre $'$ (produit de $$ par l'accroissement de moyenne qu'on s'autorise pendant la rétro-propagation) et on choisit pour $_s$ la valeur maximale autorisée par l'intervalle dans . On obtient ainsi un système qui donne deux valeurs possibles pour $_s-_x$~:\\ \{{ll} _s - _x & = {m}\\ \\ _s - _x & = {n}-{2}} . Dans la première équation de , on a $' = C $ où $C$ représente le taux de croissance des moyennes : C = ( { ^{l}_{j}} )}{( { ^{l+1}_{j}} )}. Par exemple si $C=1$, les moyennes ne changent pas. Si $C=2$, à chaque fois qu'on traverse une couche la moyenne est doublée etc. \'Evidemment on cherche une stratégie qui n'augmente pas les moyennes de façon trop significative. D'autant que la croissance des moyennes conduit à une croissance des variances à cause de . Dans la deuxième équation de , on a $'= m {n}-{2}}$. Comme, d'après , il faut $'=C > C $, on a forcément ${n}-{2}} > (C/m){2}^2_x}$. En passant au carré la dernière égalité et la dernière inégalité, on en tire les conditions suivantes sur $$ : && C^2 ^2 = m^2( {n}-{2}), \\ && ^2 > {2} ( {m^2} +1 )_x^2. La condition va se substituer à la condition (on note en particulier que implique ). De la condition on peut tirer une expression de $$ : = {2(m^2-C^2n)} } qui n'a de sens que si la variation $C$ de la moyenne des gradients est contrôlée par C {} . \\ { Récapitulons : } \\ On choisit $C$ vérifiant et d'un ordre de grandeur raisonnable pour que la relation ne conduise pas à l'explosion de la moyenne des gradients. Nous considérons la variable $$ petite pour calculer $C = {}-$. La soustraction de la valeur $$ permet de garantir que l'inégalité est maintenue. \\ On définit ensuite $$ par . \\ On remarque alors que $${^2} = {m \, n\, _x}.$$ Il ne reste plus qu'à contrôler cette dernière quantité, en utilisant si besoin la marge de manoeuvre qu'on a sur $C$, pour que l'équation assure que la variance des gradients n'explose pas et ne dégénère pas non plus.\\ \\ Une fois l'étape précédente terminée, on a une valeur définitivement établie pour $$. Celle ci est fixé pour l'entraînement. \\ On peut alors choisir $_s$ et $_s$ par . Comme mentionné précédemment, nous avons deux expressions pour définir $_s$. Afin de garantir la condition établie par , on propose donc considérer le min entre les deux expressions obtenues pour $_s$. Ainsi on pose~: _s = (_x + {m}; _x + {n}-{2}} ) Dans les paragraphes suivants sont présentées les expérimentations numériques correspondant à l'ensemble des règles d'initialisation que nous venons de développer. On rappelle ici de façon synthétique les règles d'initialisation issues des calculs des paragraphes précédents. Nous présentons ensuite un certain nombre d'expérimentations qui permettent à la fois de confirmer la pertinence de ces règles et d'illustrer les arguments mathématiques précédents. Un test sur un cas plus réaliste est présenté au paragraphe . Dans le tenseur de sortie, la composante $y_{j}= ._i_j}{}$ associée à l'hypersphère $ _j$ correspondant à la sortie d'un neurone s'écrit~: y_{j} = {2} Les conditions , et nous permettent de proposer l'algorithme d'initialisation des paramètres des hypersphère suivant pour une couche hypersphérique~:\\ [ colback=gray!5, colframe=blue!75!black, fonttitle=, title={Initialisation des paramètres des hypersphères} ] [H] _1, , _K\}_k$ tel que $_k ^n$, le nombre d'hypersphères $m$, $={}$. } $, les centres $_j = (s_{j1}, , s_{jn})$ et les rayons $_j$ des hypersphères} \\ $${cc} _x = & {Kn} _{k,i} x_{ki} \\ ^2_x = & {Kn} _{k,i} x_{ki}^2 - _x^2 $$ \\ $$C = {}- := {2(m^2-C^2n)} } $$ \\ $$_s := (_x + {m}; _x + {n}-{2}} )$$ \\ $$ ^2 := 2 _x + n $$ \\ $$ _s^2 := {n}_x^2 }- $$ \\ $} { $} { Initialiser les $s_{ji}$ aléatoirement tels que $s_{ji} (_s, _s^2)$ } Les paramètres de l'hypersphère $$ sont définis par le centre $_j =(s_{j1}, , s_{jn})$ et le rayon $$ (cf équation ). } La méthodologie d’initialisation proposée nécessite une inférence préalable couche par couche du modèle. En effet, pour pouvoir appliquer l’algorithme d’initialisation à une couche hypersphérique donnée, il est indispensable de disposer des tenseurs d’entrée associés à cette couche. Cela implique qu'il faut exécuter une passe avant (forward pass) du réseau couche par couche. Dans les sous-paragraphes suivants, on illustre la pertinence des règles d'initialisation proposées en observant la façon dont un ''signal'' est transformé au fil de ses passages dans des neurones hypersphériques. Pour des raisons de place, nous ne montrons que la distribution de données initiales et la distribution à la sortie des deux premières couches. Il faut savoir que lorsque nous annonçons que les résultats sont mauvais, ils le sont dès la sortie de la quatrième couche. Lorsque nous les annonçons bons, c'est parce que nous les avons testé sur une centaine de couches. La figure donne par exemple les résultats à la sortie de 50 couches pour~: \\ Une distribution normale avec des points de dimension 4\\ Des données extraites du jeu IRIS .\\ Une distribution composée de trois clusters de points, chacun suivant une loi normale multivariée. [htbp] [t]{0.45} [t]{0.45} [htbp] Pour les expériences qui suivent, nous utilisons un réseau de neurones à deux couches cachées hypersphériques, contenant chacune $N$ hypersphères. Lorsque $N > 1$, la sortie du neurone est obtenue en moyennant l'ensemble des produits $._i $ calculés. Il n'y a pas d'activation : nous observons simplement l'enchaînement successif des couches. Le jeu de données utilisé est un ensemble de points synthétiques de dimension 10\,000, distribués selon une loi normale de moyenne $_x$ et de variance $_x^2$. Les valeurs utilisées pour les expérimentations sont indiquées au-dessus des figures correspondantes.\\ Dans cette section, la valeur $$ est définie par $ {2}^2_x} + _{}$ car les conditions données par l'étude du gradient de la fonction de coût ne sont pas prises en compte. [htbp] = 1$: \\ $_{x0} = 0.099984$, $_{x1} = 0.080297$, $_{x2} = 0.036692$, \\ $_{x0} = 1.000006$, $_{x1} = 1.075929$, $_{x2} = 1.177815$, \\ $_{s1} = 0.005229$, $_{s2} = 0.014190$, \\ $_{s1} = 0.000000$, $_{s2} = 0.000000$.} : Les écarts-types \(_{x1}\) et \(_{x2}\) diminuent significativement (0.080297 et 0.036692), ce qui indique une perte de variance. Les moyennes \(_{x1}\) et \(_{x2}\) augmentent légèrement par rapport à \(_{x0}\), ce qui montre une dérive du signal. [H] = 1$: \\ $_{x0} = 0.099984$, $_{x1} = 0.031161$, $_{x2} = 0.019281$, \\ $_{x0} = 1.000006$, $_{x1} = 0.998604$, $_{x2} = 0.997637$, \\ $_{s1} = 0.000011$, $_{s2} = 0.000001$, \\ $_{s1} = 1.382000$, $_{s2} = 2.150632$.} : Les écarts-types \(_{x1}\) et \(_{x2}\) restent proches de \(_{x0}\), ce qui indique une bonne préservation de la variance. Les moyennes \(_{x1}\) et \(_{x2}\) restent proches de \(_{x0}\), ce qui suggère une propagation plus fidèle du signal. : Lorsque \(_s = 0\), les écarts-types et les moyennes se dégradent plus rapidement, que dans le cas où \(_s 0\), les écarts-types diminuent, mais les moyennes restent très proches de la valeur initiale, ce qui montre une meilleure stabilité. Il est plus intéressant de fixer les moyennes $_s$ proche de la valeur de la borne supérieur. (le cas proche de la valeur de la borne inf donne des résultats similaires). = 0$ et $_x = 1$ (non petit), cela ne fonctionne pas} [H] = 1$: \\ $_{x0} = 0.999842$, $_{x1} = 0.593819$, $_{x2} = 0.400592$, \\ $_{x0} = 1.000060$, $_{x1} = 0.985483$, $_{x2} = 0.967814$, \\ $_{s1} = 0.000747$, $_{s2} = 0.000302$, \\ $_{s1} = 1.631132$, $_{s2} = 2.195358$.} : Les écarts-types \(_{x1}\) et \(_{x2}\) diminuent trop fortement (0.593819 et 0.400592), ce qui indique une perte d'information excessive. Malgré une moyenne relativement stable, la propagation devient moins efficace en raison de la réduction de la variance. : Lorsque \(_x\) est grand, les écarts-types diminuent fortement, et le signal tend vers une loi gamma de signe inverse, ce qui montre une instabilité dans la propagation du signal. [H] = 1$: \\ $_{x0} = 0.099984$, $_{x1} = 0.031154$, $_{x2} = 0.019243$, \\ $_{x0} = 1.000006$, $_{x1} = 0.999603$, $_{x2} = 0.999568$, \\ $_{s1} = 0.000011$, $_{s2} = 0.000001$, \\ $_{s1} = 1.382000$, $_{s2} = 2.151655$.} [H] = 1$: \\ $_{x0} = 0.099984$, $_{x1} = 0.031153$, $_{x2} = 0.019238$, \\ $_{x0} = 1.000006$, $_{x1} = 0.999987$, $_{x2} = 0.999986$, \\ $_{s1} = 0.000011$, $_{s2} = 0.000001$, \\ $_{s1} = 1.382000$, $_{s2} = 2.152048$.} : Lorsque \(N\) augmente (\(N=10\) et \(N=1000\)), la variance \(_x\) reste bien préservée. La moyenne \(_x\) est presque inchangée, suggérant une stabilité accrue avec un grand \(N\). Une meilleure propagation du signal est observée avec un grand \(N\), validant son rôle bénéfique. : Lorsque \(N\) est grand, les écarts-types diminuent, mais les moyennes restent extrêmement proches de la valeur initiale, ce qui montre une excellente stabilité. On observe une conséquence du théorème central limite. $ sur $_s$} [H] = 10$: \\ $_{x0} = 0.099984$, $_{x1} = 0.038070$, $_{x2} = 0.036201$, \\ $_{x0} = 1.000006$, $_{x1} = 0.998295$, $_{x2} = 0.997497$, \\ $_{s1} = 0.000011$, $_{s2} = 0.000000$, \\ $_{s1} = 1.504375$, $_{s2} = 11.328631$.} [H] = 1000$: \\ $_{x0} = 0.099984$, $_{x1} = 0.094492$, $_{x2} = 0.094444$, \\ $_{x0} = 1.000006$, $_{x1} = 0.998323$, $_{x2} = 0.998099$, \\ $_{s1} = 0.000002$, $_{s2} = 0.000000$, \\ $_{s1} = 11.694160$, $_{s2} = 1001.364937$.} [H] = 10$: \\ $_{x0} = 0.999842$, $_{x1} = 0.613114$, $_{x2} = 0.585020$, \\ $_{x0} = 1.000060$, $_{x1} = 0.983484$, $_{x2} = 0.970680$, \\ $_{s1} = 0.000755$, $_{s2} = 0.000068$, \\ $_{s1} = 1.754479$, $_{s2} = 11.404689$.} [H] = 1000$: \\ $_{x0} = 0.999842$, $_{x1} = 0.946954$, $_{x2} = 0.946493$, \\ $_{x0} = 1.000060$, $_{x1} = 0.983402$, $_{x2} = 0.981176$, \\ $_{s1} = 0.000171$, $_{s2} = 0.000002$, \\ $_{s1} = 12.026035$, $_{s2} = 1001.573476$.} [H] = 10000$: \\ $_{x0} = 0.999842$, $_{x1} = 0.987210$, $_{x2} = 0.987161$, \\ $_{x0} = 1.000060$, $_{x1} = 0.994143$, $_{x2} = 0.993295$, \\ $_{s1} = 0.000020$, $_{s2} = 0.000000$, \\ $_{s1} = 102.073974$, $_{s2} = 10001.604402$.} [H] = 100000$: \\ $_{x0} = 0.999842$, $_{x1} = 0.991597$, $_{x2} = 0.000009$, \\ $_{x0} = 1.000060$, $_{x1} = 0.999441$, $_{x2} = 0.999449$, \\ $_{s1} = 0.000002$, $_{s2} = 1.000000$, \\ $_{s1} = 1002.079502$, $_{s2} = 0.000000$.} : Lorsque \(\) est très grand (\(_ = 10000\) ou \(100000\)), les écarts-types sont bien préservés, mais les moyennes dérivent légèrement. Cependant, dans le cas de \(_ = 100000\), on observe un effondrement de l'écart-type dans la deuxième couche, ce qui indique une perte totale de variance. Cela montre que il y a une valeur limite pour \(\) très grand, afin de garantir la propagation du signal. Pour une propagation efficace du signal dans un réseau de neurones, il est essentiel de : Initialiser \(_s\) à une valeur non nulle pour éviter la dégradation de la variance et de la moyenne. Maintenir \(_x\) petit permet d'assurer une propagation stable du signal. Augmenter \(N\) pour améliorer la robustesse et la stabilité du réseau. Contrôler \(\) pour éviter l'instabilité, surtout lorsque \(_x\) est grand (on revient sur ce point au paragraphe suivant qui concerne le paramètre $q_i$). Lorsque \(\) est grand, cela améliore la propagation du signal jusqu'à une valeur limite qui peut entraîner une instabilité dans le réseau, surtout lorsque \(_x\) est petit. Il est donc crucial de trouver un équilibre entre \(\) et \(_x\) pour maintenir une propagation stable du signal. Dans chaque légende de figure (en haut) apparaît un paramètre $q_i$, l'indice $i$ désignant le numéro de la couche. Celui-ci est calculé en fonction des paramètres d'entrée de la couche $i$ et des paramètres de poids calculés pour cette couche : $$ q_i = {_x} + {_s} .$$ Le paramètre $q_i$ permet de vérifier si le critère est satisfait. Les figures ci-dessus, en particulier la dernière image de la figure permettent de vérifier que l'utilisation du critère peut être un garde-fou très efficace pour éviter un effondrement de la variance du signal de sortie de la couche.\\ Afin de mieux maîtriser la valeur de $$ et de tenir compte du nombre d'hypersphères utilisées (paramètre $m$ de l'algorithme proposé), il est insuffisant de se limiter aux conditions données par et . C’est pourquoi nous avons également étudié l’impact de la variance de la fonction de coût, en suivant un raisonnement analogue à celui présenté dans . Cette démarche a permis de redéfinir le paramètre $$ (cf. équation ) et d’assurer la propagation du signal à travers les couches, quel que soit le nombre de neurones choisis dans la couche hypersphérique. Le but de cette expérience est de vérifier la propagation du signal dans un réseau, en testant un jeu de données réel, afin d’évaluer si l’algorithme proposé pour l’initialisation fonctionne correctement (voir ). Pour cela, nous avons construit un réseau de neurones comportant 10 couches cachées hypersphériques.\\ Trois configurations ont été testées, avec respectivement 8, 64 et 4096 hypersphères par couches cachées (la valeur est indiquée dans la légende). Les modèles, partagent la même architecture générale, sans fonction d’activation ni normalisation par lot (BatchNorm). \\ Nous avons pris $ = {}$ pour calculer $C = {}-$.\\ Les figures , et présentent la distribution des sorties de chaque couche cachées à l’initialisation, c’est-à-dire avant l’entraînement. Les figures , et montrent quant à elles les distributions des sorties après entraînement.\\ [H] {avant} l'entraînement (cas de 8 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {après} l'entraînement (cas de 8 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {avant} l'entraînement (cas de 64 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {après} l'entraînement (cas de 64 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {avant} l'entraînement (cas de 4096 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {après} l'entraînement (cas de 4096 hypersphères par couche cachée, pour un réseau à 10 couches cachées} On peut observer que à l’initialisation, les moyennes des distributions de sorties restent stables au fur et à mesure que l’on empile les couches, et que la variance n’explose pas. Cela permet de préserver le signal à la sortie du réseau.\\ Pour les trois configurations qui ont été testées (avec 8, 64 ou 4096 neurones par couche cachée), les taux d’accuracy obtenus sont respectivement de 98.3\ Il faut considérer un nombre suffisamment grand d'hypersphères (4096) pour observer que la méthode d'initialisation proposée contraint la distribution des sorties à tendre vers une loi normale. Le modèle de neurone hypersphérique de Banarer a été étendu pour définir un filtre de convolution hypersphérique adapté aux tenseurs d'ordre $l$ à plusieurs canaux. Cette extension consiste à transformer les coefficients du filtre ainsi que le biais en coordonnées sur une hypersphère, permettant ainsi l'optimisation de ces paramètres. Les expérimentations réalisées sur des cas simples visaient à valider et comparer ces modèles sur des architectures simples, telles que les couches denses ou Conv2d. Les résultats montrent que les taux de prédiction restent globalement similaires entre les réseaux classiques et les réseaux à couches hypersphériques pour une architecture de type Perceptron Multicouches. Il a été observé que les fonctions de perte convergent plus rapidement et atteignent des valeurs plus faibles avec les couches hypersphériques. De plus, en adaptant des réseaux convolutifs avec des filtres hypersphériques, les résultats de prédiction demeurent comparables à ceux obtenus avec des filtres classiques. Cependant, lors de la transition d'un réseau dense hypersphérique vers un réseau convolutif avec filtres hypersphériques, l'augmentation du taux de bonne prédiction est similaire à celle observée avec les réseaux classiques. La question de l'initialisation des poids dans les couches hypersphériques a été explorée afin de garantir la possibilité d'enchaîner plusieurs couches successives. La sortie d'une couche hypersphérique dépend de plusieurs paramètres, dont le facteur d'échelle $$ que nous avons introduit. Ce dernier joue un rôle essentiel dans la distribution des sorties, et une fois fixé, il permet de déterminer à la fois les valeurs des paramètres hypersphériques et les conditions d'initialisation : en particulier, l'intervalle auquel doit appartenir la moyenne des centres, ainsi que la variance de ces derniers lorsqu'ils sont initialisés selon une loi normale.\\ La méthode proposée permet non seulement de propager efficacement le signal à travers des couches hypersphériques successives, mais aussi d’augmenter le nombre de neurones par couche. Cela ouvre des perspectives intéressantes pour l'intégration des couches hypersphériques dans des architectures plus complexes. Néanmoins, l'ajustement des paramètres d'initialisation reste à affiner en fonction des différentes fonctions d'activation utilisées.

