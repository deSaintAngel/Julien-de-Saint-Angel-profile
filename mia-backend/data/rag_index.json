{
  "chunks": [
    {
      "id": "annexea.tex.txt_chunk_0",
      "text": "Cette annexe détaille un certain nombre de calculs (notamment de covariance) qui nous permis de justifier notre proposition d'initialisation pour les couches hypersphériques. (2 x_i s_i - x_i^2 - s_{ji}^2) = (2 x_i s_{ji} - x_i^2) + (s_{ji}^2) + 2 (2 x_i s_{ji} - x_i^2, s_{ji}^2) Or, (2x_i s_{ij} - x_i^2, -s_{ij}^2) &= [(2x_i s_{ij} - x_i^2)(-s_{ij}^2)] - [2x_i s_{ij} - x_i^2] [-s_{ij}^2] \\\\ \\\\ &= -[2x_i s_{ij}^3] + [x_i^2 s_{ij}^2] + [2x_i s_{ij}] [s_{ij}^2] - [x_i^2] [s_{ij}^2] \\\\ \\\\ &= -2[x_i] [s_{ij}^3] + [x_i^2] [s_{ij}^2] + 2[s_{ij}] [x_i] [s_{ij}^2] - [x_i^2] [s_{ij}^2] \\\\ \\\\ &= -2[x_i] [s_{ij}^3] + 2[s_{ij}] [x_i] [s_{ij}^2] \\\\ \\\\ &= 2_s _x (_s^2 + _s^2) - 2 _x (_s^3 + 3 _s _s^2) \\\\ \\\\ &= -4 _s _x _s^2 (2 x_i s_{ji} - x_i^2) = (2 x_i s_{ji}) + (-x_i^2) + 2 (2 x_i s_{ji}, x_i^2) Or, (2 x_i s_{ji}, -x_i^2) &= [(2 x_i s_{ji})(-x_i^2)] - [2 x_i s_{ji}] [-x_i^2] \\\\ \\\\ &= -2[s_{ji}] [x_i^3] + 2 [x_i][s_{ji}] [x_i^2] \\\\ \\\\ &= 2_s _x (_x^2 + _x^2) - 2 _s (_x^3 + 3 _s _x^2) \\\\ \\\\ &= -4 _s _x _x^2 Donc, (2 x_i s_{ji} - x_i^2 - s_{ji}^2) = (2 x_i s_{ji}) + (-x_i^2) + (s_{ji}^2) - 8 _s _x (_x^2 + _s^2 ) Les calculs précédents ont pris en compte uniquement des couches hypersphériques de",
      "source_file": "annexea.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_1",
      "text": "type dense. Cependant, la plupart des réseaux sont constitués de couches convolutionnelles. Chaque élément du tenseur de sortie d'un neurone correspond au produit scalaire entre la version vectorisée de la sphère et une partie du tenseur d'entrée de taille identique au noyau de convolution. Il est donc possible de considérer des vecteurs constitués des parties vectorisées par blocs de l'entrée. En conséquence, pour une sphère donnée, contrairement au cas des couches de type dense où la sortie est un scalaire, on obtient un vecteur dont chaque élément provient du produit scalaire entre la sphère et les sous-vecteurs correspondants. \\\\ Pour les calculs suivants, on considère une couche convolutive où le tenseur d'entrée est de taille $(c,n)$ (un vecteur à $c$ canaux), la sortie est un tenseur de taille $(m,d')$ où $m$ représente le nombre de sphères, et $d'$ le nombre de composantes issues du produit de convolution (on a $d'= n-d-1$, dans le cas d'une convolution sans padding, et $d'= n$ avec padding). Les éléments de la sortie sont donc les $y_{i'j}$ définis par $y_{i'j} = } _i }{}$.\\\\ Pour le produit entre un vecteur de composantes $}^{}$ et une sphère $}$ qui donne l'élément $y_{",
      "source_file": "annexea.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_2",
      "text": "i'j}$ de la convolution, avec $d$ la taille du filtre tel que $i\\{1,,d\\}$ et $d'$ le nombre de composantes tel que $i'\\{1,,d'\\}, $ on obtient pour une sphère $j$ donnée l'écriture de la $i'$-ième composante ~:\\\\ $${ y_{i'j} = {2}}$$ L'élément $x_{i'ci}$ représente le $i$-ème élément pour le canal $c$ de la composante $}^{}$. On considère pour l'écriture suivante les composantes $p$ et $q$ pour la sphère $}$. Pour alléger la notation l'indice $j$ n'est pas mentionné. Le terme en $^2$ est intégré dans la double somme.\\\\ $$ y_p = _i = _{c=1}^{C}_{i=1}^{d} , p \\{1,,d'\\}$$\\\\ $$ y_q = _i = _{c=1}^{C}_{i=1}^{d} , q \\{1,,d'\\}$$ On calcule alors : \\\\ { ${ll} & (y_p,y_q) = cov( , ) p<q = q-p ] \\\\ \\\\ & =( _{c=1}^{C} _{i=1}^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd}, _{c'=1}^{C} _{i\"=1}^{d}{{x_{qi\"c'}}} {{s_{i\"}}}-{2}{{x^{2}_{qi\"c'}}}-{2}{{s^{2}_{i\"c'}}}+}{2Cd})\\\\ \\\\ & =_{c=1}^{C}(_{i=1}^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{c'=1}^{C}_{i\"=1}^{d}{{x_{qi\"c'}}} {{s_{i\"c'}}}-{2}{{x^{2}_{qi\"c'}}}-{2}{{s^{2}_{i\"c'}}}+}{2Cd})\\\\ \\\\ \\\\ & =_{c=1}^{C} ^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{i\"=1}^{d}{{x_{qi\"c}}}",
      "source_file": "annexea.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_3",
      "text": "{{s_{i\"c}}}-{2}{{x^{2}_{qi\"c}}}-{2}{{s^{2}_{i\"c}}}+}{2Cd})}_{ c=c'}\\\\ \\\\ & ++_{c=1}^{C} ^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{i\"=1}^{d}{{x_{qi\"c'}}} {{s_{i\"c'}}}-{2}{{x^{2}_{qi\"c'}}}-{2}{{s^{2}_{i\"c'}}}+}{2Cd})}_{ c c'} $}\\\\ On peut restreindre le calcul à la condition $c=c'$. En effet, on cherche à calculer la somme des covariances de termes independants deux à deux. Cela implique que si on développe les termes de la somme pour $c c\"$, alors ils sont tous nuls. Donc pour $c=c'$ fixé on a~: \\\\ $$ (y_p,y_q) = _{c=1}^{C} (_{i=1}^{d}{{x_{pi}}} {{s_{i}}}-{2}{{x^{2}_{pi}}}-{2}{{s^{2}_{i}}}+}{2Cd},_{i=1}^{d}{{x_{qi}}} {{s_{i}}}-{2}{{x^{2}_{qi}}}-{2}{{s^{2}_{i}}}+}{2Cd})}_{cov(_i ,_i )_c}$$\\\\ Ainsi le calcul du terme cov$(y_p,y_q)_c$ donne~:\\\\ ${ll} (y_p,y_q)_c & = (_i ,_i )_c\\\\ \\\\ & = \\\\ \\\\ \\\\ & = \\\\ $\\\\ ${ll} (y_p,y_q)_c & = ^{ d}(x_{pi} s_i-{2}x^{2}_{pi}) ,_{i=1}^{d}(x_{qi} s_i-{2}x^{2}_{ql})]}_{(1)}\\\\ \\\\ & + }_{(2)}\\\\ \\\\ \\\\ & }_{(3)}+ }_{(4)} $\\\\ ${rl} (4) : & s_i ^2 \\\\ & _s, s_{i} (_s,^{2}_{s}), ~:\\\\ \\\\ &{ccl} var(s^{2}_{i}) = -^{2} = 2 _s^2 (_s^2 + 2 _s^2 ) $\\\\ ${rl} & \\\\ \\\\ & {lll} & = & \\\\ \\\\ & = & (_{i=1}^{ d}-{2}s^{2}_{i})+ ({2cd})}_{0}+(_{i=1",
      "source_file": "annexea.tex.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_4",
      "text": "}^{ d}-{2}s^{2}_{i},{2cd})}_{0}\\\\ & = & {4}(s^{2}_{i}) s_i \\\\ \\\\ & = & {2} _s^2 (_s^2 + 2 _s^2 ) $\\\\ Les termes (2) et (3) étant symétriques, leur calcul est identique.\\\\ ${lll} & = & _{i=1}^{d}_{i=1}^{d} \\\\ $ ${ll} = & _{i=1}^{d}_{i=1}^{d} $\\\\ {$ et les $s_i$, pour tout $i \\{1, , d\\}$, sont indépendants alors on a l'égalité [x^{2}_{pi}s^{2}_{i}]=[x^{2}_{pi}][s^{2}_{i}]$} qui permet d’établir que la covariance du terme donné est nulle.\\\\ }} ${ll} (*) & _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,-{2}s^{2}_{i})=_{i=1}^{d} $\\\\ $$ - = - $$\\\\ Ainsi, $$ =-d_s _x _x^2$$\\\\ En reprenant le terme (1), on a~:\\\\ $${ll} & \\\\ \\\\ = & _{i=1}^{d}_{i=1}^{d} \\\\ $$\\\\ {ll} = & _{i=1}^{d}_{i=1}^{d}\\\\ \\\\ Le terme est donc décomposé en 4 termes que l'on calcule~: \\\\ $${ll} (B),(C) & _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,-{2}x^{2}_{qi})= _{i=1}^{ d}_{i=1}^{ d}(-{2}x^{2}_{pi},x_{qi} s_i)= 0.\\\\ \\\\ \\\\ & i \\{1,, d\\} \\ :\\\\ & (-{2} x_{pi} s_{i} x^{2}_{qi})-(x_{pi} s_{i})(-{2}x^{2}_{qi})\\\\ \\\\ &= (s_{i})(-{2}x_{pi}x^{2}_{qi})-(x_{pi})(s_{i}) (-{2}x^{2}_{qi})\\\\ \\\\ &= -{2}(s_{i})(x_{pi} ) (x^{2}_{qi})+{2}(x_{pi})(s_{i}) (x^{2}_{qi})\\\\ \\\\ & = 0\\\\ $$\\\\ Pour tout $i \\{1, , d\\}$, les $s_{i}$ sont indépendants des $x_{p_i}$ et des $x_{q_",
      "source_file": "annexea.tex.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_5",
      "text": "i}$, ce qui implique que l'espérance des produits entre ces termes est égale au produit des espérances. Cela justifie que les termes $(B)$ et $(C)$ sont nuls.\\\\ Donc $$ (x_{pi} s_i,-{2}x^{2}_{qi}) =0 (x_{qi} s_i,-{2}x^{2}_{pi}) =0 $$\\\\ $(A)$ $$ les vecteurs $X_{p}$ et $X_{q}$, pour $p q$, correspondent à une partie de la forme vectorisée du vecteur d'entrée $X$, dont la taille est identique à la forme vectorisée du noyau de convolution. Ainsi les éléments des deux vecteurs $X_p$ et $X_q$ ont respectivement pour indices $p+k$ $q+k$ avec $k\\{0,,d-1\\}$. En posant $k' = k + 1$, il en résulte que~: $$X_p=\\{x_{pi}\\}_{i\\{1,,d\\}}=\\{x_{p+k'-1}\\}_{k'\\{1,,d\\}}$$ et $$X_q=\\{x_{qi}\\}_{i\\{1,,d\\}}=\\{x_{q+k'-1}\\}_{k'\\{1,,d\\}}$$\\\\ La double somme du terme A peut s'écrire de la façon suivante~:\\\\ $$_{k'=1}^{d}_{k'=1}^{d}(x_{p+k'-1} s_{k'},x_{{q}+k'-1} s_{k'})=_{k'=1}^{d}_{k'=1}^{d}(x_{p+k'-1} s_{k'},x_{{p+} +k'-1} s_{k'}) $$\\\\ ou bien encore~:\\\\ $$_{k'=1}^{d}_{k''=1}^{d}(x_{{p}+k'-1} s_{k'},x_{{p+} +k''-1} s_{k''}) $$\\\\ En réarangeant les indices en fonction de $k'$ et $k''$, on obtient~:\\\\ $$_{k'={p}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1}) $$. \\\\ Pour continuer le c",
      "source_file": "annexea.tex.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_6",
      "text": "alcul, il faut émetre l'hypothèse que $ d$ et est non nul. En effet dans le cas contraire, pour tout $k'\\{p,,p+d-1\\}$ et tout $k''\\{p+,,p++d-1\\}$, les $x_{k'}$, $x_{k''}$, $s_{k'-p+1}$ et $s_{k''-p-+1}$ sont tous distincts, et on peut donc justifier le passage des espérances des produits au produit des espérances de chaque terme et observer que la covariance des termes calculés est nulle. En effet, les $x_{k'}$ et $x_{k''}$ sont indépendants entre eux, et indépendants des $s_{k'-p+1}$ et $s_{k''-p+1}$. En développant les termes de la covariance, il apparaît que l'expression dépend uniquement des espérances des produits de ces termes.\\\\ $${lll} (x_{k'} s_{k'-p+1}, x_{k''} s_{k''-p-+1}) & = & \\\\ \\\\ & & - \\\\ \\\\ \\\\ & = & [x_{k'}] [x_{k''}] [s_{k'-p+1} s_{k''-p-+1}]\\\\ \\\\ & & - [x_{k'}] [s_{k'-p+1}] [x_{k''}] [s_{k''-p-+1}]\\\\ \\\\ \\\\ & = & [x_{k'}] [x_{k''}] [s_{k'-p+1}] [s_{k''-p-+1}]\\\\ \\\\ & & - [x_{k'}] [s_{k'-p+1}] [x_{k''}] [s_{k''-p-+1}]\\\\ $$\\\\ Tous les produits d'espérance de ces termes se simplifient et donc sont nuls. Ainsi, chaque terme de la somme est nul, ce qui implique que la somme est aussi nulle.\\\\ Par linéarité de la covariance, on peut séparer la double somme selon les ter",
      "source_file": "annexea.tex.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_7",
      "text": "mes suivants~: $$ {p}}^{{p+-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})}_{=0}+ _{k'={p+}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1}) $$ $ et tout $k''\\{p+,,p+d-1\\}$, les $x_{k'}$ et $x_{k''}$ sont tous distincts. Un raisonnement analogue à l'étape précedente, permet de conclure que chaque terme de la somme est nul, donc la somme est nulle.} $$=_{k'=p+}^{p+d-1}_{k''={p+}}^{{p+d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})+ ^{p+d-1}_{k''={p+d}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})}_{=0} $$ {$ et tout $k''\\{p+,,p++d-1\\}$, les $x_{k'}$ et $x_{k''}$ sont tous distincts. Ainsi chaque terme de la somme est nul, donc la somme totale est nulle. }} Dans la double somme, les termes où $k' k''$ vont donner des $x_{k'}$ et $x_{k''}$ distincts, ce qui permet également de conclure que les termes de covariance calculés sont nuls. Il ne reste donc que les termes où $k'=k''$. Ce qui permet de simplifier la double somme sou la forme suivante~:\\\\ $$_{k'=p+}^{p+d-1}(x_{k'} s_{k'-p+1},x_{k'} s_{k'-p-+1})$$\\\\ Pour chaque terme de cette dernière somme le calcul de la covariance donne~:\\\\ $${lll} (x_{k'} s_{k'-p+1},x_{k'} s_{k'-p-+1})& = & [x_",
      "source_file": "annexea.tex.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_8",
      "text": "{k'}^2] [s_{k'-p+1}] [s_{k'-p-+1}]\\\\ \\\\ & & - [s_{k'-p+1}] \\\\ \\\\ & = & ({_x^2 + _x^2}) _s^2 - _x^2 _s^2 $$ $$ _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,x_{qi} s_i)=(d-) _s^2 _x^2$$\\\\ Pour le calcul du terme $D$, le raisonnement sur les indices de sommation est similaire à ce qui a été effectué pour le calcul du terme $A$. \\\\ Comme énoncé précédemment, les éléments des deux vecteurs $X_p$ et $X_q$ ont respectivement pour indices $p+k$ et $q+k$ avec $k\\{0,,d-1\\}$. On fait l'hypthèse que $ d$, et on a $=q-p$. On pose $k' = k + 1$, il en résulte que~: $$X_p=\\{x_{pi}\\}_{i\\{1,,d\\}}=\\{x_{p+k'-1}\\}_{k'\\{1,,d\\}}$$ et $$X_q=\\{x_{qi}\\}_{i\\{1,,d\\}}=\\{x_{q+k'-1}\\}_{k'\\{1,,d\\}}$$. ${ll} (D) & = _{i=1}^{d}_{i=1}^{d}(-{2}x^{2}_{pi},-{2}x^{2}_{qi})\\\\ \\\\ & = _{k'=1}^{d}_{k''=1}^{d}(-{2}x^{2}_{p+k'-1},-{2}x^{2}_{q+k''-1}) \\\\ \\\\ & = _{k'=1}^{d}_{k''=1}^{d}(-{2}x^{2}_{p+k'-1},-{2}x^{2}_{p++k''-1}) \\\\ \\\\ & = _{k'={p}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}} (-{2}x^{2}_{k'},-{2}x^{2}_{k''})\\\\ \\\\ & = {p}}^{{p+-1}}_{k''={p+}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})}_{=0}+ _{k'={p+}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})\\\\ \\\\ & = _{k'=p+}^{p+d-1}_{k''={p+}}^{{p+d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''",
      "source_file": "annexea.tex.txt",
      "chunk_index": 8,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_9",
      "text": "})+ ^{p+d-1}_{k''={p+d}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})}_{=0}\\\\ $ $$ {ll} (D) & = _{k'=p+}^{p+d-1} (-{2}x^{2}_{k'}, -{2}x^{2}_{k'}) x_{pi} (0,^{2}_{x})\\\\ \\\\ & = _{i=1+}^{d} (x_{k'}^2)}{4} = \\{ {l} {2} ^{2}_{x} (^{2}_{x} + 2 _x^2 ) (=q-p) \\{1, , d\\}, \\\\ 0 . $$\\\\ {_{k'}$ et $x^{2}_{k''}$ distincts. Grâce à l'hypothèse d'indépendance on a [x^{2}_{k'}x^{2}_{k''}]=[x^{2}_{k'}][x^{2}_{k''}]$}. Cela permet de conclure que les termes de covariance calculés sont nuls. Il ne reste donc que les termes où $k'=k''$.}} Reprenons , enfin, le calcul . Cette covariance correspond à la somme des termes suivants~: $$\\{ {cl} (1) : & (A = {(d-) _s^2 _x^2 }) + (B = {0} ) + (C = {0}) \\\\ \\\\ & +(D = {{2} ^{2}_{x} (^{2}_{x} + 2 _x^2 )} \\{1, , d\\} \\\\ \\\\ (2) : & {-d _x _s _x^2} \\\\ \\\\ (3) : & {-d _x _s _x^2}\\\\ \\\\ (4) : & {{2} _s^2 (^{2}_{s} + 2_s^2)} . $$\\\\ soit donc~: $${ll} (y_p,y_q) &= \\{{l} _{c=1}^{C}{2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 ) \\{ 1,,d\\}. \\\\ \\\\ _{c=1}^{C}{2}_s^2 (_s^2 + 2 _s^2 -2 _x _s ) .\\\\ \\\\ &= \\{{l} C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 ) ) \\{1,,d\\}. \\\\ \\\\ C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )) .\\\\ $$\\\\ Pour le cas où",
      "source_file": "annexea.tex.txt",
      "chunk_index": 9,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_10",
      "text": "$^t$ est un tenseur de dimension $(m,n,c)$, c'est-à-dire une image à $c$ canaux, nommons $$ et $$ les indices de colonnes des éléments du tenseur $Y$ de sortie. On reprend $=q-p$ et on pose $ = -$. On peut rappeler que $d'$ représente le nombre de composantes dans la direction $i'$ telle que $i'\\{1,,d'\\}$. Si l'on considère un filtre de convolution de taille $d d$ alors, $d''$ représente le nombre de composantes dans la direction $i''$ telle que $i''\\{1,,d''\\}$. Le tenseur $Y$ de sortie est donc constitué des éléments $\\{y_{i'i''j}\\}$. On suppose $ $. On considère donc pour une sphère $j$ fixée les composantes $y_{p}$ et $y_{q}$ telles que $ \\{1,,d''\\}$ et $ \\{1,,d''\\}$. Le nombre total de d'éléments pour la partie vectorisée d'une composante donnée sera alors $d'd''$. On a donc~:\\\\ $${ll} (y_{p},y_{q}) & {l} = \\{{l} C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 )) \\\\ , \\{ 0,,d\\}. \\\\ \\\\ C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )) . \\\\ $$\\\\ On a finalement obtenu la matrice de covariance, dont la largeur de la bande dépendant du facteur $({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s ))$ correspond à la dimension du noyau de convolution~: { $ (y_j) & _1+ _2 & _1 & & _1 \\\\ \\\\ _1",
      "source_file": "annexea.tex.txt",
      "chunk_index": 10,
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_11",
      "text": "+ _2 & (y_j) & _1 + _2 & & \\\\ \\\\ _1 & _1 + _2 & (y_j) & & _1 \\\\ \\\\ & & & & _1 + _2 \\\\ \\\\ _1 & & _1 & _1 + _2 & (y_j) $ } avec,\\\\ { \\{ {l} (y_j) = {2 ^2} (_s^2 + _x^2) ( 2(_x^2 - _s^2)^2 + 2_s_x + _s^2 + _x^2 ) \\\\ \\\\ _1 = C({2}_s^2 (_s^2 + 2 _s^2 - 2 _x _s ) ) \\\\ \\\\ _2 = C({2} _x^2 (_x^2 + 2 _x^2 + 2_s^2 ) ) . } ${ll} \\\\ ({ ^{l}_{}} ) & = ( {}(S^{l+1}_{}- ^{l+1}_{}){ ^{l+1}_{}} ) \\\\ \\\\ \\\\ & = ( _{j'=1}^{m'}{}(s^{l+1}_{jj'}- x^{l+1}_{j}){ z^{l+1}_{j'}} )\\\\ \\\\ \\\\ & = m'( {}(s^{l+1}_{jj'}- x^{l+1}_{j}){ z^{l+1}_{j'}} )\\\\ \\\\ \\\\ & = {^2}\\\\ \\\\ \\\\ & = {^2}\\\\ $",
      "source_file": "annexea.tex.txt",
      "chunk_index": 11,
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_0",
      "text": "Cette annexe reprend les différents éléments de la démonstration du théorème d'approximation classique pour une couche dense. Cette première étape correspond à la Proposition 3.3 de l'article de Pinkus . On suppose que si $f^0(^n)$ alors $f (,,)}$ pour des sous-ensembles $$ et $$ de $$ bien choisis, o\\`u $$ (,,) = \\{( x - )\\, :\\, ,\\, \\}.$$ \\\\ On choisit \\( A S^{n-1} \\), où \\( S^{n-1} = \\{ ^n : \\|\\|_2 = 1\\} \\), tel qu'il n'existe pas de polyn\\^ome homog\\`ene (non trivial) qui s'annule sur $A$. Alors, d'apr\\`es un r\\'esultat de Vostrecov et Kreine (voir Th. 3.2 dans ), l'espace $(A)$ engendré par les fonctions ``ridge\" continues $$ (A) = g() : A^n, g ^0(): $$ est dense dans $^0(^n)$ pour la norme uniforme sur les compacts. On a le résultat suivant: Soit~: $$(, A,) = \\{( -)~:w A, )\\}.$$ Sous les hypothèses précédentes, $(, A,)$ est dense dans $^{0}(^n)$ au sens de la convergence uniforme sur tout compact. Soit \\( f \\) une fonction à plusieurs variables continue sur un compact \\( ^n \\)~: $${cccl} f~: & ^n & & \\\\ &(x_1,,x_n) & & f(x_1,,x_n)\\\\ $$ Cette fonction $f$ est la fonction à approcher. Soit $ >0$ donné quelconque. Comme $(A)$ est dense dans $^0()$ alors on a~:\\\\ $$ r , g_q^0(), ^",
      "source_file": "annexeb.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_1",
      "text": "q A~, \\, 1 q r \\,$$ f()-_{q=1}^{r} g_q(^q) {2} pour tout $x $. Étant donné que \\( \\) et que \\( \\) est un compact de \\( ^n \\), il en découle que \\( \\) est bornée. Comme $A S^{n-1}$, pour tout $q\\{1,,r\\}$, les $^q$ sont également bornées ($S^{n-1}$ étant la sphère unité, les $^q$ sont des vecteurs unitaires). Ainsi $\\{^q ~: \\}$ est un ensemble borné. Donc pour tout $}, il existe des réels \\( _q \\) et \\( _q \\) tels que~: $$ \\{^q : \\} . $$ Comme $(,,)$ est dense dans $^0()$ pour $q\\{1,,r\\}$ alors~: $$ C_{qj},_{qj},_{qj}, j\\{1,,m\\} $$ g_q(t)-_{j=1}^{m}C_{qj}(_{qj}t-_{qj}) {2r} t [_q,_q]. \\\\ En injectant la formule dans , on obtient~: && f() - _{q=1}^{r}(_{j=1}^{m}C_{qj}(_{qj}-_{qj}) ) \\\\ && = f() -_{q=1}^{r} g_q(^q) + _{q=1}^{r} (g_q(^q) - _{j=1}^{m}C_{qj}(_{qj}-_j) ) \\\\ && f() -_{q=1}^{r} g_q(^q) + _{q=1}^{r} g_q(^q) - _{j=1}^{m}C_{qj}(_{qj}-_j) \\\\ && {2}+r{r}=. On a approché $f$ dans un espace de type $(, A,)$ à une précision $$ donnée.\\\\ Ainsi, si $(,,)$ est dense dans ^0()$} au sens de la convergence uniforme sur les compacts, toute fonction continue sur un compact $$ de $^n$ peut \\^etre approchée par la sortie d'un réseau de neurones telle que définie en (2.1). Il reste certes à mo",
      "source_file": "annexeb.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_2",
      "text": "ntrer que $(,,)}=^0()$, mais cette première étape nous a permis de nous ramener au cadre unidimensionnel. (,,)$ est dense dans $^0()$} La deuxième étape correspond à la Proposition 3.4 et au Corollaire 3.5 de l'article de Pinkus . Elle consiste à montrer que tout élément $f$ appartenant à l'ensemble des fonctions continue à support compact peut être approché par une combinaison linéaire d'éléments, de $(, ,)$ où $$ et $$ sont des sous-ensembles appropriés de $$. Soit $ ^{}()$, $$ non polynomiale. Soit $$ un sous-ensemble de $$ dont on peut extraire une suite tendant vers zéro. Soit $$ un intervalle ouvert de $$. Alors, $(,,)$ est dense dans $^0()$. \\\\ On commence par démontrer que $$(,,)} = ^0().$$ Soit $f$ une fonction continue. On veut donc montrer que $f$ est approchable par une combinaison linéaire d'éléments de $(,,)$ : $$>0, C_j, _j, _j, $$ $$ f(x)-_{j=1}^{m_j} C_j (_j x -_j) $$ pour tout $x $ o\\`u $$ est un compact de $$. Par définition de l'adhérence, la limite du taux d'accroissement de la fonction $ ( x-)$ est dans l'adhérence de $(,,)$. En effet $${h} (,,)$$ et donc $$ D_{=0} = _{{h 0, =0}}{h} (,,)}.$$ La quantité $D_{=0}$ correspond en fait à la dérivée première de $ (",
      "source_file": "annexeb.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_3",
      "text": "x-)$. De même on peut vérifier que les dérivées successives d'ordre supérieur de cette fonction appartiennent à $(,,)}$. Comme $$ n'est pas un polynôme, aucune de ses dérivée n'est identiquement nulle. Donc, pour tout $k$, il existe un $_k$ tel que $^{(k)}(-_k) 0$ où $^{(k)}$ est la dérivée $k$-ième de $$. Et d'après ce qui précède, $$ {d^k} ( ( x-)) (0) = x^k ^{(k)}(-_k) (,,)}.$$ Ainsi , x^{k} (,,)}$}. Comme n'importe quel polynôme s'écrit comme combinaison linéaire de monômes, et que tout les monômes sont dans $(,,)}$ alors, tout les polynômes sont dans $(,,)}$.\\\\ D'après le théorème de Weierstrass, toute fonction \\( f ^0(K) \\) peut être approchée par une fonction polynomiale \\( p \\) à coefficients réels en norme uniforme. On vient de voir que tout polynôme est dans $(,,)}$. On en conclut que $f (,,)}$. On vient de montrer que $$(,,)} =^0()$$ au sens de la convergence uniforme. Le corollaire 3.5 dans Pinkus montre qu'on peut en fait remplacer $(,,)$ par $(,,)$ où $$ et $$ vérifient les conditions données au chapitre 2 dans le théorème auquel cette annexe est consacrée.\\\\ On veut maintenant démontrer que si $ ^0()$ (et non plus $ ^()$ comme à l'étape précédente), on a toujours $$(",
      "source_file": "annexeb.tex.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_4",
      "text": ",,)} =^0().$$ Soit donc $$ une fonction continue. On sait qu'elle peut être régularisée par convolution (voir par exemple la démonstration de la proposition IV.21 dans ). Précisément, pour toute fonction $ ^()$ à support compact, on peut construire la fonction $_ ^()$ telle que $$ _( x - ) = _{-}^ ( x - -y)(y)\\, dy.$$ En écrivant l'intégrale comme une somme de Riemann, on constate que $ _( x - ) (,,)}$. Une conséquence est que $$ (_,,)} (,,)} .$$ On raisonne alors par l'absurde. Si on suppose $(,,)} ^0()$, on ne peut en particulier pas approcher par des éléments de $(_,,)$ les fonctions continues. D'après l'étape 2 $_$ est donc nécessairement un polynôme et ce pour tout $ ^()$ à support compact. Ce n'est possible que si $$ est un polynôme, cas exclu du théorème qu'on est en train de démontrer. C'est donc impossible. On a encore $(,,)} =^0()$. Le théorème est démontré.",
      "source_file": "annexeb.tex.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_0",
      "text": "Cette petite annexe vise \\`a observer le phénomène de collapse mentionné par (voir aussi paragraphe ) qui a pour conséquence la n\\'ecessité de fixer les paramètres de l'hypersphère recherchée. La structure de l'encodeur $_W$ donnée dans la figure . Dans les expérimentations à suivre, on considère une sphère en dimension 64 pour laquelle les hyper-paramètres ont été fixés comme $= 0.1$ et $ = 0.0003321558199348189$. La fonction de coût a optimiser est celle de la Deep SVDD. \\\\ [hbp] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(2)$}; (linear1) [rectangle, right of=input, draw, rotate=90] { Linear $(64)$}; (bn1) [rectangle, draw, right of=linear1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (linear2) [rectangle, draw, right of=lrelu1, rotate=90] { Linear $(64)$}; (bn2) [rectangle, draw, right of=linear2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (linear3) [rectangle, draw, right of=lrelu2, rotate=90] { Linear $(64)$}; [->] (input) -- (linear1); [->] (linear1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (linear2); [->] (linear2) -- (bn2); [->] (bn2) -- (lrel",
      "source_file": "annexec.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_1",
      "text": "u2); [->] (lrelu2) -- (linear3); On s'intéresse tout d'abord cas où le centre et le rayon de la sphère ne sont pas optimisés ; le rayon est fixé à 0 et le centre est fixé à une valeur arbitraire (Deep soft-boundary SVDD). La figure montre l'évolution du rayon de l'hypersphère, de la moyenne de la norme de $_W(x)$ au cours des itérations et l'histogramme des $^2 - ||_{W^*}() - ||^2$ à la fin de l'algorithme.\\\\ [htpb] {0.26} {0.26} $} {0.44} () - ||^2$} $ fixés} On observe dans les sous-figures et que l'encodeur $_W$ apprend à projeter les points d'entrée $$ sur le centre de l'hypersphère puisque la norme de $_W(x)$ converge vers une valeur non nulle et la distribution de $-||_{W^*}() - ||^2$ est autour de $0$.\\\\ On se place maintenant dans le cas où le centre de l'hypersphère est fixé à une valeur arbitraire et le rayon est fixé à 1. La figure montre les mêmes éléments que la figure mais pour le cas où le rayon de l'hypersphère est fixé à 1.\\\\ [htpb] {0.26} {0.26} $} {0.44} () - ||^2$} $ fixés} La méthode correspondante à cette expérimentation n'est pas la méthode SVDD car il n'y a pas d'optimisation concernant le rayon. L'encodeur $_W$ a placé 90\\ On considère maintenant le cas où",
      "source_file": "annexec.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_2",
      "text": "les paramètres de l'hypersphère sont libres, c'est-à-dire que le centre et le rayon de l'hypersphère sont optimisés pendant l'entraînement. La figure montre l'évolution du rayon de l'hypersphère (initialisé à 0), de la moyenne de la norme de $_W(x)$ au cours des itérations et l'histogramme des $^{*2} - ||_{W^*}() - ^*||^2$ après entraînement.\\\\ [htpb] {0.26} {0.26} $} {0.44} - ||_{W^*}() - ^*||^2$} $ libres} On observe dans que le rayon reste constant à 0 alors que $$ converge lentement vers une valeur fixe supérieure à 0. La sphère a donc dégénéré et les scores $||_{W^*}() - ^*||^2$ pour les données d'entraînement et surtout de test sont toutes proches de $0$. La méthode est donc inutilisable pour la détection d'anomalie.\\\\ Enfin, si on initialise le rayon de l'hypersphère à 1 tout en laissant les paramètres $$ et $$ libres, on observe le même phénomène de dégénérescence. Cela est illustré dans la figure .\\\\ [htpb] {0.26} {0.26} $} {0.44} - ||_{W^*}() - ^*||^2$} $ libres}",
      "source_file": "annexec.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_0",
      "text": "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\chapter{Réseaux inspirés du formalisme de l'algèbre géométrique conforme}\n\nCe chapitre est consacré à la construction de réseaux de neurones à couches hypersphériques. \nLeur définition est inspirée du modèle conforme défini par Hestenes et al. \\cite{Hestenes2001}.\nLes neurones définissent des hypersphères en dimension \\(n\\), paramétrées par un vecteur de l’algèbre géométrique conforme \\(R^{n+1,1}\\) (les poids des neurones correspondent aux paramètres de l’hypersphère).\nUne approche similaire pour les couches denses a déjà été proposée par Banarer et al. \\cite{Banarer2003}.\nIci, nous allons plus loin en introduisant également un modèle convolutif de couches hypersphériques.\\\\\n\nL’implémentation de ces nouveaux types de couches, en utilisant des opérateurs d’algèbre linéaire ou de convolution classiques, est décrite en détail, ainsi que les contraintes pour la mise à jour des poids.  \n\nLes premières expérimentations sur des données synthétiques et des bases d’images montrent que les variantes hypersphériques des couches denses et convolutives améliorent le comportement de la fonction de coût et permettent une convergen",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_1",
      "text": "ce plus rapide, à nombre de paramètres égal. Cependant, ces couches se révèlent parfois plus sensibles à l’initialisation, ce qui peut entraîner une certaine instabilité. \n\nNous nous penchons donc sur la question de l'initialisation.\nOn montre d'abord pourquoi l'approche heuristique de Glorot et Bengio \\cite{glorot} et donc la normalisation classique associée s'avèrent inefficaces dans le cadre des couches hypersphériques.\nIl faut donc comprendre comment une distribution suivant une loi de probabilité complexe se propage dans le réseau hypersphérique.\nNous mettons alors en place une stratégie d'initialisation basée sur des propriétés asymptotiques des lois de type Gamma généralisées.\nLa pertinence de la méthode est confirmée par les expérimentations numériques.\n\n\\section{Vers des réseaux de neurones à couches hypersphériques}\n\nLes réseaux de neurones \"classiques\" reposent sur un produit scalaire $\\mathbf{x} \\cdot \\mathbf{w}$ (appliqué localement dans le cas convolutif), où $\\mathbf{x}$ représente l'entrée et $\\mathbf{w}$ le vecteur de poids\\footnote{Pour simplifier, on peut supposer que $\\mathbf{x}$ est augmenté pour intégrer le biais dans le vecteur des poids: $\\mathbf{w}\\cdot \\ma",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_2",
      "text": "thbf{x} +b = (\\mathbf{w},b)\\cdot (\\mathbf{x},1)$.} (ou le filtre de convolution). Pour un vecteur $\\mathbf{x}$ de taille $n$, le nombre de paramètres de la couche est $n+1$. Une fonction d'activation peut ensuite être ajoutée. Si on utilise une activation \"\\emph{ReLU}\", la sortie de chaque couche partitionne alors $\\mathbb{R}^n$ en deux sous-espaces, séparés par un hyperplan (plus généralement, selon le choix de la fonction d'activation, les valeurs positives du produit scalaire sont conservées, tandis que les valeurs négatives sont atténuées ou annulées). \\\\\n\nL'idée développée dans cette thèse est de faire une partition de $\\mathbb{R}^n$ par des hypersphères plutôt que par des hyperplans. On note que la partition induite dans ce cas est faite entre deux sous-espaces dont l’un est compact. \\\\\n\nLes hypersphères pourraient être paramétrées de manière classique, si bien qu'à l'étape d'apprentissage, l’optimisation des centres et des rayons serait distincts. Cette méthode serait donc similaire à ce qui est classiquement utilisé pour les fonctions à base radiale (réseaux dits RBF). \nBien que cette approche soit valide, elle occulte le lien géométrique entre hyperplan et hypersphère : un",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_3",
      "text": "hyperplan peut être considéré comme une hypersphère dont le centre est situé à l'infini et dont le rayon est infini. \nPour répondre à ce besoin d'unification, nous adoptons dans la suite le formalisme des algèbres géométriques conformes, dans lequel hyperplans comme hypersphères sont représentés par un vecteur de dimension $n+2$. Dans ce cadre mathématique, Banarer et al. \\cite{Banarer2003} ont déjà défini un modèle de neurone hypersphérique qui constitue la base de ce travail.\nNous allons l'incorporer dans des réseaux à couches denses ou convolutives. Mais nous commençons par quelques rappels sur le formalisme de l'algèbre géométrique conforme.\n\n\n\\subsection*{Formalisme de l'algèbre géométrique conforme }\n\\label{sec:formaliste_alc}\n\nOn décrit ici des éléments concernant les algèbres géométriques et le modèle conforme. \nLe vocabulaire est mathématique mais, il faut garder à l'esprit que l'idée derrière la mobilisation des outils algébriques qui suivent est de fournir une représentation uniforme et ''simple'' des hyperplans et des hypersphères. \n\n\\subsubsection{Le modèle conforme d'Hestenes}\n\n\nNous considérons l'espace euclidien $\\mathbb{R}^n$, l'espace vectoriel sous-jacent $\\math",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_4",
      "text": "bb R^n$ étant muni d'une base orthonormale $(e_1,\\dots,e_n)$. La construction suivante explique en particulier par l'utilisation de la projection stéréographique pourquoi le modèle d'espace auquel on aboutit est qualifié de conforme. On commence par plonger l'espace euclidien dans un espace de dimension $n+1$ au moyen de l'inverse d'une projection stéréographique :\n\\begin{equation*}\n{\\bf x}=x_1 e_1 + \\cdots + x_n e_n \\longmapsto {2(x_1 e_1 + \\cdots + x_n e_n) \\over x_1^2 + \\cdots + x_n^2 + 1}+ {x_1^2 + \\cdots + x_n^2 - 1 \\over x_1^2 + \\cdots + x_n^2 + 1} e_+,\n\\end{equation*}\noù $e_+$ désigne le point à l'infini. Ensuite on homogénéise le résultat obtenu en ajoutant le vecteur $e_-$. Ceci donne le plongement de $\\mathbb{R}^n$ défini par :\n\\begin{gather*}\n{\\bf x}=x_1 e_1 + \\cdots + x_n e_n \\longmapsto x_1 e_1 + \\cdots + x_n e_n+{(x_1^2 + \\cdots + x_n^2)\\over 2}(e_++e_-)+{(e_--e_+)\\over 2}\\\\\n{\\bf x}\\longmapsto {\\bf x}+{(x_1^2 + \\cdots + x_n^2)\\over 2}(e_++e_-)+{(e_--e_+)\\over 2}=\\tilde x\n\\end{gather*}\n\nL'intérêt de ce plongement est qu'il va permettre de considérer les objets de base de la géométrie euclidienne comme des vecteurs. Pour rendre compte de cette idée, il est nécessaire d'",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_5",
      "text": "introduire un peu de vocabulaire mathématique. Tout d'abord, on considère sur l'espace $\\mathbb{R}^{n+1,1}$ muni de la base formée des vecteurs $e_1$,$\\ldots$, $e_n$, $e_+$, et $e_-$ la forme quadratique\n\\begin{equation*}\nQ_{n+1,1}=\\left(\\begin{array}{ccccc}1 & 0 & \\cdots & 0 & 0 \\\\0 & 1 & \\cdots & 0 & 0 \\\\\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\0 & 0 & \\cdots & 1 & 0 \\\\0 & 0 & \\cdots & 0 & -1\\end{array}\\right).\n\\end{equation*}\n\nAu couple $(\\mathbb{R}^{n+1,1}, Q_{n+1,1})$, on associe l'algèbre géométrique conforme qui, formellement, est le quotient de l'algèbre tensorielle de $\\mathbb{R}^{n+1,1}$ par l'idéal bilatère engendré par les éléments du type $a\\otimes a-Q_{n+1,1}(a)$. On dispose alors d'un produit, dit géométrique, tel que pour tout vecteur $a$ de l'algèbre géométrique conforme, on a\n\\begin{equation*}\naa=a^2=Q_{n+1,1}(a).\n\\end{equation*}\nPlus généralement, le produit géométrique de deux vecteurs $a$ et $b$ s'écrit\n\\begin{gather*}\n\tab = \\underbrace{\\frac{1}{2}(ab + ba)}_{\\text{symétrique}}+ \\underbrace{\\frac{1}{2}(ab - ba)}_{\\text{anti-symétrique}}\n\\end{gather*}\navec\n\\begin{gather*}\n{\\frac{1}{2}(ab + ba)}=a._i b=b._i a= B_{n+1,1}(a,b)\n\\end{gather*}\n$B_{n+1,1}$ étant la",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_6",
      "text": "forme bilinéaire symétrique associée à $Q_{n+1,1}$, et\n\\begin{gather*}\n{\\frac{1}{2}(ab - ba)}=a\\wedge b=-b\\wedge a.\n\\end{gather*}\nTraditionnellement, le produit $a._i b$ est appelé produit interne, il s'agit d'un réel. Le produit extérieur $a\\wedge b$ est un bivecteur qui peut s'interpréter comme l'analogue en dimension deux d'un vecteur, {\\em i.e.} ``un morceau'' de plan vectoriel muni d'une orientation.\n\nLe bivecteur\n\\begin{equation*}\nE=e_+e_-=e_+\\wedge e_-\n\\end{equation*}\ndétermine une décomposition dite conforme\n\\begin{equation*}\n\\mathbb{R}^{n+1,1}=\\mathbb{R}^{n}\\oplus\\mathbb{R}^{1,1}\n\\end{equation*}\noù $\\oplus$ désigne la somme directe  et  $\\mathbb{R}^{1,1}$ le plan de Minkowski.\nPlus précisément, si nous notons $e_\\infty=e_++e_-$, $e_0=(e_--e_+)/2$ (voir Li {\\it et al} \\cite{Li2001}), et\n\\begin{equation}\na= a_1 e_1 + \\cdots + a_n e_n + a_\\infty e_\\infty + a_0e_0,\n\\end{equation}\nun vecteur générique de $\\mathbb{R}^{n+1,1}$, on peut décomposer $a$ en\n\\begin{equation*}\na=\\pi_E(a)+\\pi_E^{\\bot}(a),\n\\end{equation*}\noù $\\pi_E$ est la projection définie par\n\\begin{equation*}\n\\pi_E(a)=(a._i E)E=a_\\infty e_\\infty+a_0e_0,\n\\end{equation*}\net $\\pi_E^{\\bot}$ est la réjection définie par",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_7",
      "text": "\\begin{equation*}\n\\pi_E^{\\bot}(a)=(a\\wedge E)E=a_1 e_1 + \\cdots + a_n e_n\n\\end{equation*}\npermet de retrouver la partie euclidienne de $a$. \\\\\nPour définir la projection $\\pi_E$ et la réjection $\\pi_E^{\\bot}$, il faut utiliser le produit interne et le produit extérieur entre un vecteur et un bivecteur. Comme nous ne les utiliserons pas par la suite, nous omettons ici les détails.\n\nOn vérifie facilement que  \n\\begin{equation} \n\te_{0}^2 = e_{\\infty}^2 =0,~~e_{\\infty}._i e_0 =-1\n\t\\label{eq:propminkoski}\n\\end{equation}\nDésignons par $\\mathcal P^{n+1}(e_\\infty,e_0)$ l'hyperplan de $\\mathbb R^{n+1,1}$  normal à  $e_\\infty$ et contenant $e_0$, donné par l'équation\n\\begin{equation*}\ne_\\infty._i(a-e_0)=0,\n\\end{equation*}\net par $\\mathcal N^{n+1}$ le cône nul de $\\mathbb R^{n+1,1}$ donné par l'équation\n\\begin{equation*}\na^2=a._i a=0.\n\\end{equation*}\n\n\\noindent{\\bf Définition. }\nLa représentation conforme de l'espace euclidien $\\mathbb{R}^n$ associée à la décomposition conforme donnée par $E=e_+\\wedge e_-=e_\\infty\\wedge e_0$, est l'horosphère\n\\begin{equation*}\n\\mathcal H^n(e_\\infty, e_0)=\\mathcal N^{n+1}\\cap \\mathcal P^{n+1}(e_\\infty,e_0)=\\left\\{a\\in\\mathbb R^{n+1,1},\\ a^2=0,\\ e_\\infty\\cdot(a",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_8",
      "text": "-e_0)=0\\right\\}.\n\\end{equation*}\n\nDans la suite, nous noterons $\\widetilde x$ le vecteur de $\\mathbb R^{n+1,1}$ image du plongement du point ${\\bf x}$ de l'espace euclidien $\\mathbb R^n$ dans cette horosphère, {\\em i.e.}\n\\begin{equation}\n\\widetilde x={\\bf x}+{{\\bf x}^2\\over 2}e_\\infty+e_0.\n\\label{eq:plongement}\n\\end{equation}\n\n\n\\subsubsection{Géométrie euclidienne et algèbre géométrique conforme}\nEn notant $\\mathbf{p}$ et $\\mathbf{q}$ deux points de l'espace euclidien $\\mathbb R^n$, on a en particulier,\n \\begin{gather*}\n\t\\tilde{p}._i\\tilde{q}  =(\\mathbf{p}+\\frac{1}{2}\\Vert\\mathbf{p}\\Vert^2 e_{\\infty} + e_0)._i(\\mathbf{q}+\\frac{1}{2}\\Vert\\mathbf{q}\\Vert^2 e_{\\infty} + e_0)=\\mathbf{p}\\cdot\\mathbf{q} + \\frac{1}{2}\\Vert\\mathbf{q}\\Vert^2 \\underbrace{\\mathbf{p}._ie_{\\infty}}_{0}+ \\underbrace{\\mathbf{p}._ie_0}_{0}\\nonumber\\\\\n\t+\\frac{1}{2}||\\mathbf{p}||^2 \\left(\\underbrace{\\mathbf{q}._ie_{\\infty}}_{0} + \\frac{1}{2}\\Vert\\mathbf{q}\\Vert^2 \\underbrace{e_{\\infty}._ie_{\\infty}}_{0} + \\underbrace{e_{\\infty}._ie_{0}}_{-1}\\right) + \n\t \\underbrace{e_{0}._i\\mathbf{q}}_{0} + \\frac{1}{2}\\Vert\\mathbf{q}\\Vert^2 \\underbrace{e_{0}._ie_{\\infty}}_{-1} + \\underbrace{e_{0}._ie_{0}}_{0}\\nonumber\\\\\n= -\\frac{1}{",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 8,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_9",
      "text": "2}\\Vert\\mathbf{p}-\\mathbf{q}\\Vert^2 \\label{eq:pspp}\n\\end{gather*}\n\nCette formule permet de comprendre comment définir une hypersphère de $\\mathbb R^n$ comme un vecteur de l'algèbre géométrique conforme et comment tester par un produit intérieur l'appartenance d'un point à cette hypersphère. Soit donc une hypersphère de centre $\\mathbf{c}$ et de rayon $\\rho$, {\\em i.e.} l'ensemble des $\\mathbf{x}$ vérifiant~:\n\n\\begin{equation*}\n\t\\Vert\\mathbf{x}-\\mathbf{c}\\Vert^2 = \\rho^2.\n\t\\label{eq:defhs}\n\\end{equation*}\n\nAvec les notations introduites précédemment, cette équation peut s'écrire sous la forme :\n\\begin{equation*}\n\\tilde{x}._i \\tilde{c} = -\\frac{1}{2}\\Vert\\mathbf{x}-\\mathbf{c} \\Vert^2=-\\frac{1}{2}\\rho^2.\n\\end{equation*}\n\nOn en déduit qu’en utilisant $\\tilde{x}._i e_{\\infty}=-1$ :\n\\begin{gather*}\n\t\t\\tilde{x}._i \\tilde{c} = -\\frac{1}{2} \\rho^2 \\Longleftrightarrow \\tilde{x}._i \\tilde{c} - \\frac{1}{2} \\rho^2 = 0 \\\\\n\t\t\\Longleftrightarrow\\tilde{x}._i \\tilde{c} - (-\\frac{1}{2}\\rho^2)(-1)=0 \\Longleftrightarrow \\tilde{x}._i \\tilde{c} -(\\frac{1}{2}\\rho^2)(x._i e_{\\infty})=0\\Longleftrightarrow \\\\\n\t\t\\Longleftrightarrow \\tilde{x}._i \\underbrace{(\\tilde{c}-\\frac{1}{2}\\rho^2e_{\\infty})}_{\\tilde{s}}=",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 9,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_10",
      "text": "0 \\Longleftrightarrow \\tilde{x}._i \\tilde{s} = 0,\n\\end{gather*}\\\\\navec\n\\begin{equation}\n\t\t\\tilde{s} = \\mathbf{c} + e_0 + \\frac{1}{2}\\parallel \\mathbf{c} \\parallel^2 e_{\\infty} - \\frac{1}{2}\\rho^2 e_{\\infty}\n\t\t\\label{eq:tildes}\n\\end{equation}\n\t\t\nPar conséquent, dans l'algèbre géométrique conforme, l'hypersphère de centre $\\mathbf{c}$ et de rayon $\\rho$ correspond au vecteur $\\tilde{s}$ de $\\mathbb{R}^{n+1,1}$ mentionné précédemment. On constate par ailleurs que deux contraintes de normalisation doivent être satisfaites : \n\n\\begin{equation*}\n\t\\begin{array}{l}\n\t\t\\tilde{s}^2 = \\rho^2 >0,\\ \\ e_{\\infty}._i \\tilde{s} = -1.\n\t\\end{array} \n\\end{equation*}\n\nLes propriétés que l'on retient pour la suite sont les suivantes : le point ${\\bf x}$ est\n\n\\begin{itemize}\n\t\\item à l'intérieur de l'hypersphère si $$ 0 < \\tilde{x} ._i \\tilde{s} \\leq \\frac{1}{2} \\rho^2,$$\n\t\\item sur l'hypersphère si $$\\tilde{x}._i \\tilde{s} = 0,$$\n\t\\item à l'extérieur de l'hypersphère s'il vérifie $$\\tilde{x}._i \\tilde{s} < 0.$$\n\\end{itemize}\n\nEn particulier,\n\\begin{equation*}\n\\tilde{x}._i \\tilde{c}=\\frac{1}{2}\\rho^2 \\quad \\mbox{ ssi } \\bf x=\\bf c .\n\\end{equation*}\n\n\nLes figures \\ref{fig:ps} et \\ref{fig:ds} illustrent les",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 10,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_11",
      "text": "variations du produit $\\tilde{x}._i \\tilde{s}$ pour une sphère de centre $(0, 0)$, de rayon égal à 5. \n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\begin{tikzpicture}\n\t\t\\begin{axis}[height=10cm,\n\t\t\twidth=10cm,\n\t\t\tgrid= major ,\n\t\t\txlabel = {$x_0$} ,\n\t\t\tylabel = { $\\tilde{x}._i \\tilde{s}$} ,\n\t\t\txmin=-10,xmax=10,\n\t\t\tlegend entries={$\\frac{1}{2}\\rho^2$},\n\t\t\t]\n\t\t\t%\\addplot table [x=a, y=c, col sep=comma] {ProduitScalaire.csv};\n\t\t\t%\\addplot coordinates {(0,0) (0.1,200) (0.2,300) (0.3,350) (0.4,375) };\n\t\t\t\\addplot[mark =*, green] coordinates {(0,12.25)};\n\t\t\t\\addplot[mark=none, blue] coordinates {(-10,0) (10,0)};\n\t\t\t\\addplot[red, mark=none] table [x=a, y=b,col sep=comma] {sources/ProduitScalaire.csv};\n\t\t\\end{axis}\n\t\\end{tikzpicture}\n\t\\caption{Profil du produit  $\\tilde{x}._i \\tilde{s}$}\n\t\\label{fig:ps}\n\t\n\\end{figure}\n\n\\begin{figure}[H]\n\t\\begin{center}\n\t\t\\includegraphics[width=13cm]{figs/ps2d.pdf}\n\t\t\\caption{Distance signée $\\tilde{x}._i \\tilde{s}$}\n\t\t\\label{fig:ds}\n\t\\end{center}\n\\end{figure}\n\nIl est possible de même de représenter l'hyperplan de $\\mathbb R^n$ de vecteur normal unitaire $\\bf n$ et passant par le point ${\\bf a}$ par le vecteur $\\widetilde h$ de l'algèbre géométrique conforme qui s'é",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 11,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_12",
      "text": "crit\n\\begin{equation*}\n\\widetilde h = \\bf n+\\delta e_\\infty,\n\\end{equation*}\navec $\\delta = n\\cdot{\\bf a}$. On vérifie facilement que \n\n\\begin{equation*}\n\\widetilde x._i\\widetilde h=\\bf n\\cdot({\\bf x}-{\\bf a}).\n\\end{equation*}\n\nSi $x$ appartient à l'hyperplan représenté par $\\tilde{h}$ alors $\\tilde{x}._i\\tilde{h}=0$. \nGrâce à cette description, on peut vérifier que l'hypersphère représentée par le vecteur\n\\begin{equation*}\n\\widetilde s=({\\bf c}+\\alpha {\\bf n})+{(\\bf c+\\alpha \\bf n)^2\\over 2}e_\\infty+e_0-{(\\alpha +\\rho)^2\\over 2}e_\\infty\n\\end{equation*}\n{\\em i.e.} l'hypersphère de centre ${\\bf c}+\\alpha {\\bf n}$ et de rayon $\\alpha +\\rho$ tend vers  l'hyperplan $\\widetilde h$ précédent lorsque $\\alpha>0$ tend vers l'infini. En effet, par la renormalisation \n\\begin{equation*}\ne_\\infty._i \\widetilde s=-1/\\alpha,\n\\end{equation*}\nl'hypersphère correspond au vecteur\n\\begin{gather*}\n\\widetilde s=\n{(\\bf c+\\alpha \\bf n)\\over \\alpha}+{(\\bf c+\\alpha \\bf n)^2\\over 2\\alpha}e_\\infty+{e_0\\over\\alpha}-{(\\alpha +\\rho)^2\\over 2\\alpha}e_\\infty\\\\\n=\\left({\\bf c\\over\\alpha}+\\bf n\\right)+\\left({{\\bf c}^2\\over 2\\alpha}+{\\bf c}\\cdot {\\bf n}+{\\alpha\\over 2}-{\\alpha\\over 2}-\\rho-{\\rho^2\\over 2\\alpha}\\right)",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 12,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_13",
      "text": "e_\\infty+{e_0\\over\\alpha}.\n\\end{gather*}\nQuand $\\alpha>0$ tend vers l'infini, alors ce vecteur tend vers\n\\begin{equation*}\n\\widetilde h=\\bf n+(\\bf c\\cdot \\bf n-\\rho)e_\\infty,\n\\end{equation*}\navec\n\\begin{equation*}\n\\bf a=\\bf c-\\rho\\bf n,\\ \\ \\bf n\\cdot(\\bf c-\\rho \\bf n)=\\bf c\\cdot \\bf n-\\rho.\n\\end{equation*}\n\nCe type de calcul, donné en exemple, illustre bien la cohérence des définitions et des opérations du modèle conforme d'Hestenes.\n\n\n%%%%%%%%%%%%%%%%%%%%%%\n\\newpage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Définition des couches hypersphériques}\n\n\nLes modèles de réseaux à couches hypersphériques se composent d'une ou plusieurs hypersphères, dont les paramètres, représentant le centre et le rayon, sont ajustés au cours de l'entraînement pour optimiser la séparation des données. Sans ajout de fonction d'activation, ces couches permettent de créer des frontières décisionnelles non linéaires, contrastant ainsi avec les couches traditionnelles.\n\n\nDans les réseaux de neurones classiques, les couches sont généralement de type dense ou à convolution. Les couches de type dense permettent de réaliser des séparations complexes dans l'espace des caractéristiques, tandis que les couches à convolution appliq",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 13,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_14",
      "text": "uent ce principe de manière localisée, ce qui est particulièrement utile pour le traitement d'images. \n\nNous montrons ici comment construire l'analogue de ces deux types de couches à partir du modèle conforme décrit précédemment.\n\n\n\\subsection{Couches hypersphériques denses}\n\nUne couche hypersphérique a été développée par \\cite{Banarer2003}  en utilisant des hypersphères représentées par des vecteurs de \\(\\mathbb{R}^{n+1,1}\\).\n\nLe schéma \\ref{fig:reseau1cc} illustre un réseau de neurones avec une couche cachée utilisant une hypersphère. La première étape, intégrée dans l'implémentation de la couche hypersphérique, consiste à envoyer les données d'entrée dans l'espace de l'algèbre géométrique conforme en utilisant le plongement décrit par \\eqref{eq:plongement}. Ensuite, il est nécessaire de prendre en compte la contrainte d'unicité, qui peut être considérée selon plusieurs méthodes décrites dans la section \\ref{sec:sec_optimisation}.\n\nLa sortie de la couche pour une hypersphère de centre $\\mathbf{c}$ et de rayon $\\rho$  s'écrit~:\n\n\n\\begin{equation}\n\ty = -\\frac{1}{2} \\left[ \\| \\mathbf{x} - \\mathbf{c} \\|^2 - R^2 \\right]\n\\end{equation}\n\n\nLe signe de \\(y\\) détermine la position relative",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 14,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_15",
      "text": "du point \\(\\mathbf{x}\\) par rapport à l'hypersphère, conformément aux propriétés établies précédemment. Cette quantité est donc calculée par le produit interne $\\tilde{s}\\cdot_i \\tilde{x}$.\n\\bigskip\n\n\\def\\layersep{6cm}\n\\begin{figure}[htbp]\n\t\\centering\n\t\\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\\layersep]\n\t\t\\tikzstyle{every pin edge}=[<-,shorten <=1pt]\n\t\t\\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n\t\t\\tikzstyle{neuron_}=[]\n\t\t\\tikzstyle{enter neuron}=[neuron, fill=red!100];\n\t\t\\tikzstyle{input neuron}=[neuron, fill=yellow!120];\n\t\t\\tikzstyle{output neuron}=[neuron, fill=green!100];\n\t\t% Draw neuron_ info\n\t\t\\foreach \\name / \\y in {1,...,2}\n\t\t\\node[enter neuron, left of=J, pin=left:{\\tiny $e_{\\name}$}] (K-\\name) at (0,-\\y+1/2) {$x_{\\y}$};\n\t\t\\node[enter neuron, left of=J, pin=left:{\\tiny $e_0$}] (K-3) at (0,-3+1/2) {$1$};\n\t\t\\node[enter neuron, left of=J, pin=left:{\\tiny $e_{\\infty}$}] (K-4) at (0,-4+1/2) {$\\frac{||x||^2}{2}$};\n\t\t% Draw the input layer nodes\n\t\t\\foreach \\name / \\y in {0}\n\t\t\\node[input neuron] (J-\\name) at (0,-1.75) {$y = \\tilde{s}._i\\tilde{x}$};\n\t\t\\foreach \\name / \\y in {0}\n\t\t\\path (K-1) edge (J-\\name);\n\t\t\\foreach \\name /",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 15,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_16",
      "text": "\\y in {0}\n\t\t\\path (K-2) edge (J-\\name);\n\t\t% Draw the input layer nodes\n\t\t\\foreach \\name / \\y in {0}\n\t\t\\path (K-3) edge[dashed,->,red] (J-\\name);\n\t\t\\foreach \\name / \\y in {0}\n\t\t\\path (K-4) edge[dashed,->,red] (J-\\name);\n\t\\end{tikzpicture}\n\t\\caption{Schéma d'un réseau à une couche cachée hypersphérique}\n\t\\label{fig:reseau1cc}\n\\end{figure}\n\nOn peut noter que la définition du produit interne induit les différences suivantes sur le paramétrage des couches hypersphériques~:\n\\begin{itemize}\n\t\\item  Couche dense ($n+1$ paramètres) : le produit $\\mathbf{w} \\cdot \\mathbf{x} $ est linéaire en $x$ et n'est pas borné\n\t\\item Couche sphérique   ($n+1$ paramètres) : le produit $\\tilde{x} ._i \\tilde{s}$ est quadratique en les coordonnées de $\\mathbf{x}$ et est majoré par $\\rho^2/2$\n\\end{itemize}\n\n\\bigskip\n\n\\noindent\\textbf{Remarque :} Comme expliqué précédemment, les paramètres du vecteur conforme représentant l'hypersphère \\(\\tilde{s}\\) sont ajustés par l'apprentissage. Ses paramètres encodent à la fois le centre (les $n$ premières coordonnées) et une forme implicite du rayon à travers la composante en $e_{\\infty}$ qui sont ajustées par le processus d'apprentissage.\n\n\n\\subsection{Couches convolut",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 16,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_17",
      "text": "ives}\n\nL'extension du modèle de neurone hypersphérique à un filtre de convolution hypersphérique ne nécessite pas de nouveaux outils mathématiques. Cette extension sera décrite pour un filtre de convolution 2D discret, applicable au traitement des images ; toutefois, l'approche reste valable en toute dimension, indépendamment du mode de \\textit{padding} et des valeurs de \\textit{strides} choisies.\n\nLa convolution standard est bien connue pour être équivalente au produit scalaire entre les versions vectorisées du bloc image et du filtre \\( F \\) auquel est ajouté un biais~:\n\n\n\\begin{gather*}\n\t(I * F)[j,j']=  \\sum_{d_j} \\sum_{d_{j'}} I[j+d_j, j'+d_{j'}] * F[d_j, d_{j'}] + b\n\t\\nonumber \\\\\n\t= \\mathrm{vec}(I[j-d_j:j+d_j, j'-d_{j'}:j'+d_{j'}]) \\cdot\n\t\\mathrm{vec}(F[-d_j:d_j, -d_{j'}:d_{j'}])+ b .   \n\t%\\label{eq:vecps}\n\\end{gather*}\noù  $d_j$ et $d_{j'}$ désignent les indices de parcours du filtre qui dépendent de la taille du filtre $d \\times d$.\nDe façon naturelle, on peut voir vec$(I[j-d_j:j+d_j, j'-d_{j'}:j'+d_{j'}])$ comme un vecteur $x_I(j,j')$ de $\\mathbb{R}^{d^2}$ que l'on va plonger dans l'algèbre géométrique conforme $\\mathbb{R}^{d^2+1,1}$. Le filtre de convolution est remplacé p",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 17,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_18",
      "text": "ar une sphère  en dimension $\\mathbb{R}^{d^2}$ paramétrée dans cette même algèbre. Si l'on note $\\circledast$ cette \"nouvelle\" convolution, on en définit le résultat comme suit~: \n\n\n\\begin{equation}\n\t(I \\circledast s)[j,j'] = x_I(j,j') ._i s\n\t\\label{eq:sphericalconv}\n\\end{equation}\n\nLa valeur de sortie de ce filtre possède donc les mêmes propriétés que le produit interne $\\tilde{s}._i\\tilde{x}$ défini dans la sous-section précédente (voir figure \\ref{fig:ps}). L'extension de ce filtre aux images à $c$-canaux ne pose pas de difficultés. Dans cet esprit, on envisagera deux approches, ``Conv2d'' et ``DepthWiseConv2d'' (dans le cas où une hypersphère est définie par canal)~:\\\\\n\n\\begin{itemize}\n\t\\item $(I \\circledast s)[j,j'] = \\sum_c x_I(j,j',c) ._i s$ ($\\sim$ Conv2d)\\\\\n\t\n\t\\item $(I \\circledast s)[j,j'] = \\sum_c x_I(j,j',c) ._i s_c$  ($\\sim$ DepthWiseConv2d)\n\\end{itemize}\n\n\n\\begin{figure}[H]\n\t\\begin{center}\t\t\\includegraphics[width=16cm]{figs/dessin_prod_convol_img_spheric2.png}\n\t\t\\caption{Approche par convolution hypersphérique}\n\t\t\\label{fig:conv}\n\t\\end{center}\n\\end{figure}\n\n\\subsection{Implémentation des couches hypersphériques}\n\n\n\\subsubsection{Couches denses (formulation matricielle",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 18,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_19",
      "text": ")}\n\n\n\nDans la base  $\\{e_1, e_2,\\ldots,e_n, e_0, e_{\\infty}\\}$, le vecteur $\\mathbf{x}$ et la sphère $s$  de centre $\\mathbf{c}$  et de rayon $\\rho$ s'expriment comme des vecteurs à $n+2$ coordonnées~:\n\\begin{align}\n\t\\mathbf{x}  & \\mapsto  \\check{x}:= [x_1, x_2, \\ldots, x_n, 1, \\frac{1}{2} ||\\mathbf{x}||^2]^{t}\\label{eqcheckx}\\\\\n\t(\\mathbf{c}, \\rho) & \\mapsto \\check{s} := [c_1, c_2, \\ldots, c_n, 1, \\frac{1}{2} (||\\mathbf{c}||^2-\\rho^2)]^{t}\n\\end{align}\n\\\\\n\n\nPour garantir l'efficacité du calcul du produit interne $._i$, celui-ci va être exprimé comme un produit matriciel $\\check{x}^{t} M \\check{s}$.\nOn rappelle que les règles de calcul suivantes doivent être vérifiées :\n\\begin{equation}\n\t\\left\\{\\ \\begin{array}{l r}\n\t\te_j ._i e_{j'} = \\delta_{jj'}, & \\forall j,j' \\in \\{1,\\ldots,n\\}\\\\\n\t\te_j ._i e_{0} = e_j ._i e_{\\infty} = 0\\\\\n\t\te_{\\infty} ._i e_0 = e_0 ._i e_{\\infty} = -1\n\t\\end{array}\\right.\n\t\\label{eq:metrique}\n\\end{equation}\navec\n\\[\n\\delta_{jj'} = \\begin{cases} \n\t1 & \\text{si } j = j', \\\\\n\t0 & \\text{sinon}.\n\\end{cases}\n\\]\n\n\nOn peut vérifier que la matrice $M$ qui traduit les relations données dans le système  \\eqref{eq:metrique} s'écrit~:\\\\\n\n\\begin{equation}\n\tM =\\left( \\begin{array}",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 19,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_20",
      "text": "{cccccc}\n\t\t\n\t\t1 & 0 & \\dots & 0 & 0 & 0\\\\\n\t\t\n\t\t0 & 1 & \\dots & 0 & 0 & 0\\\\\n\t\t\n\t\t\\vdots & \\vdots & \\ddots & 0 & 0 & 0\\\\\n\t\t\n\t\t0 & 0 & \\dots & 1 & 0 & 0\\\\\n\t\t\n\t\t0 & 0 & \\dots & 0 & 0 & -1\\\\\n\t\t\n\t\t0 & 0 & \\dots & 0 & -1 & 0\\\\\n\t\t\n\t\\end{array}\\right)\t\n\\end{equation}\\\\\n\n\nOn peut alors vérifier (voir les calculs ci-dessous) les deux propriétés  suivantes pour le produit de deux points ou d'un point et d'une hypersphère~:\\\\\n\n\\begin{equation}\n\t\\check{x}^{t} M \\check{s} =  \\frac{1}{2}\\left(\\rho^2 - ||\\mathbf{x} - \\mathbf{c}||^2\\right)\n\t\\mbox{~~et~~}\\check{p}^{t} M \\check{q} =  -\\frac{1}{2}||\\mathbf{p} - \\mathbf{q}||^2\n\t\\label{eq:ecriturematriciel}\n\\end{equation}\\\\\n\n\n\\noindent Vérification : produit $\\tilde{p} ._i \\tilde{q}$ entre deux points :\n\n$\\begin{array}{lcl}\n\t\\check{p}^{t} M \\check{q} & = & (\\mathbf{p}^t) M \\mathbf{q}\n\t\\\\\n\t& = & \\left(\\mathbf{p}^t, 1 , \\frac{1}{2}\\parallel \\mathbf{p} \\parallel^2\\right) \n\t\\left(\n\t\\begin{array}{ccc}\n\t\tId_{n} & 0 & 0 \\\\\n\t\t0 & 0 & -1 \\\\\n\t\t0 & -1 & 0\n\t\\end{array}\n\t\\right)\n\t\\left(\\begin{array}{c}\n\t\t\\mathbf{q}\\\\\n\t\t1\\\\\n\t\t\\frac{1}{2}\\parallel \\mathbf{q} \\parallel^2\n\t\\end{array}\\right)\n\t\t\\\\\n\t& = & \\left(\\mathbf{p}^t, -\\frac{1}{2}\\parallel \\mathbf{p} \\parallel^2, -1",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 20,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_21",
      "text": "\\right) \\left(\\begin{array}{c}\n\t\t\\mathbf{q}\\\\\n\t\t1\\\\\n\t\t\\parallel \\mathbf{q} \\parallel^2\n\t\\end{array}\\right)\\\\\n\t\\\\\n\t& = & \\mathbf{p}^t \\mathbf{q} - \\frac{1}{2} \\left(\\parallel \\mathbf{p} \\parallel^2 - \\parallel \\mathbf{q} \\parallel^2\\right)\\\\\n\t\\\\\n\t& = & -\\frac{1}{2} \\parallel \\mathbf{p} - \\mathbf{q} \\parallel^2\n\\end{array}$\n\n\\vspace{0.75cm}\n\n\\noindent Vérification : produit entre un point et une hypersphère :\n\n$\\begin{array}{lcl}\n\t\\check{x}^{t} M \\check{s} & = & \\left(\\mathbf{x}^t, 1 , \\frac{1}{2}\\parallel \\mathbf{x} \\parallel^2\\right) \\left(\\begin{array}{ccc}\n\t\tId_n & 0 & 0\\\\\n\t\t0 & 0 & -1\\\\\n\t\t0 & -1 & 0\n\t\\end{array}\\right)\\left(\\begin{array}{c}\n\t\t\\mathbf{c}\\\\\n\t\t1\\\\\n\t\t\\frac{\\parallel \\mathbf{c} \\parallel^2 - \\rho^2}{2}\n\t\\end{array}\\right)\n\t\\\\\n\t& = & \\left(\\mathbf{x}^t, -\\frac{1}{2}\\parallel \\mathbf{x} \\parallel^2, -1\\right) \\left(\\begin{array}{c}\n\t\t\\mathbf{c}\\\\\n\t\t1\\\\\n\t\t\\frac{\\parallel \\mathbf{c} \\parallel^2 - \\rho^2}{2}\n\t\\end{array}\\right)\\\\\n\t\t\\\\\n\t& = & \\frac{1}{2}\\left(\\rho^2 - \\parallel \\mathbf{x}-\\mathbf{c} \\parallel^2\\right)\n\t\n\\end{array}$\n\n\\vspace{1cm}\n\n\\subsubsection{Couches convolutives}\n\nPour le filtre hypersphérique, le calcul de $\\check{x}^{t} M \\check{s}$ peut être divisé",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 21,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_22",
      "text": "en trois étapes distinctes :\n\n\\begin{enumerate}\n\t\\item La convolution standard de l'image avec le filtre correspondant aux $n$ premières coordonnées de $\\check{s}$ redimensionnées à la taille $d \\times d$.\t\n\t\\item L'opération $-\\frac{1}{2}||x||^2$ revient à faire la convolution de l'image par un filtre constant de valeur $-\\frac{1}{2}$ et de taille $d \\times d$. Le résultat est multiplié par l'avant-dernière coordonnée de $\\check{s}$ qui vaut 1.\n\t\\item Une multiplication terme à terme est effectuée pour obtenir le tenseur contenant les coefficients correspondant au produit entre la composante $e_0$ de $\\check{x}$ et la composante  $e_{\\infty}$ de $\\check{s}$.\n\\end{enumerate}\n\nLe résultat de la convolution hypersphérique est la somme de ces trois termes.\nPar ailleurs, si l'image traitée contient plusieurs canaux, une nouvelle somme selon les canaux de profondeur permet de reproduire le comportement du réseau de type Conv2D.\n\n\n\\subsubsection{Optimisation des poids du réseau de neurones}\n\\label{sec:sec_optimisation}\n\nUne hypersphère est représentée de manière unique par un vecteur \\(\\tilde{s}\\) dont l'avant-dernière coordonnée est fixée à 1, conformément à la contrainte \\(\\tilde{s} \\c",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 22,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_23",
      "text": "dot e_{\\infty} = -1\\). Pour optimiser les paramètres de l'hypersphère, c'est-à-dire les coordonnées de son centre \\(\\mathbf{c}\\) et la valeur de son rayon \\(\\rho\\), tout en respectant cette contrainte, plusieurs approches peuvent être envisagées :\\\\\n\n\\begin{itemize}\n\\item Une première méthode consiste à effectuer la descente du gradient directement sur les $n+2$ coordonnées puis normaliser le vecteur résultat pour garantir \\(\\tilde{s} \\cdot e_{\\infty} = -1\\). La normalisation consiste à diviser le vecteur dans la base $\\{e_1, e_2,\\ldots,e_n, e_0, e_{\\infty}\\}$ par la composante en $e_0$. \\\\\n\n\\item Une autre approche fixe la composante \\(e_0\\) de \\(\\tilde{s}\\) à 1, et l'optimisation est alors effectuée uniquement sur les \\(n+1\\) autres éléments du vecteur. Cette méthode simplifie l'optimisation en réduisant le nombre de paramètres à ajuster, tout en maintenant la contrainte nécessaire.\\\\\nC'est la méthode qui a été privilégiée dans les codes utilisés pour cette thèse.\\\\\n\n\\item Enfin, une perspective possible est de développer une méthode de descente de gradient qui opère directement dans l'espace des sphères, en garantissant que le produit \\(e_0 \\cdot e_{\\infty}\\) reste égal à -1 tou",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 23,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_24",
      "text": "t au long du processus d'optimisation. Cela permettrait de maintenir la contrainte \\(s ._i e_{\\infty} = -1\\) de manière plus intégrée, évitant ainsi la nécessité de re-normalisation après chaque mise à jour.\n\\end{itemize}\n\n\n\t\\section{Lien de parenté avec d'autres réseaux}\n\tDans ce paragraphe, nous évoquons deux autres familles de réseaux dont les noms laissent entrevoir une certaine parenté avec ceux manipulés dans cette thèse :  les réseaux de neurones à fonctions à base radiale (RBF) et les réseaux de neurones de Clifford. \n\t\n\t\\subsection{Lien avec les réseaux de neurones à fonctions à bases de radiales}\n\t\n\tLes fonctions à base radiales (RBF) \\cite{buh2003} sont en particulier bien connues pour leur capacité à approcher des fonctions non linéaires. \n\tCe sont des fonctions dont la valeur dépend uniquement de la distance entre un point d'entrée et un centre fixe. L'approximation d'une fonction $f$ donnée par un réseau RBF avec $J$ fonctions radiales prend donc la forme~:\n\t\n\t\\begin{equation}\n\t\t\\vspace*{-0.1cm}\n\t\tf(x) \\approx \\sum_{j=1}^J w_j \\rho_j(||\\mathbf{x} - \\mathbf{c}_j||)\n\t\\end{equation}\n\tavec  $\\rho_j : \\mathbb{R}^+ \\mapsto \\mathbb{R}$. Un exemple classique est celui du noya",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 24,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_25",
      "text": "u gaussien, $\\rho_j(||\\mathbf{x} - \\mathbf{c}_j||) = e^{-\\lambda_j^2||\\mathbf{x} - \\mathbf{c}_j||^2}$ où $\\lambda_j$ est un paramètre d'échelle  s'apparentant au rayon d'une sphère. \n\tLe lien avec les réseaux de neurones à couches hypersphériques sera davantage détaillé dans la section \\ref{sec:lienrbf}, en particulier dans le cas $n=1$ où une formulation analytique des sorties d'un réseau à une couche hypersphérique permet de clairement discerner les différences ou les similarités avec celles d'un réseau RBF.\n\n\\subsection{Réseaux de neurones Clifford}\n\nL'algèbre géométrique conforme du modèle d'Hestenes \\cite{Hestenes2001} étant un cas particulier des algèbres géométriques, tous les travaux portant sur les réseaux de neurones dits de Clifford (initialement introduits par Buchholz et Sommer \\cite{articleBuchholz}, \\cite{Buchholz_these} pour enrichir les réseaux neuronaux avec des propriétés géométriques) peuvent naturellement être rapprochés (puis exploités) de ce qui est fait ici.\n\nLes neurones de Clifford étendent le modèle du perceptron classique en remplaçant les opérations réelles par des opérations dans l'algèbre de Clifford. Deux types de neurones Clifford sont généralement",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 25,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_26",
      "text": "utilisés~:\\\\\n\n\\begin{itemize}\n    \\item Neurone de Base Clifford (BCN) : Dans un neurone de Clifford, les entrées et les poids sont les composantes des multivecteurs appartenant à une algèbre de Clifford $C_{p,q}$. La combinaison linéaire classique est remplacée par le produit géométrique noté $\\otimes_{p,q}$ (avec $\\theta$ un biais). La fonction de propagation d'un neurone Clifford de base est définie par~:\n    \\begin{itemize}\n        \\item $f(\\mathbf{x}) = \\mathbf{w} \\otimes_{p,q} \\mathbf{x} + \\theta$ (multiplication à droite)\n        \\item $f(\\mathbf{x}) = \\mathbf{x} \\otimes_{p,q} \\mathbf{w} + \\theta$ (multiplication à gauche)\n    \\end{itemize}\n    où $\\mathbf{w}$, $\\mathbf{x}$, $\\theta \\in C_{p,q}$ sont des multivecteurs.\n\n    Comme dans notre cas, le produit scalaire usuel est remplacé par un produit géométrique. On notera que la structure est complexifiée par l'absence de commutativité.\\\\\n\n    \\item Neurone Spineur Clifford (SCN) : le produit géométrique est étendu de manière à inclure des transformations orthogonales (comme les rotations) appliquées aux entrées. Les spineurs, qui sont des éléments de Clifford agissant comme des opérateurs de rotation, sont utilisés pour tran",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 26,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_27",
      "text": "sformer les vecteurs d’entrée de manière contrôlée. La transformation de $\\mathbf{x}$ s'écrit alors~: $y= \\mathbf{w} \\otimes_{p,q} \\mathbf{x} \\otimes_{p,q} \\mathbf{\\phi(w)} + \\theta$, où $\\mathbf{\\phi(w)}$ peut représenter l'inversion, la réversion ou la conjugaison par un neurone spineur Clifford.\n\\end{itemize}\n\n\\medskip\n\nLes réseaux de neurones de Clifford sont construits en connectant plusieurs neurones de Clifford. On trouve dans la littérature trois catégories principales \\cite{articleBuchholz}~:\n\\begin{itemize}\n    \\item Clifford Multilayer Perceptron (CMLP) : utilise des BCN avec des fonctions d'activation à valeurs réelles.\n    \\item Spinor Clifford Multilayer Perceptron (SCMLP) : utilise des SCN avec des fonctions d'activation à valeurs réelles.\n    \\item Clifford MLP avec Fonctions d'Activation à Valeurs Clifford (FCMLP) : utilise des fonctions d'activation à valeurs dans une algèbre de Clifford.\n\\end{itemize}\n\n\\medskip\n\nL'apprentissage des neurones de Clifford s'effectue par une extension de la descente de gradient classique. Il faut noter que le produit de deux éléments non nuls d'une algèbre de Clifford peut être nul, ce qui induit quelques difficultés pour l'étape de",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 27,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_28",
      "text": "rétropropagation du gradient. On utilise donc des involutions (une fonction qui, appliquée deux fois à un multivecteur, retourne le multivecteur d'origine ; cela peut être l'inversion, la réversion ou la conjugaison). Par exemple, l'algorithme de rétropropagation pour les CMLP (Clifford Multilayer Perceptrons) utilise une involution unique déterminée par l'algèbre de Clifford sous-jacente. Les outils développés dans cette thèse ne présentent pas autant de complexité technique pour leur implémentation.\n\n\\section{Expérimentations}\n\nLes expérimentations qui suivent ont été mises en place pour valider les modèles de couches hypersphériques construits à partir d'un ensemble de neurones ou de filtres de convolution hypersphériques. Comme nous nous positionnons sur des briques élémentaires de bas niveau, nous avons volontairement choisi des architectures de réseau simples afin de limiter l'écueil du sur-apprentissage et des difficultés d'interprétation.\n\\\\\n\nNos expérimentations sont présentées en deux parties, sans puis avec convolution.\n\n\\subsection{Réseaux à couches hypersphériques {\\it vs} couches denses}\n\\subsubsection{Jeux de données}\nDeux jeux de données synthétiques ont été utilisé",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 28,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_29",
      "text": "s pour étudier la performance des réseaux à couches hypersphériques. \n\nLe premier nommé \\emph{Easy} est construit de façon à avoir 3 classes quasiment linéairement séparables. Celui-ci contient 350 points en dimension 2, ce qui donne de plus l'opportunité d'illustrer les résultats par différentes images.\\\\\nLe second nommé \\emph{Dif} est constitué d'une série de 450 points en dimension 2 répartis en 3 classes non linéairement séparables avec un taux de chevauchement non négligeable. Cette structure nous permettra de visualiser et de comparer la forme des frontières de décision pour différentes architectures.\\\\\n\nLes paramètres ayant servi à générer ces jeux de données sont donnés dans \\cite{saintjean:tel-00145895}. \nLes deux jeux de données ont été séparés en ensemble d'apprentissage (66.66\\%) et de validation (33.33\\%).\n\n\\subsubsection{Types de réseaux et entraînement}\nL'architecture de base que nous considérons est un perceptron multi-couches de faible profondeur. \nDans un premier cas, les couches du réseau (noté PMC) seront des couches denses classiques. Le réseau nommé GeoPMC ne contient quant à lui que des neurones hypersphériques.\n\nOn a évalué l'influence de différents paramètr",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 29,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_30",
      "text": "es :\n\\begin{itemize}\n\t\\item Le nombre de couches cachées varie de 0 à 2.\n\t\\item Le nombre de neurones par couche cachée prend les valeurs 2, 3 ou 10. \n\t\\item La batch normalisation et la fonction d'activation \\emph{ReLU} s'appliquent ou non sur toutes les couches cachées.\n\\end{itemize}\n\nChaque réseau se termine par une couche de 3 neurones classiques ou hypersphériques avant de passer par la fonction d'activation softmax car il s'agit d'un problème de classification ; celle-ci n'est jamais précédée d'un \\emph{ReLU}.\n\nPour faciliter la lecture des résultats, chaque architecture du réseau est codée par une chaîne de caractères : un chiffre correspond au nombre de neurones par couche, \"r\" à l'utilisation de la fonction d'activation relu, \"b\" à une batch normalisation et \"sf\" à la fonction d'activation softmax.\n\nLa taille du batch est de 30, l'algorithme d'optimisation est Adam \\cite{Kingma2014AdamAM}.\n\n\\subsubsection{Résultats}\nBien que l'ensemble des configurations ci-dessus ait été testé, seul un ensemble de configurations efficaces a été sélectionné pour ce manuscrit.\nLes figures \\ref{fig:acceasy} et \\ref{fig:accdif} reportent le taux de bonne prédiction (\\emph{accuracy}) sur les j",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 30,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_31",
      "text": "eux de données de validation au cours des itérations.\nPour des architectures PMC et GeoPMC identiques, on constate que l'\\emph{accuracy} augmente plus rapidement pour les modèles GeoPMC.\nL'échec du modèle GeoPMC \"b 2 r b 2 r b 3 sf\" s'explique probablement par son architecture qui contribue à l'inhibition de certains neurones (\\emph{dying-ReLU})\n\n.\\begin{figure}[H]\n\t\\centering\n\t\\begin{tikzpicture}\n\t\t\\begin{axis}[height=9cm,\n\t\t\twidth=9cm,\n\t\t\tgrid= major ,\n\t\t\txlabel = {Epoch} ,\n\t\t\tylabel = {Accuracy} ,\n\t\t\txmax=200,\n\t\t\tlegend entries={geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, },\n\t\t\tlegend style={at={(0.5,0)},anchor=south west,nodes={scale=0.6, transform shape}}],\n\t\t\t]\n\t\t\t\\addplot[red,mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/easy_geoPMC_3_accuracy.csv};\n\t\t\t\\addplot[black,mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/easy_geoPMC_10_3_relu_accuracy.csv};\n\t\t\t\\addplot[blue, mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/easy_geoPMC_2_2_3_bn_relu_accuracy.csv};\n\t\t\t\\addplot[red, mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/easy_PMC_3_accuracy.csv};\n\t\t\t\\addp",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 31,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_32",
      "text": "lot[black,mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/easy_PMC_10_3_relu_accuracy.csv};\n\t\t\t\\addplot[blue,mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/easy_PMC_2_2_3_bn_relu_accuracy.csv};\n\t\t\t\n\t\t\\end{axis}\n\t\\end{tikzpicture}\n\t\\caption{easy: \\emph{accuracy} pour les données de validation }\n\t\\label{fig:acceasy}\n\t\\vspace*{-0.5cm}\n\\end{figure}\n\n\\begin{figure}[H]\n\t\\vspace*{-0.15cm}\n\t\\centering\n\t\\begin{tikzpicture}\n\t\t\\begin{axis}[height=9cm,\n\t\t\twidth=9cm,\n\t\t\tgrid= major ,\n\t\t\txlabel = {Epoch} ,\n\t\t\tylabel = {Accuracy} ,\n\t\t\txmax=200,\n\t\t\tlegend entries={ geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, },\n\t\t\tlegend style={at={(0.5,0)},anchor=south west,nodes={scale=0.6, transform shape}}],\n\t\t\t]\n\t\t\t\\addplot[red,mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/dif_geoPMC_3_accuracy.csv};\n\t\t\t\\addplot[black,mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/dif_geoPMC_10_3_relu_accuracy.csv};\n\t\t\t\\addplot[blue, mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/dif_geoPMC_2_2_3_bn_relu_accuracy.csv};\n\t\t\t\\addplot[red, mark=none,dashed] table [x=Step, y=Value,col sep=",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 32,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_33",
      "text": "comma]\n\t\t\t{data/dif_PMC_3_accuracy.csv};\n\t\t\t\\addplot[black,mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/dif_PMC_10_3_relu_accuracy.csv};\n\t\t\t\\addplot[blue,mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t{data/dif_PMC_2_2_3_bn_relu_accuracy.csv};\n\t\t\t\n\t\t\\end{axis}\n\t\\end{tikzpicture}\n\t\\caption{\\emph{Dif}: \\emph{accuracy} pour les données de validation }\n\t\\label{fig:accdif}\n\t\\vspace*{-0.15cm}\n\\end{figure}\n\n\nPour aller plus loin, la table \\ref{table:acc} reporte l'\\emph{accuracy} en fin d'entraînement des 14 réseaux les plus performants. \nExcepté les trois premiers cas, l'ensemble des réseaux efficaces sur \\emph{Easy} le sont également sur \\emph{Dif}. \n\\begin{table}[H]\n\t\\center\n\t\\small\n\t\\begin{tabular}{|c|c|c|c|c|}\n\t\t\\hline\n\t\t\\multicolumn{1}{|c|}{}&\\multicolumn{2}{|c|}{\\emph{Easy}}&\\multicolumn{2}{|c|}{\\emph{Dif}}\\\\\n\t\t\\hline\n\t\t\\multicolumn{1}{|c|}{Architectures} & GeoPMC & PMC & GeoPMC & PMC\\\\\n\t\t\\hline\n\t\t10 r : 3 sf  & 92\\% & \\textbf{99\\%} & 94.6\\% & 92.6\\%\\\\\n\t\t\\hline\n\t\tb 2 r : b 2 r : b 3 sf  & 41\\% & 68\\% & 86\\% & \\textbf{94\\%}\\\\\n\t\t\\hline\n\t\t3 r : 3 sf & 67.2\\% & \\textbf{95\\%} & 93.3\\% & 92.6\\%\\\\\n\t\t\\hline\n\t\t3 sf & 96.5\\% &\\textbf{97.4\\%} & 93.3\\% & 92\\%\\\\",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 33,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_34",
      "text": "\\hline\n\t\tb 2 : b 2 : b 3 sf & \\textbf{98.3\\%} & \\textbf{98.3\\%} & 91.3\\% & 92\\%\\\\\n\t\t\\hline\n\t\t10 : 3 sf & \\textbf{99\\%} & 96\\% & 92\\% & 93.3\\%\\\\\n\t\t\\hline\n\t\t10 : 10 : 3 sf & \\textbf{99\\%} & 94\\% & 91.3\\% & \\textbf{93.3\\%}\\\\\n\t\t\\hline\n\t\t2 : 2 : 3 sf & \\textbf{98.3\\%} & 95.7\\% & 91.3\\% & 93.3\\%\\\\\n\t\t\\hline\n\t\t3 : 3 : 3 sf & \\textbf{98.3\\%} & 95.7\\% & 90\\% & \\textbf{93.3\\%}\\\\\n\t\t\\hline\n\t\tb 10 : b 3 sf & \\textbf{98.3}\\% & 97.4\\% & 93\\% & 91.3\\%\\\\\n\t\t\\hline\n\t\tb 10 r : b 3 sf & \\textbf{98.3\\%} & \\textbf{98.3\\%} & 92\\% & 92\\%\\\\\n\t\t\\hline\n\t\tb 3 r : b 3 sf & \\textbf{98.3\\%} & \\textbf{98.3\\%} & 92\\% & 91.6\\%\\\\\n\t\t\\hline\n\t\tb 3 sf & 97.4\\% & \\textbf{98.3\\%} & 92.4\\% & 92\\%\\\\\n\t\t\\hline\n\t\tb 2 : b 3 sf & \\textbf{98.3\\%} & \\textbf{98.3\\%} & 91.8\\% & 91.6\\%\\\\\n\t\t\\hline\n\t\\end{tabular}\n\t\\normalsize\n\t\\caption{Accuracy pour différentes architectures}\n\t\\label{table:acc}\n\\end{table}\n\n\t\n\tHormis l'architecture \"10 r : 3 sf\" qui s'avère être plus performante pour les réseaux à couches hypersphériques et le réseau \"b 2 r : b 2 r : b 3 sf\" qui lui semble être meilleur que le modèle classique sur les données de \\emph{Easy}, l'ensemble des résultats d'\\emph{accuracy} ne sont pas significativement différents. Étant donné",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 34,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_35",
      "text": "e la taille des jeux de données de validation, un écart de 1\\% ne représente en effet que 1 ou 2 points mal classés.\\\\\n\t\n\tAfin de mieux voir l'impact de la fonction d'activation \\emph{ReLU}  et de la batch normalisation, les tableaux \\ref{table:releasy} et \\ref{table:reldif} présentent les résultats de l'apprentissage par architecture. \\\\\n\t\n\t\\begin{table}[H]\n\t\t\\vspace*{-0.25cm}\n\t\t\\center\n\t\t\\small\n\t\t\\begin{tabular}{|c|c|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\multicolumn{5}{|c|}{GeoPMC}\\\\\n\t\t\t\\hline\n\t\t\tArchitectures & $\\emptyset$ & \\emph{ReLU} & bn & bn + \\emph{ReLU}\\\\\n\t\t\t\\hline\n\t\t\t10  : 3 sf  &\\textbf{99\\%} & 92\\% & 98.3\\% & 98.3\\%\\\\\n\t\t\t\\hline\n\t\t\t3 sf & 95\\% & ND & \\textbf{98.3\\%} & ND\\\\\n\t\t\t\\hline\n\t\t\t3  : 3 sf & 94\\% & 67.2\\% & \\textbf{98.3\\%} & \\textbf{98.3\\%}\\\\\n\t\t\t\\hline\n\t\t\t2 : 3 sf & 92.2\\% & 97.4\\% & \\textbf{98.3\\%} & \\textbf{98.3\\%}\\\\\n\t\t\t\\hline\n\t\t\t3 : 3 : 3 sf & 98.3\\% & \\textbf{99.1\\%} & 97.4\\% & 68.9\\%\\\\\n\t\t\t\\hline\n\t\t\t\\multicolumn{5}{|c|}{PMC}\\\\\n\t\t\t\\hline\n\t\t\tArchitectures & $\\emptyset$ & \\emph{ReLU} & bn & bn + \\emph{ReLU}\\\\\n\t\t\t\\hline\n\t\t\t10  : 3 sf  & 96\\% & 95.6\\% & 97\\% & \\textbf{98.3\\%}\\\\\n\t\t\t\\hline\n\t\t\t3 sf & 97.4\\% & ND & \\textbf{98.3\\%} & ND\\\\\n\t\t\t\\hline\n\t\t\t3  : 3 sf & 95.6\\% & 94.8\\% & \\tex",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 35,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_36",
      "text": "tbf{98.3\\%} & \\textbf{98.3\\%}\\\\\n\t\t\t\\hline\n\t\t\t2 : 3 sf & 96.5\\% & 94.8\\% & \\textbf{98.3\\%} & 97.4\\%\\\\\n\t\t\t\\hline\n\t\t\t3 : 3 : 3 sf & 95.7\\% & 94\\% & \\textbf{98.3\\%} & \\textbf{98.3\\%}\\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\t\\normalsize\n\t\t\\caption{\\emph{Easy} : Comparaison de l'impact de \\emph{ReLU} et de la batch normalisation}\n\t\t\\label{table:releasy}\n\t\\end{table}\n\t\n\t\\begin{table}[H]\n\t\t\\vspace*{-0.7cm}\n\t\t\\center\n\t\t\\small\n\t\t\\begin{tabular}{|c|c|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\multicolumn{5}{|c|}{GeoPMC}\\\\\n\t\t\t\\hline\n\t\t\tArchitectures &  $\\emptyset$ & \\emph{ReLU} & bn & bn + \\emph{ReLU}\\\\\n\t\t\t\\hline\n\t\t\t10  : 3 sf  & 92\\% & \\textbf{94.6\\%} & 93\\% & 92\\%\\\\\n\t\t\t\\hline\n\t\t\t3 sf & \\textbf{93.3\\%} & ND & 92.4\\% & ND\\\\\n\t\t\t\\hline\n\t\t\t3  : 3 sf & 92.6\\% & \\textbf{93.3\\%} & 91.9\\% & 92.6\\%\\\\\n\t\t\t\\hline\n\t\t\t2 : 3 sf & \\textbf{92.6\\%} & 89.6\\% & 92\\% & 92\\%\\\\\n\t\t\t\\hline\n\t\t\t3 : 3 : 3 sf & 90\\% & 86\\% & \\textbf{92.6\\%} & 90\\%\\\\\n\t\t\t\\hline\n\t\t\t\\multicolumn{5}{|c|}{PMC}\\\\\n\t\t\t\\hline\n\t\t\tArchitectures & $\\emptyset$ & \\emph{ReLU} & bn & bn + \\emph{ReLU}\\\\\n\t\t\t\\hline\n\t\t\t10  : 3 sf  & \\textbf{93.3\\%} & 92.6\\% & 91\\% & 92\\%\\\\\n\t\t\t\\hline\n\t\t\t3 sf & \\textbf{92\\%} & ND & \\textbf{92\\%} & ND\\\\\n\t\t\t\\hline\n\t\t\t3  : 3 sf & \\textbf{92.6\\%} & \\textbf{92.6\\",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 36,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_37",
      "text": "%} & 92\\% & 92\\%\\\\\n\t\t\t\\hline\n\t\t\t2 : 3 sf & 92.6\\% & 92.6\\% & 92\\% & \\textbf{93\\%}\\\\\n\t\t\t\\hline\n\t\t\t3 : 3 : 3 sf & \\textbf{93.3\\%} & 48\\% & 92.6\\% & 92.2\\%\\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\t\\normalsize\n\t\t\\caption{\\emph{Dif} : Comparaison de l'impact de \\emph{ReLU} et de la batch normalisation}\n\t\t\\label{table:reldif} \n\t\\end{table}\n\t\n\tDe manière générale, la présence du \\emph{ReLU} semble dégrader davantage la qualité des prédictions sur les réseaux à couches hypersphériques, d'autant plus si le nombre de couches cachées croît. En conséquence, le \\emph{ReLU} ne semble pas être la fonction d'activation la plus adaptée sachant que $\\tilde{s}._i\\tilde{x}$ peut être un scalaire très négatif (cf. figure \\ref{fig:ps}). Une étude supplémentaire s'avère nécessaire pour en trouver une plus appropriée\\footnote{Cette question justifie le travail sur les fonctions d'activation qui fait partie du chapitre suivant de la thèse.}. \n\t\n\tEn ce qui concerne la batch normalisation, elle permet dans le cas des données de \\emph{Easy}, d'améliorer considérablement les résultats. Son utilisation semble moins pertinente pour les données issues du fichier \\emph{Dif}.\\\\\n\t\n\tEn regardant maintenant les valeurs de la fon",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 37,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_38",
      "text": "ction de perte (\\emph{loss}) sur le jeu de données de validation (figure \\ref{fig:easyloss} et \\ref{fig:difloss}), il est possible d'observer que les modèles GeoPMC convergent plus vite vers des valeurs plus petites.\n\t\n\t\\begin{figure}[H]\n\t\t\\centering\n\t\t\\begin{tikzpicture}\n\t\t\t\\begin{axis}[height=9cm,\n\t\t\t\twidth=14cm,\n\t\t\t\tgrid= major ,\n\t\t\t\txlabel = {Epoch} ,\n\t\t\t\tylabel = {Loss} ,\n\t\t\t\txmax=150,\n\t\t\t\tlegend entries={geoPMC : 3 sf, geoPMC : 10 r 3 sf,  geoPMC : b 2 r b 2 r b 3 sf,  PMC : 3 sf, PMC : 10 r 3 sf,  PMC : b 2 r b 2 r b 3 sf,  },\n\t\t\t\tlegend style={at={(0.5,0.5)},anchor=south west,nodes={scale=0.6, transform shape}}],\n\t\t\t\t]\n\t\t\t\t\\addplot[red,mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/easy_geoPMC_3_loss.csv};\n\t\t\t\t\\addplot[blue,mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/easy_geoPMC_10_3_relu_loss.csv};\n\t\t\t\t\\addplot[black, mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/easy_geoPMC_2_2_3_bn_relu_loss.csv};\n\t\t\t\t\\addplot[red, mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/easy_PMC_3_loss.csv};\n\t\t\t\t\\addplot[blue,mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/easy_PMC_10_3_relu_loss.csv};\n\t\t\t\t\\addplot[black",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 38,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_39",
      "text": ",mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/easy_PMC_2_2_3_bn_relu_loss.csv};\n\t\t\t\t\n\t\t\t\t{font=\\tiny} \t\t\\end{axis}\n\t\t\\end{tikzpicture}\n\t\t\\caption{\\emph{Easy}: Courbes des fonctions de pertes }\n\t\t\\label{fig:easyloss}\n\t\\end{figure}\n\t\n\t\\begin{figure}[H]\n\t\t\\vspace*{-0.5cm}\n\t\t\\centering\n\t\t\\begin{tikzpicture}\n\t\t\t\\begin{axis}[height=9cm,\n\t\t\t\twidth=14cm,\n\t\t\t\tgrid= major ,\n\t\t\t\txlabel = {Epoch} ,\n\t\t\t\tylabel = {Loss } ,\n\t\t\t\txmax=150,\n\t\t\t\tlegend entries={ geoPMC : 3 sf, geoPMC : 10 r 3 sf,  geoPMC : b 2 r b 2 r b 3 sf,  PMC : 3 sf, PMC : 10 r 3 sf,  PMC : b 2 r b 2 r b 3 sf,  },\n\t\t\t\tlegend style={at={(0.5,0.5)},anchor=south west,nodes={scale=0.6, transform shape}}],\n\t\t\t\t]\n\t\t\t\t\\addplot[red,mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/dif_geoPMC_3_loss.csv};\n\t\t\t\t\\addplot[blue,mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/dif_geoPMC_10_3_relu_loss.csv};\n\t\t\t\t\\addplot[black, mark=none] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/dif_geoPMC_2_2_3_bn_relu_loss.csv};\n\t\t\t\t\\addplot[red, mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/dif_PMC_3_loss.csv};\n\t\t\t\t\\addplot[blue,mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{da",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 39,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_40",
      "text": "ta/dif_PMC_10_3_relu_loss.csv};\n\t\t\t\t\\addplot[,black,mark=none,dashed] table [x=Step, y=Value,col sep=comma]\n\t\t\t\t{data/dif_PMC_2_2_3_bn_relu_loss.csv};\n\t\t\t\t\n\t\t\t\\end{axis}\n\t\t\\end{tikzpicture}\n\t\t\\caption{\\emph{Dif}: Courbes des fonctions de pertes}\n\t\t\\label{fig:difloss}\n\t\\end{figure}\n\t\n\t\t\n\t\n\\subsection{Illustration géométrique des résultats}\n\\label{sec:preimage}\nEn considérant $\\Omega \\subset \\mathbb{R}^2$ comme le domaine initial sur lequel le modèle est évalué, la pré-image est définie comme un sous-ensemble $\\Omega$ associé à un ensemble d'éléments de sortie définis par $\\displaystyle \\tilde{s_j} ._i \\tilde{\\Phi}(\\mathbf{x})$. En d'autres termes, pour une hypersphère $\\tilde{s_j}$, la pré-image est définie par :\n\\[ \\{ \\mathbf{x} \\in \\Omega \\subset \\mathbb{R}^2 \\mid \\tilde{s_j} ._i \\tilde{\\Phi}(\\mathbf{x}) = y \\} \\]\noù $y \\in \\mathbb{R}$ est une constante. Dans ce contexte, les courbes de niveau du produit $\\displaystyle \\tilde{s_j} ._i \\tilde{\\Phi}(x_k)$, où $\\tilde{s_j}$ est un vecteur associé à une hypersphère et $\\tilde{\\Phi}(\\mathbf{x_k})$ est la représentation de l'exemple $\\mathbf{x_k}$ dans un espace transformé (tout le réseau hormis la dernière couche), peuvent être interpr",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 40,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_41",
      "text": "étées comme des pré-images. Les lignes correspondant aux frontières de décision correspondent à l'ensemble des points où le produit $\\displaystyle \\tilde{s_j} ._i \\tilde{\\Phi}(\\mathbf{x_k})$ est nul.\n\nComme les expérimentations sont effectuées sur des données 2D, il est possible de visualiser la pré-image d'un neurone hypersphérique paramétré par une hypersphère $s$ en affichant \nles bandes de niveau pour les valeurs positives de $\\displaystyle \\tilde{\\Phi (x)}._i \\tilde{s}$ (figure \\ref{fig:3sfband} et \\ref{fig:b3rpre}).\n%Pour chaque point $\\mathbf{x}=(x_1, x_2)$ de la grille d'affichage, on infère l'entrée $\\Phi (x)$ de la couche à afficher pour $\\mathbf{x}$ et on calcule la valeur de $\\displaystyle \\Phi (x)._i s$. \n\n\n\\begin{figure}[H]\n\t\\begin{subfigure}{.46\\linewidth}\n\t\t\\begin{center}\n\t\t\t\\includegraphics[height=5cm]{images/easy_geomPMC_3_sphere.png}\n\t\t\t\\caption{$\\tilde{s}._i\\tilde{x}=0$}\n\t\t\t\\label{fig:3sf}\n\t\t\\end{center}\n\t\\end{subfigure} \\hfill\n\t\\begin{subfigure}{.46\\linewidth}\n\t\t\\begin{center}\n\t\t\t\\includegraphics[height=5cm]{images/easy_geomPMC_3_frdec.png}\n\t\t\t\\caption{\\mbox{$x .s > 0$}}\n\t\t\t\\label{fig:3sfband}\n\t\t\\end{center}\n\t\\end{subfigure}\n\t\\caption{\\emph{Easy} : \\mbox{Bandes",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 41,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_42",
      "text": "de niveaux pour geomPMC \"3 sf\"}}\n\\end{figure}\t\n\nSi l'hypersphère à afficher est située dans la couche 0 ({\\it i.e.} pas de non-linéarité) (cf. figure \\ref{fig:b3r}) ou qu'il n'existe pas de couches cachées (voir figure \\ref{fig:3sf}), les bandes de niveau sont circulaires concentriques. \\\\\n\n\n\\begin{figure}[H]\n\t\\begin{subfigure}{.45\\linewidth}\n\t\t\\begin{center}\n\t\t\t\\includegraphics[height=5cm]{images/dif_geomPMC_3_3_bn_relu_sphere.png}\n\t\t\t\\caption{Sphères de la couche 0}\n\t\t\t\\label{fig:b3r}\n\t\t\\end{center}\n\t\\end{subfigure} \\hfill\n\t\\begin{subfigure}{.45\\linewidth}\n\t\t\\begin{center}\n\t\t\t\\includegraphics[height=5cm]{images/dif_geomPMC_3_3_bn_relu_pi.png}\n\t\t\t\\caption{Pré-images de la couche 2}\n\t\t\t\\label{fig:b3rpre}\n\t\t\\end{center}\n\t\\end{subfigure}\n\t\\caption{\\emph{Dif} : Bandes de niveaux pour\\\\\n\t\tgeomPMC \"b 3 r : b 3 r : b 3 sf\"}\n\n\\end{figure}\n\nLa figure \\ref{fig:comp1} permet d'illustrer la proximité entre les pré-images des hypersphères (situées avant la couche softmax) et les frontières de décision. Cela peut paraître évident au premier abord, mais si l'on se réfère à la figure \\ref{fig:3sfband}, on sait que la frontière de décision entre deux hypersphères, en l'absence de non-linéarité, e",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 42,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_43",
      "text": "st un hyperplan...\nIl convient également de noter que les centres des hypersphères (colonne de gauche en jaune) diffèrent des centres des classes (colonne de droite).\n\n\\begin{figure}[h]\n\t\\begin{subfigure}{\\linewidth}\n\t\t\\includegraphics[width=8cm]{figs/easy_geomPMC_3_3_3_relu_pi.png}\n\t\t\\includegraphics[width=8cm]{figs/easy_geomPMC_3_3_3_relu_frdec.png}\n\t\t\\caption{\\emph{Easy} : geoPMC \"3 r :  3 r : 3 sf\"}\n\t\\end{subfigure}\n\t\\hfill\n\t\\begin{subfigure}{\\linewidth}\n\t\t\\begin{center}\n\t\t\t\\includegraphics[width=8cm]{figs/dif_geomPMC_10_3_3_pi.png}\\hfill\n\t\t\t\\includegraphics[width=8cm]{figs/dif_geomPMC_10_3_3_relu_frdec.png}\n\t\t\\end{center}\n\t\t\\caption{\\emph{Dif} : geoPMC \"10 r :  3 r : 3 sf\"}\n\t\\end{subfigure}\n\t\n\t\\caption{Pré-images couche 2 (gauche) et frontières de décision (à droite)}\n\t\\label{fig:comp1} \n\\end{figure}\n\n\nLa différence entre deux modèles GeoPMC et PMC de même architecture est flagrante si l'on compare leurs frontières de décision (cf. figure \\ref{compfront}). Si l'on regarde leur forme, PMC correspond à une succession de lignes brisées, alors que GeoPMC produit une succession d'arcs de cercle.\n\n\\begin{figure}[h]\n\t\\begin{subfigure}{.45\\linewidth}\n\t\t\\begin{center}\n\t\t\t\\includegraphi",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 43,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_44",
      "text": "cs[width=7cm]{figs/dif_PMC_3_3_relu_frdec.png}\n\t\t\t\\caption{PMC \"3 r :  3 r : 3 sf\"}\n\t\t\\end{center}\n\t\\end{subfigure} \n\t\\begin{subfigure}{.45\\linewidth}\n\t\t\\begin{center}\n\t\t\t\\includegraphics[width=7cm]{figs/dif_geomPMC_3_3_relu_frdec.png}\n\t\t\t\\caption{GeoPMC \"3 r :  3 r : 3 sf\"}\n\t\t\\end{center}\n\t\\end{subfigure}\n\t\\caption{\\emph{Dif} : Comparaison des frontières de décision}\n\t\\label{compfront}\n\\end{figure}\n\n\\subsection{Réseaux à couches convolutives {\\it vs} convolutions hypersphériques}\n\nCette section propose une comparaison entre les filtres hypersphériques et Conv2d sur le jeu de données MNIST.\nIl s'agit d'une base de 70000 mini-images étiquetées, en niveaux de gris, de taille $28 \\times 28$ représentant l'écriture manuscrite des chiffres de 0 à 9. C'est donc un problème de reconnaissance à 10 classes.\\\\\n\nDans cette expérimentation, une architecture est testée avec la séquence de couches suivante~:\n\\begin{enumerate}\n\t\\item  une batch normalization ou non\n\t\\item  une couche hypersphérique pour GeoPMC / une couche Dense pour PMC / une couche à filtres hypersphériques pour GeoConv2d /  une couche Conv2d pour Conv2d avec un certain nombre de filtres de taille $ 3 \\times 3$\n\t\\item  une fonc",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 44,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_45",
      "text": "tion \\emph{ReLU} ou non\n\t\\item  une batch normalization ou non\n\t\\item  une couche Dense ou hypersphérique avec un certain nombre de neurones\n\t\\item  une activation softmax\n\\end{enumerate}\nLe tableau \\ref{table:MNIST} donne les résultats d'\\emph{accuracy} pour l'ensemble des configurations testées.\n\n\\begin{table}[htbp]\n\t\\vspace*{-0.15cm}\n\t\\center\n\t\\small\n\t\\begin{tabular}{|c|c|c|c|c|}\n\t\t\\hline\n\t\t\\multicolumn{1}{|c|}{}&\\multicolumn{2}{|c|}{Dense}&\\multicolumn{2}{|c|}{Convolution}\\\\\n\t\t\\hline\n\t\t\\multicolumn{1}{|c|}{Architectures} & GeoPMC & PMC & GeoConv2d & Conv2d\\\\\n\t\t\\hline\n\t\t\\hline\n\t\t10 sf & \\textbf{92.7\\%} & \\textbf{92.7\\%} & \\multicolumn{2}{|c|}{ND} \\\\\n\t\t\\hline\n\t\t\\hline\n\t\t32 : 10 sf & 93\\% & 92.6\\% & \\textbf{97.5\\%} & \\textbf{97.5\\%}\\\\\n\t\t\\hline\n\t\t32 r : 10 sf & 10\\% & 96.8\\% & 98\\% & \\textbf{98.2\\%}\\\\\n\t\t\\hline\n\t\tb 32 : b 10 sf & 86.4\\% & 91.6\\% & \\textbf{97.2\\%} &\\textbf{ 97.2\\%}\\\\\n\t\t\\hline\n\t\tb 32 r : b 10 sf & 10\\% & 96\\% & 97.8\\% & \\textbf{98.2\\%}\\\\\n\t\t\\hline\n\t\t\\hline\n\t\t512 : 10 sf & 93\\% & 93\\% & \\textbf{97\\%} &\\textbf{ 97\\%}\\\\\n\t\t\\hline\n\t\t512 r : 10 sf & 10\\% & \\textbf{98\\%} & \\textbf{98\\%} & \\textbf{98\\%}\\\\\n\t\t\\hline\n\t\tb 512 : b 10 sf & 93\\% & 93\\% &\\textbf{ 97.7\\%} & 97.1\\%\\\\",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 45,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_46",
      "text": "\\hline\n\t\tb 512 r : b 10 sf & 10\\% & 98\\% & 97.7\\% & \\textbf{98.3\\%}\\\\\n\t\t\\hline\n\t\\end{tabular}\n\t\\normalsize\n\t\\caption{MNIST : Comparaison de l'\\emph{accuracy} pour différents modèles}\n\t\\label{table:MNIST}\n\\end{table}\n\n\nUne première observation concerne l’incapacité de GeoPMC à fonctionner en \"grande\" dimension avec une activation \\emph{ReLU}. Le taux d'erreur de 90\\% laisse à penser que le classifieur prédit toujours la même classe. Le problème provient, selon nous, d'une mauvaise initialisation des hypersphères qui fait que tous les points sont à l'extérieur de toutes les sphères\\footnote{Ce constat justifie le travail d'initialisation fait au paragraphe suivant.}. Par la suite, le \\emph{ReLU} annulera tout signal vers les couches suivantes du réseau. Ce problème ne se produit certainement pas pour GeoConv2d\ncar les hypersphères sont définies en dimension 9 au lieu de 784. Plus généralement, il serait pertinent de se demander si les couches hypersphériques sont sensibles à la malédiction de la dimension.\n\nOn constate la même amélioration lors du passage des modèles GeoPMC vers GeoConv2d que celui de PMC vers Conv2d. D’après nos premières expériences, GeoConv2d\net Conv2d produisent",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 46,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_47",
      "text": "des résultats identiques.\n\n\\section{Une initialisation appropriée}\n\\label{sec:pbinitdistrib}\n\nL'initialisation des paramètres dans une couche comportant plusieurs neurones hypersphériques représente un défi complexe, mais qu'il fallait relever au vu des expérimentations menées précédemment\\footnote{Il nous faut signaler ici que les résultats présentés dans ce paragraphe ont été obtenus en fin de thèse.}.\n\nPour se convaincre encore de l'importance de la question de l'initialisation, on peut aussi évoquer dans ce préambule le comportement du volume d'une hypersphère en grande dimension.\n\tOn rappelle en effet que le volume d'une hypersphère en fonction de la dimension $n$ pour un rayon $\\rho$ fixé est le suivant~:\n\t\n\t$$V(\\rho) = \\frac{\\pi^{\\frac{1}{2} n} \\rho^n}{\\Gamma\\left(\\frac{1}{2} n + 1\\right)} .$$\n\tOn peut constater dans la figure \\ref{fig:vol_hs} que le volume d'une hypersphère tend rapidement vers zéro lorsque la dimension $n$ de l'espace des données augmente. Pour un rayon $\\rho$ proche de 1, le volume devient quasiment nul dès que la dimension atteint 40. Cela devient problématique pour les données de dimensions encore plus élevées et peut donc compliquer les étapes d'initia",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 47,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_48",
      "text": "lisation. En effet, comme le volume décroît fortement avec l'augmentation de la dimension, à la limite, tout point de l'espace se situe en dehors des hypersphères comme l'on peut le voir sur la figure \\ref{fig:prod_fct_dim} (le produit $\\tilde{s} ._i \\tilde{x}$ décroît linéairement vers des valeurs négatives au fur et à mesure que la dimension augmente). \n\t\n\t\\begin{figure}[H]\n\t\t\\includegraphics[keepaspectratio,width=16cm]{figs/volume_hs.pdf}\n\t\t\\caption{Volume d'une hypersphère en fonction de la dimension de l'espace des données (pour des rayons fixés : ici 0.8, 1 et 1.2)}\n\t\t\\label{fig:vol_hs}\n\t\\end{figure}\n\t\n\t\\begin{figure}[H]\n\t\t\\includegraphics[keepaspectratio,width=16cm]{figs/sx_dim_vol.pdf}\n\t\t\\caption{Produit $\\tilde{s} ._i \\tilde{x}$ en fonction de la dimension de l'espace des données}\n\t\t\\label{fig:prod_fct_dim}\n\t\\end{figure}\n\t\n\n\nLe paragraphe est organisé de la façon suivante. \\\\\n\nNous commençons au paragraphe \\ref{etat} par une présentation des méthodes aujourd'hui les plus utilisées pour l'initialisation des réseaux à couches denses. \nC'est en effet de ces méthodes que nous avons décidé de nous inspirer, en particulier de l'approche heuristique de Glorot et Bengio \\cite{glor",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 48,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_49",
      "text": "ot}. \n\nD'autres stratégies d'initialisation auraient pu être envisageables. Par exemple, à l'instar des réseaux utilisant des fonctions radiales, il est possible de déterminer les centres à l'aide d'un algorithme de clustering et d'ajuster les rayons en fonction de la dispersion des données autour de ces centres. Cependant, ce type d'approche souffre de deux défauts. D'abord, il est crucial de ne pas confondre les centres des hypersphères avec ceux des clusters. Ensuite, et surtout, une telle stratégie qui sépare les centres et les rayons des sphères et en plus les traite avec des outils totalement différents, va à l'encontre de la philosophie de cette thèse, basée sur des outils d'algèbre géométrique et un apprentissage le plus direct possible.\\\\\n\nNous choisissons donc une stratégie de normalisation à la Glorot. \nPourtant, et c'est l'objet du paragraphe \\ref{commedhab} suivant, nous montrons rapidement que la méthode usuelle n'est pas du tout appropriée. Nous en illustrons les conséquences. Nous identifions également la cause : une distribution normale passée dans un neurone hypersphérique ne suit plus une loi normale.\n\nLe paragraphe \\ref{var} présente les calculs de variance qui",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 49,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_50",
      "text": "justifient la pertinence de la méthode d'initialisation que nous allons proposer. La loi de probabilité adaptée à la structure des neurones hypersphériques est la loi Gamma Généralisée. Une difficulté est que sa complexité permet très peu de calculs explicites et que ses propriétés sont mal connues. Nous la contournons en utilisant quelques propriétés asymptotiques des lois Gamma Généralisées.\n\nLe paragraphe \\ref{var} est à la fois très calculatoire et très probabiliste. \nLe lecteur peut passer les détails et aller directement en Section \\ref{expe_init} pour une version synthétique des règles d’initialisation obtenues et pour des expérimentations qui permettent à la fois de confirmer la pertinence de ces règles et d’illustrer les arguments mathématiques sous-jacents.  \nUn test de la méthode d'initialisation proposée est présenté au paragraphe \\ref{expe_real}, sur des données synthétiques suivant une distribution normale, composées de points en dimension 4.\n\n\\subsection{Initialisation {\\it à la Glorot}}\\label{etat}\n\nL'initialisation des réseaux de neurones est une question qui a été largement explorée.\nDans ce manuscrit, nous ne présentons pas d'état de l'art mais faisons un focus s",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 50,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_51",
      "text": "ur un type d'approche, dont des exemples emblématiques sont les travaux de Glorot \\cite{glorot} (adaptés aux fonctions d'activation type tangente hyperbolique) ou de He \\cite{KaimingHe} (adaptés aux fonctions d'activation \\texttt{\\emph{ReLU}}).\nComme déjà mentionné, nous pensons en effet ces stratégies plus adaptables aux réseaux hypersphériques. \nPour une revue de l'état de l'art, nous mentionnons par exemple les articles \\cite{DBLPinit}, \\cite{DEPATER2023579}, ainsi que \\cite{LIHuimin}.\\\\\n\nPar initialisation {\\it à la Glorot}, nous désignons des stratégies consistant à une forme de normalisation (voir aussi LeCun et al. \\cite{LeCun2012}) des points du réseau sensée garantir la propagation d'un signal à travers les couches profondes, en évitant qu'il ne se déforme trop. Typiquement on ne veut pas que l'amplitude du signal explose ou au contraire tende vers zéro.\nSimultanément l'approche vise à maintenir les poids dans une plage de valeurs raisonnable.\n\n\nEn pratique, si la distribution des éléments $x_i$, $i \\in \\{1, \\cdots, n\\}$, en entrée du réseau suit une loi normale, on choisit la distribution des poids du réseau de telle sorte que les éléments $y_j$, $j \\in \\{1, \\cdots, m\\}$,",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 51,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_52",
      "text": "en sortie du réseau suivent la même loi normale (même moyenne, même variance).\nPour que les calculs soient faisables, on suppose que les poids du réseau suivent également une loi normale.\\\\\n\nNous allons détailler les approches de Glorot \\cite{glorot} et He \\cite{KaimingHe} dans les lignes qui suivent.\nL'objectif est de comprendre ces méthodes dans le contexte classique des réseaux de neurones avant de les adapter au cas plus complexe des neurones hypersphériques. \n\n\n\\subsubsection{Glorot et He}\n\nOn se place donc ici dans le cas `classique'' où la sortie d'un neurone de type dense est donnée par la forme linéaire suivante~: \n\n\\begin{equation}\n\t\\displaystyle y_j = \\sum_{i=1}^{n} w_{ji} x_i\n\t\\label{eq:sortie_classique_he}\n\\end{equation}\nles $w_{ji}$ désignant les poids du réseau dont l'initialisation doit être choisie.\n\n\\bigskip\n\n\\begin{itemize}\n    \\item[$\\bullet$] On suppose que $x_i$, $1\\le i \\le n$, sont des variables aléatoires indépendantes et identiquement distribuées, suivant la loi normale de moyenne $\\mu_x = 0$ et d'écart-type $\\sigma_x$.\n    \\item[$\\bullet$] On veut que les sorties $y_j$, $1\\le i \\le m$, suivent encore loi normale de moyenne $\\mu_x = 0$ et d'écart-type $\\s",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 52,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_53",
      "text": "igma_x$.\n\\end{itemize}\n\nOn fait le pari qu'on va arriver à nos fins en faisant des hypothèses relativement simple sur la structuration des poids :\n\\begin{itemize}\n    \\item[$\\bullet$] On suppose que $w_{ji}$, $1\\le j \\le m$, $1\\le i\\le n$, sont des variables aléatoires indépendantes et identiquement distribuées, suivant la loi normale de moyenne $\\mu_w = 0$ et d'écart-type $\\sigma_w$.\n\\end{itemize}\n\n\\bigskip\nOn va maintenant calculer la variance des $y_j$, $1\\le i \\le m$, définis par \\eqref{eq:sortie_classique_he} et jouer sur le seul paramètre libre du problème, c'est-à-dire $\\sigma_w$ pour arriver à nos fins.\n\n\\bigskip\n\n\\noindent{\\bf Remarque.} \nPour la suite, au vu de la formule \\eqref{eq:sortie_classique_he}, on a besoin de calculer en particulier la variance de produits de variables aléatoires indépendantes. On rappelle donc que si  $X$ et $Y$ sont des variables aléatoires indépendantes, alors\\footnote{dans la suite var désigne toujours la variance et $E$ l'espérance}\n\\begin{equation}\n\t\\text{var}(XY) = \\mathbb{E}^2(X)  \\text{var}(Y) + \\mathbb{E}^2(Y)  \\text{var}(X) + \\text{var}(X)  \\text{var}(Y).\n\t\\label{eq:varianceformuleindependant}\n\\end{equation}\nEn effet, on a\n\\begin{eqnar",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 53,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_54",
      "text": "ray*}\n    \\text{var}(XY)  &=& \\mathbb{E}(X^2Y^2)-\\mathbb{E}^2(XY)\n\t = \\underbrace{\\text{cov}(X^2,Y^2)}_{0  \\text{  car X, Y indépendant. }}+\\mathbb{E}(X^2)\\mathbb{E}(Y^2)-\\mathbb{E}^2(XY)\n\t \\\\\n\t &=& \\left[\\text{var}(X)+\\mathbb{E}^2(X)\\right] \\left[\\text{var}(Y)+\\mathbb{E}^2(Y)\\right] - \\mathbb{E}^2(XY)\n\t\\\\\n\t &=& \\text{var}(X) \\text{var}(Y)+ \\mathbb{E}^2(X) \\text{var}(Y)+\\mathbb{E}^2(Y) \\text{var}(X) + \\underbrace{\\mathbb{E}^2(X) \\mathbb{E}^2(Y)-\\mathbb{E}^2(XY)}_{0 \\text{  car X, Y sont indépendant. }}\n\\end{eqnarray*}\\\\\n\n\\newpage\n\\noindent \\underline{Premier cas : sans fonction d'activation.}\\\\ \n\n\nLa sortie d'un neurone est exactement donnée par \\eqref{eq:sortie_classique_he}.\nOn a donc \n$$\\displaystyle \\text{var}\\left(y_j\\right)= \\text{var}\\left(\\sum_{i=1}^{n}w_{ji} x_i\\right)=\\sum_{i=1}^{n}\\text{var}\\left(w_{ji} x_i\\right) $$\nEn appliquant la formule  établie dans la remarque précédente, on obtient\n\\begin{equation*}\n\t\\operatorname{var}(y_j) = \\sum_{i=1}^{n} \\operatorname{var}(w_{ji})  \\operatorname{var}(x_i)\n\\end{equation*}\nLes distributions étant identiques, on arrive finalement à  : \n$$\\displaystyle \\text{var}\\left(y_j\\right)=n \\sigma_w^2 \\sigma_x^2.$$\n\nPuisqu'on veut $\\text{va",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 54,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_55",
      "text": "r}\\left(y_j\\right) = \\sigma_x^2$, on conclut que le résultat est obtenu si\n$$ n \\sigma_w^2 = 1.$$\n\nLe travail n'est pas terminé. En effet pour l'apprentissage il faut aussi envisager l'étape de rétro-propagation.\n\nNotons $L$ la fonction de perte. On veut maintenant que\n$$ \\text{var}\\Bigl( \\frac{\\partial L}{\\partial x } \\Bigr) = \\text{var} \\Bigl(  \\frac{\\partial L}{\\partial y } \\Bigr).$$\nIntuitivement, les gradients doivent conserver une certaine homogénéité au fil des passages entre couches.\n\nOr $\\displaystyle \\frac{\\partial L}{\\partial x } = \\frac{\\partial L}{\\partial y } \\frac{\\partial y}{\\partial x }$ (dérivation en chaîne) où la différentielle $\\displaystyle\\frac{\\partial y}{\\partial x }$ se calcule facilement grâce à la formule \\eqref{eq:sortie_classique_he} définissant $y$ comme une  fonction  linéaire de $x$. On a :\n\\begin{equation}\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^m   w_{ji} \\frac{\\partial L}{\\partial y_j}.\n\\label{eq:sortie_classique_backprop}\n\\end{equation} \n\nOn remarque facilement la similitude des définitions \\eqref{eq:sortie_classique_he} et \\eqref{eq:sortie_classique_backprop}. Comme la moyenne des $w_{ji}$ est nulle ($\\mu_w=0$), les moyennes des autres ter",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 55,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_56",
      "text": "mes dans \\eqref{eq:sortie_classique_backprop} le sont aussi. Pour le calcul des variances, seul le nombre de termes dans la somme change. On arrive donc à la condition\n$$ m \\sigma_w^2 = 1.$$\n\nSi $m\\ne n$, il faut finalement faire une hypothèse sur $\\sigma_w$ qui permette de faire un compromis entre les conditions $ n \\sigma_w^2 = 1$ et $ m \\sigma_w^2 = 1$.\nOn aboutit ainsi à la condition sur l'initialisation des poids suivante :\n\n\\begin{equation}\n\t\\displaystyle \\sigma_w^2=\\frac{2}{n+m} .\n\t\\label{eq:regle_glorot}\n\\end{equation}\\\\\n\n\\noindent{\\bf Remarque.} On aurait pu aussi supposer que la distribution des poids suit une loi uniforme sur $\\left[-r_w,r_w\\right]$. On a alors var$\\left(w_j\\right)=r_w^2/3$, si bien que la condition ci-dessus devient\n$r_w = \\sqrt{6/(n+m)}$. \n\n\n\\bigskip\n\\newpage\n\\noindent \\underline{Deuxième cas : avec fonction d'activation.}\\\\ \n\nDans ce cas, l'équation \\eqref{eq:sortie_classique_he} est remplacée par\n$$ y_j = f\\Bigl( \\sum_{i=1}^{n} w_{ji} x_i \\Bigr)$$\noù $f$ désigne la fonction d'activation.\nLes calculs de variances sont alors bien sûr plus techniques. \n\nNéanmoins, si la fonction $f$ est régulière, on peut l'approcher par une fonction linéaire {\\it via}",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 56,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_57",
      "text": "son développement limité en 0 (puisque les entrées sont supposées de moyenne nulle). Les calculs ci-dessus ne sont donc que très peu modifiés. On peut le faire par exemple si la fonction d'activation $f$  est une fonction logistique ou une tangente hyperbolique : au voisinage de 0,  $\\tanh{x}\\approx x$ (résultat ci-dessus inchangé), $1/1+e^{-x}\\approx 1/2+x/4$ (résultat ci-dessus inchangé à la constante près). \n\nSi la fonction $f$ n'est pas régulière, mais si elle suffisamment simple pour calculer les intégrales correspondant à la définition des variances, on peut également adapter les calculs précédents : c'est le cas par exemple de la fonction \\emph{ReLU}.\n\nLes résultats sont résumés ci-dessous.\n\t\n\n\n\\bigskip\n\n\n\\noindent \\underline{Synthèse}\\\\\n\nLe tableau  \\ref{tab:bilan_init_formule} présente une synthèse des résultats concernant les écarts types et les bornes de l'intervalle considéré, en fonction de l'utilisation de la loi normale ou uniforme pour l'initialisation des poids.\\\\\n\n\\begin{table}[h!]\n\\centering\n\t\\begin{tabular}{|c|c|c|}\n\t\t\\hline\n\t\tactivation & Loi normale & Loi uniforme \\\\ \n\t\t\\hline\n\t\t- & $\\sigma_w = \\sqrt{\\frac{2}{n + m}}$ (Glorot) & $r_w = \\sqrt{\\frac{6}{n + m}}$",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 57,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_58",
      "text": "(Glorot) \\\\ \n\t\t\\hline\n\t\t\\emph{Tanh} & $\\sigma_w = \\sqrt{\\frac{2}{n + m}}$ (Glorot) & $r_w = \\sqrt{\\frac{6}{n + m}}$ (Glorot) \\\\ \n\t\t\\hline\n\t\t\\emph{Sigmoïde} & $\\sigma_w = 4  \\sqrt{\\frac{2}{n + m}}$ (Glorot) & $r_w = 4  \\sqrt{\\frac{6}{n + m}}$ (Glorot) \\\\ \n\t\t\\hline\n\t\t\\emph{ReLU} & $\\sigma_w = \\sqrt{2}  \\sqrt{\\frac{2}{n + m}}$ (He) & $r_w = \\sqrt{6}  \\sqrt{\\frac{6}{n + m}}$ (He) \\\\ \n\t\t\\hline\n\t\\end{tabular}\n\t\\caption{Bilan des formules pour les paramètres d'initialisation}\n\t\\label{tab:bilan_init_formule}\n\\end{table}\n\n\n\\subsection{Inefficacité de l'initialisation classique}\\label{commedhab}\n\nNotre stratégie va consister à adapter l'approche présentée au paragraphe précédent pour des couches hypersphériques, {\\it i.e.} des sorties de la forme\n\\begin{equation}\n\t\\displaystyle y_{j} = \\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\\rho^{2}_{j} \n\\label{formulekilmefaut}\n\\end{equation}\nLes calculs sont bien sûr rendus plus techniques par la forme quadratique de la sortie hypersphérique.\nIls sont détaillés au paragraphe suivant. \n\n\\bigskip\n\nMais on va d'abord voir qu'on ne peut pas se placer dans un cadre aussi simple que celui qui précède. Plus précisément, travailler sur des lois norma",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 58,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_59",
      "text": "les et centrées ne permet pas d'obtenir une initialisation pertinente.\n\nNous allons en donner une illustration pratique.\nPuis nous le justifierons par des arguments probabilistes.\n\n\\bigskip\n\n\n\\subsubsection{Expérimentation}\n\nNous considérons des données, les entrées $x_i$, $1\\le i \\le n$, qui suivent une loi normale et centrée.\nPour l'initialisation des hypersphères, \nles centres sont initialement tirés aléatoirement selon une loi normale centrée, avec un écart-type ajusté en fonction de la dimension $n$ et du nombre de neurones dans la couche\\footnote{Bien sûr l'écart-type n'est pas ajusté selon la règle \\eqref{eq:regle_glorot} obtenu pour des couches classiques ; ils respectent la règle établie au paragraphe \\ref{var} mais dans le cas où d'une loi normale {\\bf centrée}.}. \nLes rayons des hypersphères sont initialisés à 1.\n\n\nOn peut observer la propagation du signal à travers les trois premières couches à la Figure \\ref{fig:distrib_out_dense_hs}. \n\n\n\\begin{figure}[H]\n\t\\begin{center}\t\t\n\t\t\\includegraphics[width=10cm]{figs/out_densesph.png}\n\t\t\\caption{Distribution de l'entrée et valeurs de sortie des couches hypersphériques}\n\t\t\\label{fig:distrib_out_dense_hs}\n\t\\end{center}\n\\end{figur",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 59,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_60",
      "text": "e}\n\nClairement, l'initialisation n'est pas efficace\\footnote{On rappelle que le but était de maintenir le signal dans une plage de valeurs contrôlées et éviter des divergences numériques.}.\n\\\\\nOn observe une explosion de l'écart-type des valeurs dès la sortie des premières couches hypersphériques.\n\n\nCe phénomène se manifeste également dans les couches à filtre de convolution hypersphériques. La figure \\ref{fig:distrib_out_conv_hs} illustre la distribution des données d'entrée et de sortie un tenseurs de taille $(3,1,28,28)$ initialisés selon une loi normale. Cela correspond, par exemple, à un batch de trois images de taille $28 \\times 28$. Chaque colonne présente un tenseur différent, tandis que chaque ligne montre l'évolution des distributions à travers les couches successives, depuis la première couche jusqu'à la quatrième couche à filtre de convolution hypersphérique.\n\n\\begin{figure}[H]\n\t\\begin{center}\t\t\n\t\t\\includegraphics[width=16cm]{figs/out_conv.png}\n\t\t\\caption{Distribution de l'entrée et valeurs de sortie des couches à filtre hypersphériques}\n\t\t\\label{fig:distrib_out_conv_hs}\n\t\\end{center}\n\\end{figure}\n\n\t\n\\bigskip\n\nDans ces expérimentations, au-delà de l'explosion des écarts",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 60,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_61",
      "text": "type, un élément frappant est le fait qu'on perd très vite la symétrie de la distribution initiale.\nEn fait on voit dans la figure \\ref{fig:distrib_out_dense_hs} que la distribution à la sortie de la première couche ne suit déjà plus une loi normale. \nL'hypothèse de normalité de la distribution au fil des couches étant à la base de tous les calculs précédents, c'est évidemment un souci : on sait conserver la moyenne et la variance d'une distribution normale au travers de la couche hypersphérique mais dès la première sortie la distribution n'est plus normale et plus rien ne sera assuré à partir de la deuxième couche.\n\t\n\n\\bigskip\n\nCe point peut être confirmé par les éléments de probabilités fondamentales qui suivent. \nIl est la principale difficulté de ce travail d'initialisation.\n\n\\bigskip \n\n\\subsubsection{Eléments de probabilités}\n\nLa densité de probabilité d'une loi normale d'espérance $\\mu$ et d'écart-type $\\sigma$ est donnée par\n$$ x \\mapsto \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\bigl(\\frac{x-\\mu}{\\sigma}\\bigr)^2}.$$\nSi on compose cette fonction avec une fonction affine $x\\mapsto wx+b$, caractéristique d'une couche dense classique, on obtient une densité\n$$ x \\mapsto \\frac",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 61,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_62",
      "text": "{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\bigl(\\frac{wx + b -\\mu}{\\sigma}\\bigr)^2}$$\nqui correspond toujours à une loi normale. L'approche {\\`a la Glorot} a du sens : une distribution normale est transformée par la couche dense en une distribution normale.\nMais si la densité de Gauss est composée avec la fonction carrée, fonction qui apparaît dans l'opérateur hypersphérique ({\\it cf. \\eqref{formulekilmefaut})}, la structure de la loi normale disparaît :\n$$ x \\mapsto \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\bigl(\\frac{x^2-\\mu}{\\sigma}\\bigr)^2}.$$\nLa couche hypersphérique ne peut donc transformer une distribution normale en une distribution normale.\nEn fait (voir par exemple  \\cite{Johnson}), si une variable aléatoire réelle $X$ suit une loi normale centrée d'écart-type $\\sigma$, noté $X \\sim \\mathcal{N}(0,\\sigma^2)$, alors $X^2$ suit une loi Gamma, $X^2 \\sim G(1/2,2\\sigma^4)$. Cette dernière n'est pas stable par le passage au carré : si $X$ suit une loi Gamma, alors $X^2$ suit une loi dite {\\bf Gamma Généralisée}. \nOn a enfin abouti à une loi suffisamment stable pour nos opérations hypersphériques car si $X$ suit une loi Gamma Généralisée alors toute puissance de $X^p$, $p\\in \\mathb",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 62,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_63",
      "text": "b{N}^*$, suit aussi une loi Gamma Généralisée. \nCe point est confirmé par la forme de la densité d'une loi Gamma Généralisée, donnée par une fonction\n$$ x \\mapsto \\frac{(p/a^d)x^{d-1}e^{-(x/a)^p}}{\\Gamma(d/p)},$$\noù $d>0$ et $p>0$ sont des paramètres de forme, $a$ est un paramètre d'échelle et $\\Gamma$ désigne la fonction Gamma.\n\nLa prépondérance de la distribution Gamma Généralisée aux sorties hypersphériques est aussi confirmée par la forme des sorties observées dans la Figure \\ref{fig:distrib_out_dense_hs} (l'impression visuelle peut être confortée par des travaux de fitting tels que \\cite{Wagener}).\n\n\\bigskip \n\nPour les premières propriétés des lois Gamma Généralisées, nous renvoyons le lecteur par exemple à \\cite{Morteza} ou \\cite{Jiang2021TheGG} où la bibliographie est bien présentée.\n\nPour reproduire les calculs du paragraphe \\ref{etat}, des propriétés plus spécifiques sont nécessaires. La difficulté est qu'il faut calculer la variance et la moyenne de produits et de sommes impliquant au moins une distribution Gamma Généralisée.\\\\\n\nDe ce point de vue :\n\\begin{itemize} \n    \\item La somme de deux variables aléatoires réelles suivant deux lois Gamma Généralisées ne suit pas {\\",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 63,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_64",
      "text": "it a priori} une loi Gamma Généralisée (voir \\cite{Soury})\\footnote{On n'avait pas ce souci au paragraphe \\ref{etat} puisque la somme de deux distributions normales suit une loi normale. Voir par exemple \\cite{springer1979algebra}}.\n    \\item Le produit de deux variables aléatoires réelles suivant respectivement une loi Gamma Généralisée et une loi normale ne suit pas une loi Gamma Généralisée (voir \\cite{Malik_1967}).\n    \\item Le même problème se pose en fait pour le produit de deux variables aléatoires réelles suivant une loi normale. Cependant, voir par exemple \\cite{SeijasMacias}, les moments caractéristiques de la loi produit sont calculables et on peut donc la rapprocher d'une loi normale.\n    \\item Pour obtenir des formules explicites pour les calculs de variance, peu de choix s'offrent à nous. \\\\\n    On peut penser à la méthode de Stein pour déterminer les fonctions densités de probabilité au fil des couches. On peut consulter \\cite{Gaunt} pour des calculs de ce type dans le contexte des lois Gamma. On voit que c'est extrêmement technique. De plus, le calcul des variances et des moyennes {\\it via} l'intégration des densités de probabilité doit ensuite être approché numériq",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 64,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_65",
      "text": "uement. Pour les premières briques dans le contexte Gamma Généralisée, on peut consulter \\cite{Podolski}, \\cite{Marques2012OnTP}, et \\cite{MARQUES201655}.\n    \\\\\n    La question de l'approximation numérique est d'autant plus sensible que des distributions  Gamma Généralisées de forme très semblables peuvent avoir des paramètres $(a,d,p)$ très différents (voir \\cite{lawless2011statistical}).\n\\end{itemize}\n\n\\bigskip\n\n\\subsubsection{Stratégie}\n\nAu vu des éléments précédents, nous allons mettre en place la stratégie suivante :\\\\\n\n\\begin{itemize}\n    \\item[$\\bullet$] Une loi  Gamma Généralisée est caractérisée par trois paramètres. La méthode {\\it à la Glorot} consiste à en imposer seulement deux.\\\\\n    L'idée est d'influer sur le degré de liberté restant pour forcer les distributions Gamma Généralisées en sortie de chaque couche à s'approcher d'une distribution normale.\n    \\\\\n    \\item[$\\bullet$] Pour parvenir à rapprocher chaque sortie d'une distribution normale, en grande dimension, nous serons aidés par le théorème central limite. Au vu de l'expérimentation précédente ce n'est évidemment pas suffisant.\\\\\n    Afin de ne pas introduire dans le problème de variable supplémentaire, nou",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 65,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_66",
      "text": "s allons peser sur les moyennes et les variances.\n    %\\\\\n    %\\item[$\\bullet$] \n\\end{itemize}\n\n\\bigskip \n\nIllustrons la méthode avec quelques calculs simples. On considère une variable aléatoire réelle $X$ suivant une loi Gamma Généralisée, $X \\sim \\mbox{GG}(a,d,p)$. On sait alors que $X^2 \\sim \\mbox{GG}(a^2,d/2,p/2)$.\n\\\\\nComparons les moyennes :\n\\begin{eqnarray*}\n&& \\mathbb{E}(X) = a \\frac{\\Gamma\\bigl(\\frac{d+1}{p}\\bigr)}{\\Gamma\\bigl(\\frac{d+1}{p}\\bigr)},\n\\\\\n&& \\mathbb{E}(X^2) = a^2 \\frac{\\Gamma\\bigl(\\frac{d+2}{p}\\bigr)}{\\Gamma\\bigl(\\frac{d}{p}\\bigr)} \n=\\mathbb{E}(X) \\times a \\times \\frac{\\Gamma\\bigl(\\frac{d+2}{p}\\bigr)}{\\Gamma\\bigl(\\frac{d+1}{p}\\bigr)} .\n\\end{eqnarray*}\nDans le cas où $p=1$ (c'est le cas où la loi Gamma Généralisée est une loi Gamma, en sortie de première couche si l'entrée est gaussienne par exemple) et pour un paramètre de forme $d\\ge 1$ (ce qui est le cas observé), on voit que\n$$ \\mathbb{E}(X^2) = a(d+1) \\mathbb{E}(X).$$\nAinsi, la moyenne se décale vers la droite. Cet effet indésirable s'observe d'ailleurs dans les sorties hypersphériques ci-dessus (avec un décalage vers la gauche à cause du signe ``$-$'' dans \\eqref{formulekilmefaut}).\nUn moyen de limiter ce",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 66,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_67",
      "text": "décalage est de réduire la valeur de $a$. Et un moyen direct d'influer sur la valeur de $a$ est de réduire la variance de la distribution $X$.\n\n\n\n\\noindent Comparons maintenant les variances :\n\\begin{eqnarray*}\n&& \\mbox{var}(X) = a^2 \\frac{\\Gamma\\bigl(\\frac{d+2}{p}\\bigr)\\Gamma\\bigl(\\frac{d}{p}\\bigr)-\\Gamma\\bigl(\\frac{d+1}{p}\\bigr)^2}{\\Gamma\\bigl(\\frac{d}{p}\\bigr)},\n\\\\\n&& \\mbox{var}(X^2) = a^4 \\frac{\\Gamma\\bigl(\\frac{d+4}{p}\\bigr)\\Gamma\\bigl(\\frac{d}{p}\\bigr)-\\Gamma\\bigl(\\frac{d+2}{p}\\bigr)^2}{\\Gamma\\bigl(\\frac{d}{p}\\bigr)}  .\n\\end{eqnarray*}\nUne fois encore, réduire la valeur de $a$ permet de contrôler l'explosion de la variance.\n\nCes considérations sur la partie carrée de la sortie hypersphérique \\eqref{formulekilmefaut} sont confortées par les travaux de Aroian et al. \\cite{Aroian1978MathematicalFO} (voir aussi \\cite{AntonioSeijas2012} pour des expérimentations numériques) qui établissent que le produit de $X \\sim \\mathcal{N}(\\mu_x,\\sigma_x^2)$ par $S \\sim \\mathcal{N}(\\mu_s,\\sigma_s^2)$ tend vers une distribution normale si\n\\begin{equation}\n    \\frac{\\mu_x}{\\sigma_x} + \\frac{\\mu_s}{\\sigma_s} \\gg 1 .\n    \\label{maline}\n\\end{equation}\n\n\\bigskip\n\n\\noindent{\\bf En bref :} la stratég",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 67,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_68",
      "text": "ie consiste à assurer des sorties normales \n\\begin{itemize}\n    \\item[$\\bullet$] en diminuant la variance des données en entrée\n    \\item[$\\bullet$] en initialisant les poids avec une moyenne non nulle\n\\end{itemize}\nOn reprend donc les calculs {\\it à la Glorot} dans le paragraphe suivant, en les adaptant aux sorties hypersphériques, mais en conservant l'hypothèse de normalité.\n\n\t\n\\subsection{Initialisation hypersphérique par le contrôle des moyennes et variances}\\label{var} \n\nCette partie est très calculatoire. Tous les détails sont fournis pour assurer la reproductibilité des résultats. Nous invitons le lecteur qui veut aller directement aux résultats à consulter le paragraphe \\ref{subsec:resume_init} suivant.\n\n\\bigskip\n\nComme dans l’approche classique, le calcul des variances entre les entrées et les sorties vise à établir une condition pour maintenir une variance constante du signal à travers les couches du réseau. En imposant l’égalité entre la variance des entrées et celle des sorties, on peut relier les  paramètres de l’hypersphère aux caractéristiques des données d’entrée. Cela permet d’imposer des contraintes sur les paramètres du modèle afin d’assurer une propagation adéqu",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 68,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_69",
      "text": "ate du signal. L'objectif principal est de déterminer les valeurs d'initialisation des poids du réseau pour minimiser les variations indésirables du signal durant l'apprentissage. Ici, on fixe le rayon des hypersphères et on joue sur la distribution des centres.\n\n\\subsubsection{Contrôle pendant la propagation (contrôle {\\it forward})}% : couches denses hypersphériques}\n\nPour l'initialisation, on introduit un paramètre supplémentaire $\\delta >0$ dans le réseau. Ce paramètre nous permettra de gagner suffisamment de latitude pour parvenir à nos fins. \nEn particulier\\footnote{mais pas seulement...}, il va jouer le rôle de facteur d'échelle pour ramener les données à des quantités à variance petite (conformément à la stratégie construite ci-dessus).\nOn considère donc que les  éléments $y_j$ du vecteur de sortie $Y$, sont donnés par $y_{j}= \\frac{\\widetilde{ X}._i\\widetilde{S}_j}{\\delta}$, c'est-à-dire :\n\\begin{equation}\n\t\\displaystyle y_{j} = \\frac{1}{2\\delta}\\left[\\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\\rho^{2}_{j} \\right] \\qquad \\forall 1 \\le j \\le m,\n\\end{equation}\nle nombre $m$ étant le nombre de sphères (neurones hypersphériques) dans la couche.\n\n\\bigskip\n\nOn suppose",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 69,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_70",
      "text": "les données identiquement distribuées suivant une loi normale : pour tout $i \\in \\{1, \\ldots, n\\}$,  $x_i \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$.\n\nOn fait une hypothèse similaire pour les sorties $y_j$ et les poids du réseau $s_{ji}$ : pour tout $i \\in \\{1, \\ldots, n\\}$, tout $j \\in \\{1, \\ldots, m\\}$, $y_j \\sim \\mathcal{N}(\\mu_y, \\sigma_y^2)$, $s_{ij} \\sim \\mathcal{N}(\\mu_s, \\sigma_s^2)$.\n\nLe rayon  $\\rho$ est supposé fixe.\n\n\n\n\\bigskip \n\n\n\n\\noindent{\\bf Remarque.} \nPour faciliter la lecture, on rappelle le résultat suivant. Soit $x$ une variable aléatoire  telle que $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Les moments centrés peuvent être calculés en fonction des moments d'ordre $p$ en développant l'expression $(x-\\mu)^p$ puis en appliquant les propriété de linéarité de l'espérance \\cite{bogaert2020probabilites}. On a donc\n$$\n\t\\begin{array}{ll}\n\t\t\\mathbb{E} \\left[ (X-\\mu)^{2} \\right] & = \\mathbb{E} \\left[ X^2 - 2 \\mu X + \\mu^2 \\right]\\\\\n\t\t\\sigma^2 & =  \\mathbb{E} \\left[ X^2 \\right] - 2 \\mu \\textcolor{red}{ \\mathbb{E} \\left[ X \\right]} +    \\mu^2 \\\\\n\t\t\\sigma^2 & =  \\mathbb{E} \\left[ X^2 \\right] - 2 \\mu \\textcolor{red}{\\mu} +    \\mu^2 \\\\\n\t\t\n\t\\end{array}\n$$\n$$\n\t\\begin{array}{ll}\n\t\t\\mathbb{E",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 70,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_71",
      "text": "} \\left[ (X-\\mu)^{3} \\right]& = \\mathbb{E} \\left[ X^3 - 3 X^2 \\mu + 3 X \\mu^2 -3\\mu^3 \\right] \\\\\n\t0\t& = \\mathbb{E} \\left[ X^3 \\right] - 3  \\mu \\textcolor{red}{ \\mathbb{E} \\left[ X^2 \\right]} + 3 \\mu^2\\textcolor{red}{ \\mathbb{E} \\left[ X \\right]}  -3 \\mu^3 \\\\\n\t0\t& = \\mathbb{E} \\left[ X^3 \\right] - 3  \\mu \\textcolor{red}{(\\mu^2 + \\sigma^2)} + 3 \\mu^2\\textcolor{red}{\\mu}  -3 \\mu^3 \\\\\n\t\\end{array}\n$$\n$$\n\t\\begin{array}{ll}\n\t\t\\mathbb{E} \\left[ (X-\\mu)^{4} \\right]& = \\mathbb{E} \\left[ X^4 - 4 \\mu X^3 + 6 \\mu^2 X^2 -4 \\mu^3 X + \\mu^4 \\right] \\\\\n\t3 \\sigma^4\t& = \\mathbb{E} \\left[ X^4 \\right] - 4  \\mu \\textcolor{red}{\\mathbb{E} \\left[ X^3 \\right]} + 6 \\mu^2\\textcolor{red}{\\mathbb{E} \\left[ X^2 \\right]}  -4 \\mu^3 \\textcolor{red}{\\mathbb{E} \\left[ X \\right]} + \\mu^4\\\\\n\t3 \\sigma^4\t& = \\mathbb{E} \\left[ X^4 \\right] - 4  \\mu \\textcolor{red}{(3\\mu\\sigma^2+\\mu^3)} + 6 \\mu^2\\textcolor{red}{(\\sigma^2+\\mu^2)}  -4 \\mu^3 \\textcolor{red}{\\mu} + \\mu^4\\\\\n\t\\end{array}\n$$\nLe calcul des quatre premiers moments donne en particulier les résultats suivants~:\n$$\\begin{array}{l l }\n    \\mathbb{E}\\left[X\\right]= \\mu &\\mathbb{E}\\left[X^2\\right]= \\sigma^2+\\mu^2\\\\\n    \\\\\n    \\mathbb{E}\\left[X^3\\right]= 3\\mu\\sigma^2+\\mu",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 71,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_72",
      "text": "^3 &  \\mathbb{E}\\left[X^4\\right]= 3\\sigma^4+6\\sigma^2\\mu^2+\\mu^4.\n\\end{array}$$\n\n\\vspace{1cm}\n\n\n\nLe calcul de l'espérance de $y_j$ donne, pour tout $1 \\le j \\le m$ :\\\\\n$\\begin{array}{ll}\n\t\\displaystyle \\mathbb{E}(y_{j}) &\\displaystyle =\\mathbb{E}\\left(\\frac{1}{2\\delta}\\left[\\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\\rho^{2}_{j} \\right]\\right)\\\\\n\t\\\\\n\t&\\displaystyle =\\mathbb{E}\\left(\\frac{1}{2\\delta}\\left[\\sum_{k=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji}) \\right]\\right)+\\mathbb{E}\\left(\\frac{1}{2\\delta}\\left[\\rho^{2}_{j} \\right]\\right)\\\\\n\t\\\\\n\t&\\displaystyle =\\frac{n}{\\delta}\\mathbb{E}\\left(x_{i} s_{ji} \\right)-\\frac{n}{2\\delta}\\mathbb{E}\\left(x^{2}_{i} \\right)-\\frac{n}{2\\delta}\\mathbb{E}\\left(s^{2}_{ji} \\right)+\\frac{1}{2\\delta}\\mathbb{E}\\left(\\rho^{2}_{j} \\right)\\\\\n\t\\\\\n\t&\\displaystyle =\\frac{n}{\\delta}\\mathbb{E}\\left(x_{i}\\right)\\mathbb{E}\\left(s_{ji}\\right)-\\frac{n}{2\\delta}\\mathbb{E}\\left(x^{2}_{i} \\right)-\\frac{n}{2\\delta}\\mathbb{E}\\left(s^{2}_{ji} \\right)+\\frac{\\rho^2_j}{2\\delta}\n\\end{array}$\n\n\\medskip\n\n\\noindent Pour assurer que $\\mathbb{E}(y_{j})= \\mu_x$, il faut donc :\t\t \n\n\\begin{equation*}\n\t\\begin{array}{ll}\n\t\t\\displaystyle \\mu_x &= \\displaystyle\\frac{n}{\\delta}",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 72,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_73",
      "text": "\\mu_x \\mu_s-\\frac{n}{2\\delta}\\left(\\sigma_x^2+\\mu_x^2\\right)-\\frac{n}{2\\delta}\\left(\\sigma_s^2+\\mu_s^2\\right)+\\frac{\\rho^2_j}{2\\delta}\\\\\n\t\t\\\\\n\t\t& \\displaystyle= \\frac{n}{2\\delta}\\left[2 \\mu_x \\mu_s - \\mu_x^2 -\\mu_s^2 -\\sigma_x^2-\\sigma_s^2 +\\frac{\\rho^2}{n}\\right]\\\\\n\t\t\\\\\n\t\t&\\displaystyle = \\frac{n}{2\\delta}\\left[ (\\mu_x-\\mu_s)^2 + \\sigma_x^2 + \\sigma_s^2 +\\frac{\\rho^2}{n}\\right]\\\\\n\t\\end{array}\n\t\\label{eq:vareq}\n\\end{equation*}\négalité qui revient à~:\\\\\n\\begin{equation*}\n\t\\begin{array}{ll}\n\t\t\\displaystyle \\rho^2 & = 2\\delta \\mu_x + n \\left[ (\\mu_x-\\mu_s)^2 + \\sigma_x^2 + \\sigma_s^2 \\right]\n\t\\end{array}\n\t\\label{eq:ecriturho2}\n\\end{equation*}\nou~:\\\\\n\n\\begin{equation}\n\t\\begin{array}{ll}\n\t\t\\displaystyle \\rho & =\\sqrt{ 2\\delta \\mu_x + n \\left[ (\\mu_x-\\mu_s)^2 + \\sigma_x^2 + \\sigma_s^2 \\right]}\\\\\n\t\\end{array}\n\t\\label{eq:ecriturho}\n\\end{equation}\\\\\n\nPassons à la recherche d'une condition induite par l'égalité des variances en entrée et en sortie. \nOn rappelle que si $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$, \nalors var$(X^2) = \\mathbb{E}\\left[X^4_{}\\right]-\\mathbb{E}^{2}\\left[X^2_{}\\right] =  2 \\sigma^2 \\left(\\sigma^2 + 2 \\mu^2 \\right)$.\nEn utilisant la formule  \\eqref{eq:varianceformuleindependa",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 73,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_74",
      "text": "nt},  on  calcule la variance de $2x_i s_{ji}$ de la façon suivante~:\n\\begin{equation}\n\t\\begin{array}{ll}\n\t\t\\displaystyle \\mbox{var}(2x_i s_{ji}) & =4 \\mbox{var}(s_{ji}) \\mbox{var}(x_i) + 4 \\mathbb{E}^2\\left[s_{ji}^2\\right] \\mbox{var}(x_i) +4 \\mathbb{E}^2\\left[x_{i}^2\\right] \\mbox{var}(s_{ji})\\\\\n\t\t\\\\\n\t\t&=4 \\sigma_s^2 \\sigma_x^2 + 4 \\mu_x^2 \\sigma_s^2 + 4 \\mu_s^2 \\sigma_x^2\n\t\\end{array}\n\t\\label{eq:ecriturvar2xs}\n\\end{equation}\nOr on a~:\n\\begin{equation}\n\\begin{array}{l l }\n    \\mbox{var}(x_i)= \\sigma_x^2  & \\mbox{var}(x_i^2)=  2 \\sigma_x^2 \\left(\\sigma_x^2 + 2 \\mu_x^2 \\right)\\\\\n\\end{array}\n\\label{eq:ecriturvarx}\n\\end{equation}\n\\begin{equation}\n\\begin{array}{l l }\n    \\mbox{var}(s_{ji})= \\sigma_s^2   & \\mbox{var}(s_{ji}^2)=  2 \\sigma_s^2 \\left(\\sigma_s^2 + 2 \\mu_s^2 \\right)\n\\end{array}\n\\label{eq:ecriturvars}\n\\end{equation}\n\\begin{equation}\n\\begin{array}{l l }\n    \\mbox{cov}\\left( 2x_i s_{ji}-x_i^2, -s_{ji}^2\\right) & = -4 \\mu_s \\mu_x \\sigma_s^2\\\\\n    \\mbox{cov}\\left( 2x_i s_{ji},-x_i^2 \\right) & = -4 \\mu_s \\mu_x \\sigma_x^2\\\n\\end{array}\n\\label{eq:ecriturdescovariances}\n\\end{equation}\nL'annexe \\ref{annex_covariance} détaille les calculs des termes de covariance. \n\\[\n\\begin{array}{ll}",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 74,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_75",
      "text": "\\displaystyle \\operatorname{var}(y_{j}) &\\displaystyle = \\operatorname{var}\\left(\\frac{1}{2\\delta}\\left[\\sum_{i=1}^{n} (2\\cdot x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\\rho^{2}_{j} \\right]\\right)\\\\\n\t\\\\\n\t\n\t&\\displaystyle = \\frac{1}{4\\delta^2}\\operatorname{var}\\left(\\left[\\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})\\right]\\right) {\\footnotesize \\text{ hypothèse de distributions identiques}} \\\\\n\t\\\\\n\t\n\t&\\displaystyle = \\frac{n}{4\\delta^2} \\operatorname{var}\\left(2  x_{i}  s_{ji} - x^{2}_{i} - s^{2}_{ji}\\right)\\\\\n\t\\\\\n\t\n\t&\\displaystyle = \\frac{n}{\\delta^2} \\operatorname{var}\\left(x_{i}  s_{ji}\\right) + \\frac{n}{4\\delta^2} \\operatorname{var}\\left(x^{2}_{i}\\right) + \\frac{n}{4\\delta^2} \\operatorname{var}\\left(s^{2}_{ji}\\right)\\\\\n\t\\\\\n\t& + \\dfrac{1}{2\\delta^2}\\mbox{cov}(2x_i s_{ji}-x_i^2, -s_{ji}^2) + \\dfrac{1}{2\\delta^2}\\mbox{cov}(2x_i s_{ji},-x_i^2) \\\\\n\\end{array}\n\\]\n\n\\newpage\n\nEn utilisant les relations \\eqref{eq:ecriturvar2xs}--\\eqref{eq:ecriturdescovariances},\non obtient  l'expression suivante~:\n\n\\begin{align*}\n\\mbox{var}(y_j) &=\\frac{n}{4 \\delta^2} (4\\mu_s^2\\sigma_s^2 + 4\\mu_s^2\\sigma_x^2 - 8\\mu_s\\mu_x\\sigma_s^2 - 8\\mu_s\\mu_x\\sigma_x^2 + 4\\mu_x^2\\sigma_s^2 + 4\\mu_x^2\\sigma_x^2 + 2\\si",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 75,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_76",
      "text": "gma_s^4 + 4\\sigma_s^2\\sigma_x^2 + 2\\sigma_x^4 )\\\\\n\\\\\n &= \\frac{n}{2 \\delta^2} (\\sigma_s^2 + \\sigma_x^2) \\left( 2(\\mu_x  - \\mu_s)^2  +\\sigma_s^2 + \\sigma_x^2 \\right)\n\\end{align*}\n\n\nOn veut que var$(y_j)=\\sigma_x^2$ pour tous $i \\in \\{1, \\ldots, n\\}$ et $j\\in \\{1, \\ldots, m\\}$. Cela se traduit par l'equation~:\n\n\\begin{equation}\n\\begin{array}{l }\n    2\\mu_s^2\\textcolor{red}{\\sigma_s^2} + 2\\mu_s^2\\sigma_x^2 - 4\\mu_s\\mu_x\\textcolor{red}{\\sigma_s^2} - 4\\mu_s\\mu_x\\sigma_x^2 + 2\\mu_x^2\\textcolor{red}{\\sigma_s^2} + 2\\mu_x^2\\sigma_x^2 + \\textcolor{blue}{\\sigma_s^4} + 2\\textcolor{red}{\\sigma_s^2}\\sigma_x^2 + \\sigma_x^4 - \\frac{2\\delta^2}{n}\\sigma_x^2=0\n\\end{array}\n\\label{eq:eqvariance}\n\\end{equation}\\\\\n\nL'équation \\eqref{eq:eqvariance} est une équation bicarrée en $\\sigma^2_s$. On peut donc la résoudre explicitement avec la méthode du discriminant. \nEn ne conservant que la solution qui peut être positive, on arrive à\n\n\\begin{equation}\n\\displaystyle \\sigma_s^2 = -\\left(\\mu_s- \\mu_x \\right)^2- \\sigma_x^2 + \\sqrt{\\left(\\mu_s^2 - \\mu_x^2\\right)^4 + \\frac{2\\sigma_x^2\\delta^2}{n}}\n\\label{eq:solution_1_sigma_s}\n\\end{equation}\\\\\n\n\\noindent  Garantir la positivité de $\\sigma^2_s$ revient à s'assurer q",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 76,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_77",
      "text": "ue son numérateur est positif étant donné que le coefficient devant $\\sigma^4_s$  est positif, c'est à dire la condition suivante~:\n\n\\begin{equation}\n \\sigma_x^2 \\left(-2\\mu_s^2 + 4\\mu_s\\mu_x - 2\\mu_x^2 - \\sigma_x^2 + \\frac{2\\delta^2}{n}\\right) >0\n\\label{eq:deltatcond}\n\\end{equation}\n\n\\noindent Pour s'en assurer, il faut cette fois résoudre une autre équation du second degré en $\\mu_s$. Il existe une solution réelle si son discriminant satisfait  $\\displaystyle \\Delta_{\\mu} =  - 8 \\sigma_x^2 + \\frac{16}{n}\\delta^2 > 0$. On en tire la condition suivante sur $\\delta$~:\n\\begin{equation*}\n\t\\displaystyle   16 \\left( \\frac{\\delta^2}{n} - \\frac{\\sigma_x^2}{2}\\right) >0\n\\end{equation*}\\\\\nc'est-à-dire~: \\\\\n\n\n\\begin{equation}\n\t\\displaystyle\n\t\\displaystyle   \\delta  > \\sqrt{ \\frac{n}{2} \\sigma_x^2 }\n\t\\label{eq:cond_delta_mu}\n\\end{equation}\\\\\n\t\nOn vérifie alors que la solution donnée par \\eqref{eq:deltatcond} est bien définie si  $\\mu_s$ satisfait~:\\\\\n\n\n\\begin{equation}\n\t\\displaystyle   \\mu_s \\in \\left[\\textcolor{red}{ \\mu_x - \\sqrt{  \\frac{\\delta^2}{n} - \\frac{\\sigma_x^2}{2} }}, \\textcolor{blue}{ \\mu_x + \\sqrt{ \\frac{\\delta^2}{n} - \\frac{\\sigma_x^2}{2}} } \\right]\n\t\\label{eq:cond_mu_s}\n\\end{eq",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 77,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_78",
      "text": "uation}\\\\\n\n\\newpage\n\t\nPour résumer, les calculs précédents aboutissent aux conditions suivantes~:\\\\\n\t\n\t\\begin{equation}\n\t\t\\displaystyle\\left\\{\\begin{array}{ll}\n\t\t\t\\displaystyle \\rho^2 & = 2\\delta \\mu_x + n \\left[ (\\mu_x-\\mu_s)^2 + \\sigma_x^2 + \\sigma_s^2 \\right]\\\\\n\t\t\t\\\\\n\t\t\t\\displaystyle \\sigma_s^2 & =  \\sqrt{ \\left(\\mu_x - \\mu_s \\right)^4 +  \\frac{2 \\delta^2}{n}\\sigma_x^2 }- \\left[ \\left( \\mu_s - \\mu_x \\right)^2 + \\sigma_x^2\\right]\n\t\t\\end{array}\\right.\n\t\t\\label{eq:sysvariance}\n\t\\end{equation}\npour $\\delta$ vérifiant \\eqref{eq:cond_delta_mu}.\n\n\\bigskip\n\n\\noindent{\\bf Remarque.} Nous nous sommes également intéressés au cas des couches de neurones à filtres convolutifs ({\\it cf} Annexe \\ref{annexcasconvolutive}), afin de déterminer comment la covariance entre deux éléments de la sortie est reliée aux contraintes du système d’équations \\eqref{eq:sysvariance} établi précédemment.\nDes travaux comme \\cite{trockman2022} ont de plus montré comment exploiter la structure de covariance d'un filtre convolutif peut permettre d'améliorer l'initialisation.\n\n\t\n\t\\bigskip \n\t\n\t\\subsubsection{Contrôle pendant la rétro-propagation (contrôle backward)}% : couches denses hypersphériques}\n\t\n\t\n\t\n\tDans cett",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 78,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_79",
      "text": "e partie, l'impact de la variance de la fonction de coût sur les poids est étudié. \n\t%Afin d'éviter toute confusion avec les notations utilisées dans la section sur l'algèbre conforme, les notations sont modifiées par rapport à l'article de Glorot.\n\tLa sortie d'un neurone de la couche $l$ est désormais notée $\\mathbf{z}^l$, \n\tdéfinie par\n\t$$\\begin{array}{ll}\n\t\t\\mathbf{z}^l_j = \\displaystyle \\frac{1}{\\delta}\\widetilde{\\mathbf{z}^l}._i\\widetilde{s^l_j} & = \\displaystyle\\frac{1}{\\delta}\\left[\\sum_{i=1}^{n}x^l_{i}s^l_{ji}-\\frac{1}{2}{x^l_{i}}^2 - \\displaystyle \\frac{1}{2}{s^l_{ji}}^2+\\frac{\\rho^2_j}{2}\\right]\\\\\n\t\t\\\\\n\t\t& = \\displaystyle\\frac{1}{\\delta}\\left[\\mathbf{x}^l_{} \\mathbf{s}^l_{j}-\\frac{1}{2}{\\mathbf{x}^l_{} \\mathbf{x}^l} - \\frac{1}{2}{\\mathbf{x}^l_{j}\\cdot \\mathbf{s}^l_{j}}+\\frac{\\rho^2_j}{2}\\right]\n\t\\end{array}$$\\\\\n\t\n\t\\noindent où $\\mathbf{s}^l_j$ correspond à la partie euclidienne du vecteur $S^l$, soit les coordonnées du centre de l'hypersphère.\tOn a ainsi, \\\\\n\t\n\t$$\\begin{array}{ll}\n\t\t\\mathbf{z}^{l+1}_{j'}  = & \\displaystyle\\frac{1}{\\delta}\\left[\\mathbf{x}^{l+1}_{} \\mathbf{s}^{l+1}_{j'}-\\frac{1}{2}{\\mathbf{x}^{l+1}_{}}^2 - \\frac{1}{2}{\\mathbf{s}^{l+1}_{j'} }^2+\\frac{\\rho^2_",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 79,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_80",
      "text": "{j'}}{2}\\right]\n\t\\end{array}$$\\\\\n\t\n\\noindent Si $f$ représente la fonction d'activation telle que $f'(0) = 1$,  la sortie de la couche est  définie par \\hbox{$\\mathbf{x}^{l+1} = f(\\mathbf{z}^l)$}.\\\\\n\t\n\t\n\t\n\tOn note $L$ la fonction de coût.\n\t\n\t\n\t\n\t\n\tPour adapter au cas hypersphérique le raisonnement effectué dans le cas classique pour obtenir l'équation \\eqref{eq:sortie_classique_backprop} \\footnote{Si l'on considère $S^{l+1}_{}$ la matrice de poids des centres des hypersphères, alors on note $\\mathbf{s}^{l+1}_{j\\bullet}$, la $j$ ième ligne de $S^{l+1}_{}$ qui correspond au vecteur du centre pour un $j$ donné.}, on calcule la dérivée partielle de $\\mathbf{z}^{l+1}$ par rapport à $\\mathbf{x}^{l+1}_{j}$~: \\\\\n\t\n\t\\begin{equation*}\\label{eq:d}\n\t\t\\dfrac{\\partial \\mathbf{z}^{l+1}_{}}{\\partial \\mathbf{x}^{l+1}_{j}}=\\frac{1}{\\delta}\\left[\\mathbf{s}^{l+1}_{j\\bullet}- \\mathbf{x}^{l+1}_{j}\\right]\n\t\\end{equation*}\\\\\n\t\\newpage\n\t\n\t\\noindent ce qui conduit à l'équation suivante~:\\\\\n\t\n\t\\begin{equation*}\\label{eq:e}\n\t\t\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l}_{j}} = \\frac{1}{\\delta}\\left(\\mathbf{s}^{l+1}_{j\\bullet}- \\mathbf{x}^{l+1}_{j}\\right)\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{}}\n\t\\end{equ",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 80,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_81",
      "text": "ation*}\n\t\n\t\\noindent soit~:\n\t\n\t\\begin{equation}\\label{eq:f}\n\t\t\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l}_{}} = \\frac{1}{\\delta}\\left(S^{l+1}_{}- \\mathbf{x}^{l+1}_{}\\right)\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{}}\n\t\\end{equation}\\\\\n\t\n\t\\noindent avec $S^{l+1}$, la matrice dont les coefficients sont les centres des hypersphères. L'opération $\\left(S^{l+1}_{}- \\mathbf{x}^{l+1}_{}\\right)$ consiste donc à soustraire le vecteur $\\mathbf{x}^{l+1}$ à chaque colonne.\\\\\n\t\n\tDe \\eqref{eq:f}, on tire d'abord un résultat sur les moyennes :\\\\\n\t\n\t$$\\mathbb{E}\\Bigl( \\dfrac{\\partial L}{\\partial \\mathbf{z}^{l}_{j}} \\Bigr) \n\t= \\frac{1}{\\delta} \\mathbb{E}(S^{l+1}_{j}- \\mathbf{x}^{l+1}_{j}) \\mathbb{E}\\Bigl( \\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{j}} \\Bigr)\n\t= \\frac{m}{\\delta} (\\mu_s^{l+1} - \\mu_x^{l+1}) \\mathbb{E}\\Bigl( \\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{j}} \\Bigr).$$\n\tOn a assuré au paragraphe précédent que la moyenne des distributions de sortie reste constante au cours du processus : $\\mu_x^{l+1} = \\mu_x$.\n\tAinsi\\\\\n\t\n\t\\begin{equation}\n\t    \\mathbb{E}\\Bigl( \\dfrac{\\partial L}{\\partial \\mathbf{z}^{l}_{j}} \\Bigr) \n\t= \\frac{m}{\\delta} (\\mu_s^{l+1} - \\mu_x) \\mathbb{E}\\Bigl( \\dfrac{\\",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 81,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_82",
      "text": "partial L}{\\partial \\mathbf{z}^{l+1}_{j}} \\Bigr).\n\t\\label{eq:moy_retro_hyper}\n\t\\end{equation}\n\t\n\t\\bigskip \n\t\n\tOn peut aussi calculer\\footnote{Les étapes du calculs sont présentées en Annexe \\ref{annex_variance}.} les variances dans \\eqref{eq:f} :\n\t\n\t\\begin{equation}\n\t\\begin{array}{ll}\n\t\t\\displaystyle \\mbox{var}\\left(\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l}_{}} \\right) \n\t\t&  = \\dfrac{m}{\\delta^2}\\left[\\left(\\sigma_s^2 + \\sigma_x^2\\right)\\mbox{var}\\left(\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{j'}}\\right) + \\left(\\mu_x - \\mu_s \\right)^2\\mbox{var}\\left(\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{j'}}\\right) \\right.\\\\\n\t\t\t\\\\\n\t\t\t\\\\\n\t\t\t&\\left. +\\mathbb{E}^2\\left(\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{j'}}\\right)\\left(\\sigma_s^2 + \\sigma_x^2 \\right) \\right]\\\\\n\t\\end{array}\n\t\\label{eq:var_retro_hyper2}\n\t\\end{equation}\n\t\n\tOn cherche une initialisation permettant de limiter les écarts entre les moyennes et les variances des gradients successifs  $\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l}_{j}} $ et  $\\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{j}} $ au cours de la rétro-propagation.\n\t\n\tEn gardant à l'esprit que $\\mu_s $ est seulement défini par l'intervalle donné dans \\eqref{",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 82,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_83",
      "text": "eq:cond_mu_s}, le choix de $\\mu_s$ et $\\delta$ tels que $m \\vert \\mu_s - \\mu_x \\vert / \\delta \\ll 1$ permet de limiter les variations de moyenne dans \\eqref{eq:moy_retro_hyper}, en gardant à l'esprit que l'équation \\eqref{eq:var_retro_hyper2} limite la borne supérieure de $\\delta$ pour que $m(\\sigma_x^2+\\sigma_s^2)/\\delta^2$, c'est-à-dire $m\\sigma_x/\\delta$, reste d'un ordre non négligeable. Ceci dit, d'autres choix sont possibles. On voit par exemple dans \\eqref{eq:var_retro_hyper2}  que si on accepte un peu de variations dans l'espérance des gradients, cela évite la dégénérescence de leur variance... C'est le principe que nous utilisons dans la suite.\\\\\n\t\n\n\tOn réunit maintenant les conditions établies de \\eqref{eq:solution_1_sigma_s} à \\eqref{eq:var_retro_hyper2}. D'abord, on introduit un paramètre $\\delta'$ (produit de $\\delta$ par l'accroissement de moyenne qu'on s'autorise pendant la rétro-propagation) et on choisit pour $\\mu_s$ la valeur maximale autorisée par l'intervalle dans \\eqref{eq:cond_mu_s}.\n\tOn obtient ainsi un système qui donne deux valeurs possibles pour $\\mu_s-\\mu_x$~:\\\\\n\t\\begin{equation}\n\t\t\\displaystyle\\left\\{\\begin{array}{ll}\n\t\t\t \\mu_s - \\mu_x & = \\displaystyle",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 83,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_84",
      "text": "\\frac{\\delta'}{m}\\\\\n\t\t\t\\\\\n\t\t\t\\displaystyle\\mu_s - \\mu_x & =  \\displaystyle\\sqrt{\\frac{\\delta^2}{n}-\\frac{\\sigma^2_x}{2}} \n\t\t\\end{array}\\right.\n\t\t\\label{eq:sys_musmux}\n\t\\end{equation}\n\t\n\n Dans la première équation de \\eqref{eq:sys_musmux}, on a  $\\delta' = C \\delta$ où $C$ représente le taux de croissance des moyennes :\n\\begin{equation} \nC = \\frac{\\mathbb{E}\\Bigl( \\dfrac{\\partial L}{\\partial \\mathbf{z}^{l}_{j}} \\Bigr)}{\\mathbb{E}\\Bigl( \\dfrac{\\partial L}{\\partial \\mathbf{z}^{l+1}_{j}} \\Bigr)}.\n\\label{defC}\n\\end{equation}\nPar exemple si $C=1$, les moyennes ne changent pas. Si $C=2$, à chaque fois qu'on traverse une couche la moyenne est doublée etc. \\'Evidemment on cherche une stratégie qui n'augmente pas les moyennes de façon trop significative. D'autant que la croissance des moyennes conduit à une croissance des variances  à cause de \\eqref{eq:var_retro_hyper2}. \n\nDans la deuxième équation de \\eqref{eq:sys_musmux}, on a $\\delta'= m \\displaystyle \\sqrt{\\frac{\\delta^2}{n}-\\frac{\\sigma^2_x}{2}}$. Comme, d'après \\eqref{eq:cond_delta_mu}, il faut $\\delta'=C\\delta > C \\sqrt{(n/2)\\sigma^2_x}$, on a forcément $\\displaystyle\\sqrt{\\frac{\\delta^2}{n}-\\frac{\\sigma^2_x}{2}} >  (C/m)\\sqrt{\\frac{",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 84,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_85",
      "text": "n}{2}\\sigma^2_x}$. En passant au carré la dernière égalité et la dernière inégalité, on en tire les conditions suivantes sur $\\delta$ : \n\\begin{eqnarray}\n&& C^2 \\delta^2 = m^2( \\frac{\\delta^2}{n}-\\frac{\\sigma^2_x}{2}),\n\\label{cond111}\n\\\\\n&& \\delta^2 > \\frac{n}{2} \\Bigl(  \\frac{C^2n}{m^2} +1 \\Bigr)\\sigma_x^2.\n\\label{cond222}\n\\end{eqnarray}\nLa condition \\eqref{cond222} va se substituer à la condition \\eqref{eq:cond_delta_mu} (on note en particulier que \\eqref{cond222} implique \\eqref{eq:cond_delta_mu}).\nDe la condition \\eqref{cond111} on peut tirer une expression de $\\delta$ :\n\\begin{equation}\n\\delta = \\sqrt{\\frac{m^2n\\sigma_x^2}{2(m^2-C^2n)} } \n    \\label{conddelta000}\n\\end{equation}  \nqui n'a de sens que si la variation $C$ de la moyenne des gradients est contrôlée par\n\\begin{equation}\n    C \\le \\frac{m}{\\sqrt{n}} .\n    \\label{eq:condC000}\n\\end{equation}  \n\\\\\n\n\n\\bigskip \n\n{\\bf Récapitulons : }\n\\\\\n\\begin{itemize} \n\\item On choisit $C$ vérifiant \\eqref{eq:condC000} et d'un ordre de grandeur raisonnable pour que la relation \\eqref{defC} ne conduise pas à l'explosion de la moyenne des gradients. Nous considérons la variable $\\epsilon$ petite pour calculer $C = \\frac{m}{\\sqrt{n}}-\\epsil",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 85,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_86",
      "text": "on$. La soustraction de la valeur $\\epsilon$ permet de garantir que l'inégalité \\ref{eq:condC000} est maintenue.\n\\\\\n\\item On définit ensuite $\\delta$ par \\eqref{conddelta000}.\n\\\\\n\\item On remarque alors que \n$$\\frac{m}{\\delta^2} = \\displaystyle \\frac{2(m^2-C^2n)}{m \\, n\\, \\sigma_x}.$$ \nIl ne reste plus qu'à contrôler cette dernière quantité, en utilisant si besoin la marge de manoeuvre qu'on a sur $C$, pour que l'équation \\eqref{eq:var_retro_hyper2} assure que la variance des gradients n'explose pas et ne dégénère pas non plus.\\\\\n\\\\\n\\item Une fois l'étape précédente terminée, on a une valeur définitivement établie pour $\\delta$. Celle ci est fixé pour l'entraînement. \n\\\\\n\\item On peut alors choisir $\\mu_s$ et $\\sigma_s$ par \\eqref{eq:sysvariance}.\n\\end{itemize}\n\n\n\\vspace{1cm} \n\n\nComme mentionné précédemment, nous avons deux expressions pour définir $\\mu_s$. Afin de garantir la condition établie par \\ref{eq:cond_mu_s}, on propose donc considérer le min entre les deux expressions obtenues pour $\\mu_s$. Ainsi on pose~:\n\\begin{equation}\n    \\mu_s =\\displaystyle \\min \\left(\\mu_x + \\frac{\\delta}{m}; \\mu_x + \\sqrt{\\frac{\\delta^2}{n}-\\frac{\\sigma^2_x}{2}} \\right)\n    \\label{eq:defmin_mus}",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 86,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_87",
      "text": "\\end{equation}\n\n\n\t\n\t\n\n\n\\vspace{1cm} \n\n\t\n\tDans les paragraphes suivants sont présentées les expérimentations numériques correspondant à l'ensemble des règles d'initialisation que nous venons de développer.\n\t\n\t\n\t\n\n\t\n\t\n\t%$\\left(\\frac{c\\cdot d}{2}\\sigma^4\\right)$\n\t%$\\left(\\frac{c\\cdot d}{2}\\sigma^4+\\frac{c\\cdot\\left(d-\\epsilon\\right)}{2}\\sigma^{4}_{x}\\right)$\n\t%$\\frac{n}{2}\\left(\\sigma^{2}_{x}+\\sigma^{2}\\right)^{2}$\n\t\n\t\n\t\n\n    \\section{L'initialisation hypersphérique en pratique}\\label{expe_init} \n    \n    On rappelle ici de façon synthétique les règles d'initialisation issues des calculs des paragraphes précédents. Nous présentons ensuite un certain nombre d'expérimentations qui permettent à la fois de confirmer la pertinence de ces règles et d'illustrer les arguments mathématiques précédents. Un test sur un cas plus réaliste est présenté au paragraphe \\ref{expe_real}.\n    \n    \n    \\subsection{Résumé des règles d'initialisation}\\label{subsec:resume_init}\n    \n    Dans le tenseur de sortie, la composante $y_{j}= \\frac{\\tilde{ x}._i\\tilde{s}_j}{\\delta}$ associée à l'hypersphère $\\displaystyle \\tilde{s}_j$ correspondant à la sortie d'un neurone s'écrit~:\n\n    \\begin{equation}\n    \t\\disp",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 87,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_88",
      "text": "laystyle y_{j} = \\frac{1}{2\\delta}\\left[\\sum_{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+\\rho^{2}_{j} \\right]\n    \\end{equation}\n    \n    \\bigskip\n    \n    %On rappelle les hypothèses suivantes~:\\\\\n    \n    %\\begin{itemize}\n        %\\item Pour tout $i \\in \\{1, \\ldots, n\\}$, les $x_i$ sont des variables aléatoires indépendantes identiquement distribué, et suivent une loi normal de moyenne $\\mu_x$ et de variance $\\sigma^2_x$.   Soit $x_i \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$.\\\\\n        %\\item Pour l'initialisation les paramètre $\\rho_j$ des rayons sont constants.\\\\\n        %\\item Les paramètres $s_{ji}$ des centres des hypersphères sont également des variables aléatoires indépendantes et identiquement distribué. Ils suivent une loi normal de moyenne $\\mu_s$ et de variance $\\sigma^2_s$.   Soit $s_{ji} \\sim \\mathcal{N}(\\mu_s, \\sigma_s^2)$.\\\\\n        \n        %\\item Pour tout $i\\in \\{1, \\ldots, n\\}$ et $j \\in  \\{1, \\ldots, m\\}$, on a $var(y_j)=var(x_i)$\n        \n    %\\end{itemize}\n    \n    %\\bigskip\n    \n    %Le développement des calcules des variances de sortie, nous permet d'aboutir à une expression pour les rayons $\\rho_j$ et les variances $\\sigma^2_s$ des centres afin d'initialis",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 88,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_89",
      "text": "er les paramètres des hypersphères et de maintenir une distribution stable entre chaque sorties de couches cachées.  La résolution de l'équation \\ref{eq:eqvariance} nous montre également qu'il y a des conditions sur le choix des moyennes $\\mu_s$ pour les paramètres de centres, et également une condition nécessaire a respecter sur la valeur $\\delta$ tel que~:\\\\\n    \n    \n    \\newpage\n    \n    \n    Les conditions \\eqref{maline}, \\eqref{eq:cond_delta_mu} et \\eqref{eq:sysvariance}\n\t  nous permettent  de proposer l'algorithme d'initialisation des paramètres des hypersphère suivant pour une couche hypersphérique~:\\\\\n    \n    \n    \\begin{tcolorbox}[\n\tcolback=gray!5,\n\tcolframe=blue!75!black,\n\tfonttitle=\\bfseries,\n\t%boxrule=0mm,\n\t%notitle\n\ttitle={Initialisation des paramètres des hypersphères}\n]\t\n\t\\begin{algorithm}[H]\n\t    \\label{algo:initialisationsph}\n\t\t\\Donnees{Ensemble de données d'entrée de la couche hypersphérique $X = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_K\\}_k$ tel que $\\mathbf{x}_k \\in \\mathbb{R}^n$, le nombre d'hypersphères $m$, $\\epsilon=\\SI{1e-3}{}$. }\n\t\t \\BlankLine\n\t\t \n\t\t\\Res{Pour tout $j \\in \\{1, \\dots, m\\}$, les centres $\\mathbf{c}_j = \\left(s_{j1}, \\dots, s_{jn}\\right)$ et les r",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 89,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_90",
      "text": "ayons $\\rho_j$ des hypersphères}\n\t\t\n\t\t\n\t\t\\BlankLine\n\t\t\\textbf{Étape 1~: Déterminer la distribution des données d'entrée (ou un de ses sous-ensemble)}\\\\\n\t\t\n\t\t$$\\begin{array}{cc}\n\t\t  \\displaystyle \\mu_x  = &\\displaystyle   \\frac{1}{Kn} \\sum_{k,i} x_{ki} \\\\\n\t\t   \\displaystyle \\sigma^2_x = & \\displaystyle\\frac{1}{Kn} \\sum_{k,i} x_{ki}^2 - \\mu_x^2 \n\t\t\\end{array}$$\n\t\t\n\t\t\\BlankLine\n\t\t\\textbf{Étape 2~: Fixer $C$ puis calculer le facteur d'échelle $\\delta$}\\\\\n\t\t\n\t\t$$C = \\frac{m}{\\sqrt{n}}-\\epsilon \\quad \\textit{et} \\quad \\displaystyle   \\delta  := \\sqrt{\\frac{m^2n\\sigma_x^2}{2(m^2-C^2n)} }  $$\n\t\t\n\t\t\\BlankLine\n\t\t\\textbf{Étape 3~: Calcul de $\\mu_s$} \\\\\n\t\t$$\\mu_s :=\\displaystyle \\min \\left(\\mu_x + \\frac{\\delta}{m}; \\mu_x + \\sqrt{\\frac{\\delta^2}{n}-\\frac{\\sigma^2_x}{2}} \\right)$$\n\t\t\n\t\t\\BlankLine\n\t\t\\textbf{Étape 4~: Calcul de $\\rho$}\\\\\n\t\t$$\\displaystyle \\rho^2 := 2\\delta \\mu_x + n \\left[ (\\mu_x-\\mu_s)^2 + \\sigma_x^2 + \\sigma_s^2 \\right]$$\n\t\t\n\t\t\\BlankLine\n\t\t\\textbf{Étape 5~: Calcul de $\\sigma^2_s$}\\\\\n\t\t$$\\displaystyle \\sigma_s^2 :=  \\sqrt{ \\left(\\mu_x - \\mu_s \\right)^4 +  \\frac{2 \\delta^2}{n}\\sigma_x^2 }- \\left[ \\left( \\mu_s - \\mu_x \\right)^2 + \\sigma_x^2\\right]$$\n\t\t\n\t    \\BlankLine\n\t\t\\textbf{Éta",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 90,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_91",
      "text": "pe 6 : Initialisation des hypersphères}\\\\\n\t\t\\PourCh{$j \\in \\{1, \\dots, m\\}$} {\n\t\t\\PourCh{$i \\in \\{1, \\dots, n\\}$} {\n\t\tInitialiser les $s_{ji}$ aléatoirement tels que $s_{ji} \\sim \\mathcal{N}(\\mu_s, \\sigma_s^2)$\n\t\t}\n\t\t\\BlankLine\n\t      Les paramètres de l'hypersphère $\\tilde{s_j}$  sont définis par le centre $\\textbf{c}_j =\\left(s_{j1}, \\dots, s_{jn}\\right)$ et le rayon $\\rho$  (cf équation \\ref{eq:tildes}).\n\t\t}\n\t\t\n\t\t\n        \n        \n\t\t\n\t\\end{algorithm}\n\\end{tcolorbox}\n    \\noindent{\\bf Remarque. }  \n\tLa méthodologie d’initialisation proposée nécessite une inférence préalable couche par couche du modèle. En effet, pour pouvoir appliquer l’algorithme d’initialisation à une couche hypersphérique donnée, il est indispensable de disposer des tenseurs d’entrée associés à cette couche. Cela implique qu'il faut exécuter une passe avant (forward pass) du réseau couche par couche.\n\n\n\t\\newpage\n\t\n\tDans les sous-paragraphes suivants, on illustre la pertinence des règles d'initialisation proposées en observant la façon dont un ''signal'' est transformé au fil de ses passages dans des neurones hypersphériques. Pour des raisons de place, nous ne montrons que la distribution de données initiales",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 91,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_92",
      "text": "et la distribution à la sortie des deux premières couches. Il faut savoir que lorsque nous annonçons que les résultats sont mauvais, ils le sont dès la sortie de la quatrième couche. Lorsque nous les annonçons bons, c'est parce que nous les avons testé sur une centaine de couches.\n\tLa figure \\ref{fig:ma_enchainement} donne par exemple les résultats à la sortie de 50 couches pour~: \\\\\n\t\n\t\\begin{itemize}\n\t    \\item Une distribution normale avec des points de dimension 4\\\\\n\t    \\item Des données extraites du jeu IRIS \\cite{IRISdataset}.\\\\\n\t    \\item Une distribution composée de trois clusters de points, chacun suivant une loi normale multivariée.\n\t\\end{itemize}\n \n\\begin{figure}[htbp]\n    \\centering\n    % Deux premières figures côte à côte avec sous-titres\n    \\begin{subfigure}[t]{0.45\\textwidth} % alignement en haut\n        \\centering\n        \\includegraphics[scale=0.075]{images/plot_distrib_50_layers_2d_test_normal_data.png}\n        \\caption{Jeu de données synthétique 4D distribué selon une loi normale multivariée}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[t]{0.45\\textwidth} % alignement en haut\n        \\centering\n        \\includegraphics[scale=0.075]{images/plot_distrib_5",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 92,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_93",
      "text": "0_layers_iris_data.png}\n        \\caption{Jeu de données Iris}\n    \\end{subfigure}\n\n    \\caption{Enchaînement de couches hypersphériques}\n    \\label{fig:ma_enchainement}\n\\end{figure}\n\n\\bigskip\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[scale=0.08]{images/plot_distrib_50_layers_normal_data.png}\n    \\caption{Enchaînement de couches hypersphériques pour un jeu de données synthétique de points 4D suivant une loi normale centrée}\n    \\label{fig:enchainement_4d_normal}\n\\end{figure}\n\t\n\\bigskip\n\t\n    \n    \\subsection{Illustrations expérimentales}\n    \\label{sec:manip_init_chap1}\n    Pour les expériences qui suivent, nous utilisons un réseau de neurones à deux couches cachées hypersphériques, contenant chacune $N$ hypersphères. Lorsque $N > 1$, la sortie du neurone est obtenue en moyennant l'ensemble des produits $\\tilde{s}._i \\tilde{x}$ calculés. Il n'y a pas d'activation : nous observons simplement l'enchaînement successif des couches. Le jeu de données utilisé est un ensemble de points synthétiques de dimension 10\\,000, distribués selon une loi normale de moyenne $\\mu_x$ et de variance $\\sigma_x^2$. Les valeurs utilisées pour les expérimentations sont indiquées au-dessus de",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 93,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_94",
      "text": "s figures correspondantes.\\\\\n    \n    Dans cette section, la valeur $\\delta$ est définie par $ \\sqrt{\\frac{n}{2}\\sigma^2_x} + \\epsilon_{\\delta}$ car les conditions données par l'étude du gradient de la fonction de coût ne sont pas prises en compte. \n   \n        \\subsubsection{Expérience 1  : Montrer que $\\mu_s$ doit être différent de zéro}\n        \n        \\begin{figure}[htbp]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_zero/fig10.png}\n            \\caption{$\\mu_s = 0$, $\\sigma_x = 0.1$, $\\mu_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 1$: \n            \\\\\n            $\\sigma_{x0} = 0.099984$, $\\sigma_{x1} = 0.080297$, $\\sigma_{x2} = 0.036692$,\n            \\\\\n            $\\mu_{x0} = 1.000006$, $\\mu_{x1} = 1.075929$, $\\mu_{x2} = 1.177815$,\n            \\\\\n            $\\sigma_{s1} = 0.005229$, $\\sigma_{s2} = 0.014190$,\n            \\\\\n            $\\mu_{s1} = 0.000000$, $\\mu_{s2} = 0.000000$.}\n            \\label{fig:ma_figure10}\n        \\end{figure}\n        \n        \n        \\begin{itemize}\n            \\item \\textbf{Observations} :\n            \\begin{itemize}\n                \\item Les écarts-types \\(\\sigma_{x1}\\) et \\(\\sigma_{x2}\\) diminuent significativeme",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 94,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_95",
      "text": "nt (0.080297 et 0.036692), ce qui indique une perte de variance.\n                \\item Les moyennes \\(\\mu_{x1}\\) et \\(\\mu_{x2}\\) augmentent légèrement par rapport à \\(\\mu_{x0}\\), ce qui montre une dérive du signal.\n            \\end{itemize}\n        \\end{itemize}\n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig11.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 0.1$, $\\mu_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 1$: \n            \\\\\n            $\\sigma_{x0} = 0.099984$, $\\sigma_{x1} = 0.031161$, $\\sigma_{x2} = 0.019281$,\n            \\\\\n            $\\mu_{x0} = 1.000006$, $\\mu_{x1} = 0.998604$, $\\mu_{x2} = 0.997637$,\n            \\\\\n            $\\sigma_{s1} = 0.000011$, $\\sigma_{s2} = 0.000001$,\n            \\\\\n            $\\mu_{s1} = 1.382000$, $\\mu_{s2} = 2.150632$.}\n            \\label{fig:ma_figure11}\n        \\end{figure}\n        \n        \\begin{itemize}\n            \\item \\textbf{Observations} :\n            \\begin{itemize}\n                \\item Les écarts-types \\(\\sigma_{x1}\\) et \\(\\sigma_{x2}\\) restent proches de \\(\\sigma_{x0}\\), ce qui indique une bonne préservation de la varianc",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 95,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_96",
      "text": "e.\n                \\item Les moyennes \\(\\mu_{x1}\\) et \\(\\mu_{x2}\\) restent proches de \\(\\mu_{x0}\\), ce qui suggère une propagation plus fidèle du signal.\n            \\end{itemize}\n        \\end{itemize}\n        \n        \n        \\begin{itemize}\n            \\item \\textbf{Analyse} :\n            \\begin{itemize}\n                \\item Lorsque \\(\\mu_s = 0\\), les écarts-types et les moyennes se dégradent plus rapidement, que dans le cas où \\(\\mu_s \\neq 0\\), les écarts-types diminuent, mais les moyennes restent très proches de la valeur initiale, ce qui montre une meilleure stabilité. Il est plus intéressant de fixer les moyennes $\\mu_s$ proche de la valeur de la borne supérieur. (le cas proche de la valeur de la borne inf donne des résultats similaires).\n            \\end{itemize}\n        \\end{itemize}\n        \\subsubsection{Expérience 2 : Montrer que si $\\epsilon_{\\delta} = 0$ et $\\sigma_x = 1$ (non petit), cela ne fonctionne pas}\n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig12.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 1$, $\\mu_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 1$:",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 96,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_97",
      "text": "\\\\\n            $\\sigma_{x0} = 0.999842$, $\\sigma_{x1} = 0.593819$, $\\sigma_{x2} = 0.400592$,\n            \\\\\n            $\\mu_{x0} = 1.000060$, $\\mu_{x1} = 0.985483$, $\\mu_{x2} = 0.967814$,\n            \\\\\n            $\\sigma_{s1} = 0.000747$, $\\sigma_{s2} = 0.000302$,\n            \\\\\n            $\\mu_{s1} = 1.631132$, $\\mu_{s2} = 2.195358$.}\n            \\label{fig:ma_figure12}\n        \\end{figure}\n        \n        \\begin{itemize}\n            \\item \\textbf{Observations} :\n            \\begin{itemize}\n                \\item Les écarts-types \\(\\sigma_{x1}\\) et \\(\\sigma_{x2}\\) diminuent trop fortement (0.593819 et 0.400592), ce qui indique une perte d'information excessive.\n                \\item Malgré une moyenne relativement stable, la propagation devient moins efficace en raison de la réduction de la variance.\n            \\end{itemize}\n        \\end{itemize}\n        \n        \\begin{itemize}\n            \\item \\textbf{Analyse} :\n            \\begin{itemize}\n                \\item Lorsque \\(\\sigma_x\\) est grand, les écarts-types diminuent fortement, et le signal tend vers une loi gamma de signe inverse, ce qui montre une instabilité dans la propagation du signal. \n            \\end{it",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 97,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_98",
      "text": "emize}\n        \\end{itemize}\n        \n        \n        \\subsubsection{Expérience 3 : Influence de $N$ grand}\n        \n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig13.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 0.1$, $\\mu_x = 1$, $N = 10$, $\\epsilon_{\\delta} = 1$: \n            \\\\\n            $\\sigma_{x0} = 0.099984$, $\\sigma_{x1} = 0.031154$, $\\sigma_{x2} = 0.019243$,\n            \\\\\n            $\\mu_{x0} = 1.000006$, $\\mu_{x1} = 0.999603$, $\\mu_{x2} = 0.999568$,\n            \\\\\n            $\\sigma_{s1} = 0.000011$, $\\sigma_{s2} = 0.000001$,\n            \\\\\n            $\\mu_{s1} = 1.382000$, $\\mu_{s2} = 2.151655$.}\n            \\label{fig:ma_figure13}\n        \\end{figure}\n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig14.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 0.1$, $\\mu_x = 1$, $N = 1000$, $\\epsilon_{\\delta} = 1$: \n            \\\\\n            $\\sigma_{x0} = 0.099984$, $\\sigma_{x1} = 0.031153$, $\\sigma_{x2} = 0.019238$,\n            \\\\\n            $\\mu_{x0} =",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 98,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_99",
      "text": "1.000006$, $\\mu_{x1} = 0.999987$, $\\mu_{x2} = 0.999986$,\n            \\\\\n            $\\sigma_{s1} = 0.000011$, $\\sigma_{s2} = 0.000001$,\n            \\\\\n            $\\mu_{s1} = 1.382000$, $\\mu_{s2} = 2.152048$.}\n            \\label{fig:ma_figure14}\n        \\end{figure}\n        \n        \n        \\begin{itemize}\n            \\item \\textbf{Observations} :\n            \\begin{itemize}\n                \\item Lorsque \\(N\\) augmente (\\(N=10\\) et \\(N=1000\\)), la variance \\(\\sigma_x\\) reste bien préservée.\n                \\item La moyenne \\(\\mu_x\\) est presque inchangée, suggérant une stabilité accrue avec un grand \\(N\\).\n                \\item Une meilleure propagation du signal est observée avec un grand \\(N\\), validant son rôle bénéfique.\n            \\end{itemize}\n        \\end{itemize}\n        \n        \\begin{itemize}\n            \\item \\textbf{Analyse} :\n            \\begin{itemize}\n                \\item Lorsque \\(N\\) est grand, les écarts-types diminuent, mais les moyennes restent extrêmement proches de la valeur initiale, ce qui montre une excellente stabilité. On observe une conséquence du théorème central limite.\n            \\end{itemize}\n        \\end{itemize}\n        \n        \\subsubsectio",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 99,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_100",
      "text": "n{Expérience 4 : Influence de $\\epsilon_{\\delta}$ sur $\\mu_s$}\n        \n        \\textbf{Cas $\\sigma_x$ petit}\n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig15.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 0.1$, $\\mu_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 10$: \n            \\\\\n            $\\sigma_{x0} = 0.099984$, $\\sigma_{x1} = 0.038070$, $\\sigma_{x2} = 0.036201$,\n            \\\\\n            $\\mu_{x0} = 1.000006$, $\\mu_{x1} = 0.998295$, $\\mu_{x2} = 0.997497$,\n            \\\\\n            $\\sigma_{s1} = 0.000011$, $\\sigma_{s2} = 0.000000$,\n            \\\\\n            $\\mu_{s1} = 1.504375$, $\\mu_{s2} = 11.328631$.}\n            \\label{fig:ma_figure15}\n        \\end{figure}\n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig16.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 0.1$, $\\mu_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 1000$: \n            \\\\\n            $\\sigma_{x0} = 0.099984$, $\\sigma_{x1} = 0.094492$, $\\sigma_{x2} = 0.094444$,\n            \\\\\n            $\\mu_{x0} = 1.0000",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 100,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_101",
      "text": "06$, $\\mu_{x1} = 0.998323$, $\\mu_{x2} = 0.998099$,\n            \\\\\n            $\\sigma_{s1} = 0.000002$, $\\sigma_{s2} = 0.000000$,\n            \\\\\n            $\\mu_{s1} = 11.694160$, $\\mu_{s2} = 1001.364937$.}\n            \\label{fig:ma_figure16}\n        \\end{figure}\n        \n        \\textbf{Cas $\\sigma_x$ grand}\n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig17.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 1$, $\\mu_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 10$: \n            \\\\\n            $\\sigma_{x0} = 0.999842$, $\\sigma_{x1} = 0.613114$, $\\sigma_{x2} = 0.585020$,\n            \\\\\n            $\\mu_{x0} = 1.000060$, $\\mu_{x1} = 0.983484$, $\\mu_{x2} = 0.970680$,\n            \\\\\n            $\\sigma_{s1} = 0.000755$, $\\sigma_{s2} = 0.000068$,\n            \\\\\n            $\\mu_{s1} = 1.754479$, $\\mu_{s2} = 11.404689$.}\n            \\label{fig:ma_figure17}\n        \\end{figure}\n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig18.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 1$, $\\m",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 101,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_102",
      "text": "u_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 1000$: \n            \\\\\n            $\\sigma_{x0} = 0.999842$, $\\sigma_{x1} = 0.946954$, $\\sigma_{x2} = 0.946493$,\n            \\\\\n            $\\mu_{x0} = 1.000060$, $\\mu_{x1} = 0.983402$, $\\mu_{x2} = 0.981176$,\n            \\\\\n            $\\sigma_{s1} = 0.000171$, $\\sigma_{s2} = 0.000002$,\n            \\\\\n            $\\mu_{s1} = 12.026035$, $\\mu_{s2} = 1001.573476$.}\n            \\label{fig:ma_figure18}\n        \\end{figure}\n        \n        \\begin{figure}[H]\n            \\centering\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig19.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 1$, $\\mu_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 10000$: \n            \\\\\n            $\\sigma_{x0} = 0.999842$, $\\sigma_{x1} = 0.987210$, $\\sigma_{x2} = 0.987161$,\n            \\\\\n            $\\mu_{x0} = 1.000060$, $\\mu_{x1} = 0.994143$, $\\mu_{x2} = 0.993295$,\n            \\\\\n            $\\sigma_{s1} = 0.000020$, $\\sigma_{s2} = 0.000000$,\n            \\\\\n            $\\mu_{s1} = 102.073974$, $\\mu_{s2} = 10001.604402$.}\n            \\label{fig:ma_figure19}\n        \\end{figure}\n        \n        \\begin{figure}[H]\n            \\centerin",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 102,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_103",
      "text": "g\n            \\includegraphics[width=1.0\\textwidth]{cas_mu_x_un/fig20.png}\n            \\caption{$\\mu_s = \\mu_s^\\star - \\eta$, $\\eta \\ll 1$, $\\sigma_x = 1$, $\\mu_x = 1$, $N = 1$, $\\epsilon_{\\delta} = 100000$: \n            \\\\\n            $\\sigma_{x0} = 0.999842$, $\\sigma_{x1} = 0.991597$, $\\sigma_{x2} = 0.000009$,\n            \\\\\n            $\\mu_{x0} = 1.000060$, $\\mu_{x1} = 0.999441$, $\\mu_{x2} = 0.999449$,\n            \\\\\n            $\\sigma_{s1} = 0.000002$, $\\sigma_{s2} = 1.000000$,\n            \\\\\n            $\\mu_{s1} = 1002.079502$, $\\mu_{s2} = 0.000000$.}\n            \\label{fig:ma_figure20}\n        \\end{figure}\n        \n        \\begin{itemize}\n            \\item \\textbf{Analyse} :\n            \\begin{itemize}\n                \\item Lorsque \\(\\delta\\) est très grand (\\(\\epsilon_\\delta = 10000\\) ou \\(100000\\)), les écarts-types sont bien préservés, mais les moyennes dérivent légèrement. Cependant, dans le cas de \\(\\epsilon_\\delta = 100000\\), on observe un effondrement de l'écart-type dans la deuxième couche, ce qui indique une perte totale de variance. Cela montre que il y a une valeur limite pour \\(\\delta\\) très grand, afin de garantir la propagation du signal. \n                \\it",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 103,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_104",
      "text": "em Pour une propagation efficace du signal dans un réseau de neurones, il est essentiel de :\n                \\begin{itemize}\n                    \\item Initialiser \\(\\mu_s\\) à une valeur non nulle pour éviter la dégradation de la variance et de la moyenne.\n                    \\item Maintenir \\(\\sigma_x\\) petit permet d'assurer une propagation stable du signal.\n                    \\item Augmenter \\(N\\) pour améliorer la robustesse et la stabilité du réseau.\n                    \\item Contrôler \\(\\delta\\) pour éviter l'instabilité, surtout lorsque \\(\\sigma_x\\) est grand (on revient sur ce point au paragraphe suivant qui concerne le paramètre $q_i$).\n                \\end{itemize}\n                \\item Lorsque \\(\\delta\\) est grand, cela améliore la propagation du signal jusqu'à une valeur limite qui  peut entraîner une instabilité dans le réseau, surtout lorsque \\(\\sigma_x\\) est petit. Il est donc crucial de trouver un équilibre entre \\(\\delta\\) et \\(\\sigma_x\\) pour maintenir une propagation stable du signal.\n                \n            \\end{itemize}\n        \\end{itemize}\n        \n        \n        \\subsubsection{Le paramètre $q_i$}\n        \n        Dans chaque légende de figure (en haut",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 104,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_105",
      "text": ") apparaît un paramètre $q_i$, l'indice $i$ désignant le numéro de la couche. Celui-ci est calculé en fonction des paramètres d'entrée de la couche $i$ et des paramètres de poids calculés pour cette couche :\n        $$ q_i = \\frac{\\mu_x}{\\sigma_x} + \\frac{\\mu_s}{\\sigma_s} .$$\n        Le paramètre $q_i$ permet de vérifier si le critère \\eqref{maline} est satisfait.\n        Les figures ci-dessus, en particulier la dernière image de la figure \\ref{fig:ma_figure20} permettent de vérifier que l'utilisation du critère \\eqref{maline} peut être un garde-fou très efficace pour éviter un effondrement de la variance du signal de sortie de la couche.\\\\\n        \n        \n    \\noindent{\\bf Remarque.} Afin de mieux maîtriser la valeur de $\\delta$ et de tenir compte du nombre d'hypersphères utilisées (paramètre $m$ de l'algorithme proposé), il est insuffisant de se limiter aux conditions données par \\eqref{eq:cond_delta_mu} et \\eqref{eq:sysvariance}. C’est pourquoi nous avons également étudié l’impact de la variance de la fonction de coût, en suivant un raisonnement analogue à celui présenté dans \\cite{glorot}. Cette démarche a permis de redéfinir le paramètre $\\delta$ (cf. équation \\eqref{conddel",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 105,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_106",
      "text": "ta000}) et d’assurer la propagation du signal à travers les couches, quel que soit le nombre de neurones choisis dans la couche hypersphérique.\n\n\n            \t\n\t\n    \\section{Validation de l'initialisation sur le jeu de données  Iris }\\label{expe_real} \n    \n    Le but de cette expérience est de vérifier la propagation du signal dans un réseau, en testant un jeu de données réel, afin d’évaluer si l’algorithme proposé pour l’initialisation fonctionne correctement (voir \\ref{algo:initialisationsph}). Pour cela, nous avons construit un réseau de neurones comportant 10 couches cachées hypersphériques.\\\\\n    \n    Trois configurations ont été testées, avec respectivement 8, 64 et 4096 hypersphères par couches cachées (la valeur est indiquée dans la légende). Les modèles, partagent la même architecture générale, sans fonction d’activation ni normalisation par lot (BatchNorm). \\\\\n    Nous avons pris $\\epsilon = \\SI{1e-3}{}$ pour calculer $C = \\frac{m}{\\sqrt{n}}-\\epsilon$.\\\\ \n    \n    Les figures \\ref{fig:figaventrainnementsph8}, \\ref{fig:figaventrainnementsph64} et \\ref{fig:figaventrainnementsph4096} présentent la distribution des sorties de chaque couche cachées à l’initialisation, c’est-",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 106,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_107",
      "text": "à-dire avant l’entraînement. Les figures \\ref{fig:figapentrainnementsph8}, \\ref{fig:figapentrainnementsph64} et \\ref{fig:figaventrainnementsph4096} montrent quant à elles les distributions des sorties après entraînement.\\\\\n    \n    \\begin{figure}[H]\n\t\\begin{center}\t\t\n\t\t\\includegraphics[width=16cm]{figs/fig av_entrainnement sph8.png}\n\t\t\\caption{Distribution des valeurs de sortie des couches hypersphériques \\textcolor{blue}{avant} l'entraînement (cas de 8 hypersphères par couche cachée, pour un réseau à 10 couches cachées}\n\t\t\\label{fig:figaventrainnementsph8}\n\t\\end{center}\n\\end{figure}\n\n\n    \\begin{figure}[H]\n\t\\begin{center}\t\t\n\t\t\\includegraphics[width=16cm]{figs/fig ap_entrainnement sph8.png}\n\t\t\\caption{Distribution des valeurs de sortie des couches hypersphériques \\textcolor{red}{après} l'entraînement (cas de 8 hypersphères par couche cachée, pour un réseau à 10 couches cachées}\n\t\t\\label{fig:figapentrainnementsph8}\n\t\\end{center}\n\\end{figure}\n\n\n     \\begin{figure}[H]\n\t\\begin{center}\t\t\n\t\t\\includegraphics[width=16cm]{figs/fig av_entrainnement sph64.png}\n\t\t\\caption{Distribution des valeurs de sortie des couches hypersphériques \\textcolor{blue}{avant} l'entraînement (cas de 64 hypersphèr",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 107,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_108",
      "text": "es par couche cachée, pour un réseau à 10 couches cachées}\n\t\t\\label{fig:figaventrainnementsph64}\n\t\\end{center}\n\\end{figure}\n\n\n    \\begin{figure}[H]\n\t\\begin{center}\t\t\n\t\t\\includegraphics[width=16cm]{figs/fig ap_entrainnement sph64.png}\n\t\t\\caption{Distribution des valeurs de sortie des couches hypersphériques \\textcolor{red}{après} l'entraînement (cas de 64 hypersphères par couche cachée, pour un réseau à 10 couches cachées}\n\t\t\\label{fig:figapentrainnementsph64}\n\t\\end{center}\n\\end{figure}\n\n    \\begin{figure}[H]\n\t\\begin{center}\t\t\n\t\t\\includegraphics[width=16cm]{figs/fig av_entrainnement sph4096.png}\n\t\t\\caption{Distribution des valeurs de sortie des couches hypersphériques \\textcolor{blue}{avant} l'entraînement (cas de 4096 hypersphères par couche cachée, pour un réseau à 10 couches cachées}\n\t\t\\label{fig:figaventrainnementsph4096}\n\t\\end{center}\n\\end{figure}\n\n\n    \\begin{figure}[H]\n\t\\begin{center}\t\t\n\t\t\\includegraphics[width=16cm]{figs/fig ap_entrainnement sph4096.png}\n\t\t\\caption{Distribution des valeurs de sortie des couches hypersphériques \\textcolor{red}{après} l'entraînement (cas de 4096 hypersphères par couche cachée, pour un réseau à 10 couches cachées}\n\t\t\\label{fig:figapentrainnemen",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 108,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_109",
      "text": "tsph4096}\n\t\\end{center}\n\\end{figure}\n\nOn peut observer que à l’initialisation, les moyennes des distributions de sorties restent stables au fur et à mesure que l’on empile les couches, et que la variance n’explose pas. Cela permet de préserver le signal à la sortie du réseau.\\\\\n\nPour les trois configurations qui ont été testées (avec 8, 64 ou 4096 neurones par couche cachée), les taux d’accuracy obtenus sont respectivement de 98.3\\% pour 8 hypersphères, 98.3\\% pour 64, et 96.7\\% pour 4096. Ces résultats indiquent que l’augmentation excessive du nombre d’hypersphères nuit à la performance du modèle, probablement en raison d’un surapprentissage. On en déduit qu’il n’est pas nécessaire, voire contre-productif, d’utiliser un nombre trop important de paramètres. Cependant  les résultats restent pertinents\\footnote{Il faut garder à l'esprit qu'ici on a ajouté un nombre démesuré de paramètres pour tester les performances du processus d'initialisation...}. \\\\\nIl faut considérer un nombre suffisamment grand d'hypersphères (4096) pour observer que la méthode d'initialisation proposée contraint la distribution des sorties à tendre vers une loi normale. \n\n    \n    \n\t\\section{Conclusion}\n\t\n\tLe",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 109,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_110",
      "text": "modèle de neurone hypersphérique de Banarer \\textit{et al.} \\cite{Banarer2003} a été étendu pour définir un filtre de convolution hypersphérique adapté aux tenseurs d'ordre $l$ à plusieurs canaux. Cette extension consiste à transformer les coefficients du filtre ainsi que le biais en coordonnées sur une hypersphère, permettant ainsi l'optimisation de ces paramètres.\n\t\n\t\\bigskip\n\t\n\t\n\tLes expérimentations réalisées sur des cas simples visaient à valider et comparer ces modèles sur des architectures simples, telles que les couches denses ou Conv2d. Les résultats montrent que les taux de prédiction restent globalement similaires entre les réseaux classiques et les réseaux à couches hypersphériques pour une architecture de type Perceptron Multicouches. Il a été observé que les fonctions de perte convergent plus rapidement et atteignent des valeurs plus faibles avec les couches hypersphériques. De plus, en adaptant des réseaux convolutifs avec des filtres hypersphériques, les résultats de prédiction demeurent comparables à ceux obtenus avec des filtres classiques. Cependant, lors de la transition d'un réseau dense hypersphérique vers un réseau convolutif avec filtres hypersphériques, l'a",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 110,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_111",
      "text": "ugmentation du taux de bonne prédiction est similaire à celle observée avec les réseaux classiques. \n\t\n\t\\bigskip\n\t\n\t\nLa question de l'initialisation des poids dans les couches hypersphériques a été explorée afin de garantir la possibilité d'enchaîner plusieurs couches successives. La sortie d'une couche hypersphérique dépend de plusieurs paramètres, dont le facteur d'échelle $\\delta$ que nous avons introduit. Ce dernier joue un rôle essentiel dans la distribution des sorties, et une fois fixé, il permet de déterminer à la fois les valeurs des paramètres hypersphériques et les conditions d'initialisation : en particulier, l'intervalle auquel doit appartenir la moyenne des centres, ainsi que la variance de ces derniers lorsqu'ils sont initialisés selon une loi normale.\\\\\n\nLa méthode proposée permet non seulement de propager efficacement le signal à travers des couches hypersphériques successives, mais aussi d’augmenter le nombre de neurones par couche. Cela ouvre des perspectives intéressantes pour l'intégration des couches hypersphériques dans des architectures plus complexes. Néanmoins, l'ajustement des paramètres d'initialisation reste à affiner en fonction des différentes fonctio",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 111,
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_112",
      "text": "ns d'activation utilisées.",
      "source_file": "chapitre1.tex.txt",
      "chunk_index": 112,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_0",
      "text": "Dans ce chapitre, nous explorons l’utilisation d’une couche hypersphérique pour résoudre des problèmes de régression, en nous intéressant notamment à sa capacité d’approximation. Cette étude s’inscrit dans le prolongement naturel des travaux sur l'approximation universelle dans les réseaux neuronaux classiques. Rappelons que dans ce cadre, une question fondamentale concerne la capacité d’une couche dense, composée d’un nombre fini \\( J \\) de neurones classiques (dotés de fonctions d’activation standard comme la sigmoïde ou ), à approcher une fonction donnée. Dans ce chapitre, on s'intéresse à l'approximation d'une fonction continue à support compact $ f ^0(K) $, où $K$ est un compact de $^n$. Derrière cette question, il y a l'idée que c'est la capacité d'un réseau de neurones à approximer une grande variété de fonctions qui en fait un outil particulièrement adapté à de nombreuses tâches, telles que la classification ou la régression.\\\\ Nous adoptons ici la convention usuelle de la communauté pour l'approximation d'une fonction de $^n$ vers $$. Cependant, dans le chapitre suivant, afin d'éviter toute ambiguïté, nous réserverons la notation $n$ pour désigner la dimension de l'entrée",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_1",
      "text": "et introduirons $m$ pour la dimension du plongement. Par cohérence, nous avons choisi de garder $J$ pour le nombre de fonctions ridge aussi bien que pour le nombre de sphères.\\\\ Bien que différentes architectures de réseaux de neurones soient possibles, nous posons ici la question de savoir si une couche composée de neurones hypersphériques possède des capacités d'approximation intéressantes. Plus précisément, peut-on, grâce à une combinaison linéaire des $ J $ sorties de ces neurones, approximer avec une précision arbitraire toute fonction $ f ^0(K)$, $K$ compact de $^n$, au sens de la norme uniforme ?\\\\ Pour répondre à cette question, nous commençons par rappeler quelques notions concernant le théorème d’approximation universelle dans le cadre classique des réseaux à une couche cachée. Dans ce cadre, le théorème est dit universel : il garantit la capacité d’un réseau dense, composé d’un nombre fini de neurones classiques avec des fonctions d’activation appropriées, à approcher toute fonction continue définie sur un compact de $^n$. L'approche classique pour démontrer le théorème d'approximation universelle repose sur des propriétés et des méthodes analytiques qui ne peuvent pas t",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_2",
      "text": "outes être directement appliquées dans le contexte des neurones à couches hypersphériques.\\\\ Cela nous a conduit à adopter une approche différente. Plus précisément, nous établissons un théorème d’approximation pour les fonctions continues définies sur un compact de $$, en nous appuyant sur un théorème de Schwartz. Ce dernier exploite les combinaisons linéaires de translations. Cette approche, simple et efficace, présente ici une limitation notable : elle n’est pas universelle au sens large, car elle impose que les fonctions à approcher soient scalaires.\\\\ \\\\ Ce paragraphe est un rappel sur les propriétés d’approximation bien établies des réseaux de neurones denses à une couche cachée. Nous y rappelons les conditions permettant de démontrer que ces réseaux peuvent approximer toute fonction continue \\( f ^0(K) \\), où \\(K\\) est un compact de \\(^n\\), au sens de la norme uniforme. Ces résultats posent une base essentielle pour ensuite discuter des capacités d’approximation des neurones dans des architectures non conventionnelles. \\\\ Dans cette perspective, tout d’abord la sortie d’un réseau de neurones dense à une couche cachée est décrite, ce qui permet d’illustrer le rôle des combina",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_3",
      "text": "isons linéaires des sorties des neurones, avec application d’une fonction d’activation, dans le processus d’approximation. Cette formalisation conduit naturellement à introduire la notion de fonction . Les fonctions permettent, en effet, de simplifier l’analyse en réduisant la question d’approximation à l’étude de propriétés de fonctions unidimensionnelles. Enfin, cette section pose la question de la densité des espaces fonctionnels engendrés par les sorties de réseaux, c’est-à-dire de déterminer si ces espaces sont suffisamment riches pour permettre une approximation uniforme de toute fonction continue définie sur un compact donné. \\\\ {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!80]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,3} (K-) at (0,-1.8-3) {$x_{}$}; / in {1,...,5} (J-) at (2,-1.8-1.5) { $ _{}. + b_{}$}; / in {1,...,5} (K-1) edge (J-); / in {1,...,5} (K-2) edge (J-); / in {1,...,5} (K-3) edge (J-); (I) {$ _{j=1}^{5}_j(_{j}.+b_{j})$}; in {1,...,5} (J-) edge (I); ^3$ et à valeur dans $$} Dans le cas classique des",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_4",
      "text": "couches denses, le schéma correspond à l'architecture du réseaux de neurone considéré. On notera \\[ = (x_1, , x_n) ^n \\] les valeurs d'entrée, et \\[ H()=(_1 + b_1, , _J + b_J) ^J \\]\\\\ est le vecteur composé des éléments de sortie de chaque neurone de la couche cachée avant application de la fonction d'activation $ : $. On peut écrire la sortie d'un réseau de $J$ neurones en une couche cachée comme suit~: \\\\ y() = _{j=1}^{J} _j (_j + b_j) , \\\\ les \\( _j = (w_{j,i})_{i=1}^n ^n \\) (respectivement \\( b_j \\)), \\( j = 1, , J \\), étant des vecteurs poids (respectivement des biais). Les coefficients $_j$ de la combinaison linéaire sont également des scalaires. Le produit scalaire de deux vecteurs \\( \\) et \\( = (x_i)_{i=1}^n \\) de \\( ^n \\) est noté \\( \\) et défini par~:\\\\ \\[ = _{i=1}^{n} w_i x_i. \\]\\\\ Si on considère une fonction donnée, supposée à valeur scalaire, \\( f : ^n \\), alors, dans l'idéal, le but est d'écrire la fonction $f$ sous une forme correspondant à la structure d'un réseau de neurones~: \\[ f() = _{j=1}^{J} _j (_j + b_j) = y(). \\] Comme la structure du réseau est fixée, les inconnues sont la dimension \\( J \\) de la couche cachée ainsi que les paramètres \\( _j, _{j}, b_j \\),",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_5",
      "text": "\\( 1 j J \\), qui dépendent de la fonction \\( f \\). Il faut noter cependant que la fonction $$ ne doit pas dépendre de $f$, sinon cela impliquerait que la fonction d'activation change selon la fonction à approcher. L'égalité ci-dessus pour une classe infinie de fonctions est donc ``difficile\" à obtenir. On cherche plutôt une approximation.\\\\ En reprenant l’écriture de la sortie d’un réseau de neurones, il est possible d’observer que, pour tout choix de \\( w_{j,i} \\) et \\( b_j \\), les termes \\( (_{j} + b_j) \\) apparaissant dans la somme peuvent être identifiés comme des fonctions , définies selon la définition suivante~: [Pinkus ] Une fonction est une fonction à plusieurs variables $h : ^n $ de la forme )$} où $g : $ et $= (a_1, , a_n) ^n $. Il est possible d’écrire la sortie du réseau sous la forme d'une combinaison linéaire de fonctions ridge~:\\\\ $${ll} y( ) & = _{j} _{j} ( _{ j} _{}+b_{j})\\\\ \\\\ & := _{j} _{j} g_j(_j )\\\\ \\\\ & \\{g() : ^n \\} $$ en posant $g_j(x):=(x+b_j)$. On voit en particulier que la sortie du réseau appartient à un espace vectoriel engendré par des fonctions ridge. Il faut donc voir quel type de fonction peut être approchée efficacement par cet espace vectoriel en",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_6",
      "text": "gendré. C'est l'objet du paragraphe suivant. Dans son article \"Approximation theory of the MLP model in neural networks\", A. Pinkus effectue un état de l'art sur la question de densité qui consiste à montrer que l'ensemble () := \\{ ( - b_{}): b , ^n\\}$} est dense dans ^0(^n)$}. L’objectif est d’approcher toute fonction continue sur un compact par une combinaison linéaire de fonctions . En terme de densité, il s’agit de montrer que, toute fonction \\( f ^{0}( ^n) \\) appartient à l'adhérence de $()$~:\\\\ f \\{( + b): ^n, b\\}}= ()}, l'adhérence $()}$ étant considérée au sens de la norme qu'on choisit pour mesurer la qualité de l'approximation de $f$ par une fonction de $()$ : on utilise dans ce chapitre la norme uniforme}$ est la notation pour désigner l'adhérence au sens de la norme uniforme}. \\\\ Les étapes du raisonnement détaillé dans sont les suivantes~:\\\\ Étape 1~: Se ramener au cas unidimensionnel. Cette première étape correspond à la proposition 3.3 de l'article de Pinkus (cf. Annexe ) Étape 2~: Montrer que tout élément ^{0}(K)$}, peut être approché par une combinaison linéaire d'éléments de $(,,)$ où (,,) = vect\\{( -)~: , \\, \\} avec $^{}()$, $ $ un sous-ensemble de $$ contenant u",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_7",
      "text": "ne suite tendant vers 0 et $ $ un intervalle ouvert quelconque de $$. Cette étape correspond à la proposition 3.4 de l'article de Pinkus (cf. Annexe ). Étape 3~: Réduire l'hypothèse de régularité sur l'activation. A l'étape 2, on a supposé $^{}()$. Ici on montre qu'on peut se contenter de l'hypothèse $^{0}()$ (cf. Annexe ). Cela permet d'établir le théorème suivant~:\\\\ [colframe=blue, colback=white!10, title=] Soit $ ^{0}()$. Alors, $()$ est dense dans $^{0}( ^{n})$ au sens de la convergence uniforme sur un compact si et seulement si $$ n'est pas un polynôme. La démonstration de Pinkus est détaillée en annexe car elle est très différente de celle que nous allons construire dans la suite. Au delà de la référence mentionnée ci-dessus de Pinkus , la question de l'approximation universelle d'une fonction par un réseau dense a été très largement étudiée. On retiendra par exemple les éléments suivants : Pour un nombre de couches donn\\'ees, un nombre de neurones par couche $m$ potentiellement très grand ($m$) : l'approximation universelle est \\'etablie pour toute activation continue qui ne soit pas un polyn\\^ome (condition n\\'ecessaire et suffisante, travaux de Cybenko et Hornik, 1989 , v",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_8",
      "text": "oir aussi mentionné ci-dessus) ; Pour un nombre de couches qui tend vers l'infini mais $n+1$ neurones par couche : l'approximation universelle est \\'etablie avec l'activation pour toute fonction $f$ Lebesgue int\\'egrable (Lu et al. 2017 ; le nombre de neurones par couche peut \\^etre diminu\\'e si $f$ est continue) ;\\\\ Pour un nombre de couches qui tend vers l'infini mais $n+3$ neurones par couche : l'approximation universelle est \\'etablie avec une activation non affine pour toute fonction $f$ continue ; Pour un cadre r\\'ealiste, avec un nombre de couches et de neurones par couche fix\\'es : Ismailov et Guliyev ont construit une fonction d'activation, du type sigmo\\\" de (donc croissante de 0 \\`a 1 et $^$) qui permet d'atteindre l'approximation universelle par un r\\'eseau \\`a 3 couches et $2n+2$ neurones par couche si $n 2$ (et 2 couches \\`a 2 neurones si $n=1$). La preuve est bas\\'ee sur le th\\'eor\\`eme de superposition de Kolmogorov ; l'activation est construite algorithmiquement. Pour l'approximation d'une classe beaucoup plus large de fonctions avec des activations dans des r\\'eseaux de taille contr\\^ol\\'ee, on pourra consulter , ou encore (toujours une taille contr\\^ol\\'ee, mais",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 8,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_9",
      "text": "une activation beaucoup plus complexe) . Dans cette section, nous abordons la question de l'approximation des fonctions continues par des réseaux de neurones à couche hypersphérique, en examinant une approche différente de celle discutée précédemment. \\\\ Nous nous appuyons sur le théorème de Schwartz , qui établit que des fonctions continues à support compact peuvent être approximées de manière uniforme par des combinaisons linéaires de translatées d'une fonction donnée. Cette idée est particulièrement pertinente dans le contexte des réseaux de neurones, car les sorties peuvent être assimilées à une combinaison de fonctions translatées. En ce sens, l'application de ce théorème permet de montrer que ces réseaux peuvent effectivement approximer des fonctions continues sur des compacts de $$, à travers des combinaisons linéaires de translatées de fonctions spécifiques. De façon analogue à l'équation , la sortie du réseau à couche hypersphérique à $J$ neurones (donc $J$ sphères en dimension $n$) peut être réécrite comme~:\\\\ y() = _{j=1}^{J} _{j}{2} _{i=1}^{n} )}_{(-{2}\\|-\\|^2_{}+{2}_j^2)} \\\\ où $_j $, $_j ^n$, $_j $, $1 j J$.\\\\ On choisit de construire une approche plus directe que la",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 9,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_10",
      "text": "stratégie utilisée pour le réseau à couche dense du paragraphe précédent. Le prix à payer est de se restreindre au cas des fonctions à une variable ($n=1$).\\\\ Il s'agit de montrer si un réseau composé d'une couche cachée hypersphérique permet l'approximation de fonctions de \\( \\) vers \\( \\). La sortie du réseau à couche hypersphérique dans le contexte où l'on considère les fonctions scalaires s'écrit, avec une entrée à une dimension, de la façon suivante~: y(x) = _{j=1}^{J} _{j} ( -{2} ) avec $_j $, les centres $c_j $, les rayons $_j $, $1 j J$.\\\\ L'architecture du réseau de neurones considéré ($J$ sphères en dimension 1) peut se résumer par le schéma . [H] [shorten >=1pt,->,draw=black!50, node distance=2.5cm] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!120]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] (K-1) at (-2,-1) {$x_1=:x$}; (K-3) at (-2,-3+1/2) {$1$}; (K-4) at (-2,-4+1/2) {${2}$}; / in {1,...,5} (H-) at (0,-+1) { $_{} _i$}; in {1} in {1,...,5} (K-) edge (H-); in {3,4} in {1,...,5} (K-) edge [dashed,red] (H-); (J) at (4,-2) { $y_i = _{j=1}^5 _j(_j._i",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 10,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_11",
      "text": "_i)$}; in {1,...,5} (H-) edge (J); La question de densité dans le cas considéré consiste à montrer qu'une combinaison linéaire de sorties de neurones d'un couche hypersphérique peut approcher l'ensemble des fonctions continues à support compact au sens de la convergence uniforme. Autrement dit, l'ensemble $ \\{(-{2});c, ^{*}\\}$ est-il dense dans l'ensemble des fonctions continues à support compact, soit \\{(-{2});c, ^{*}\\}}= ^0() , pour toute activation $ ^0()$, au sens de la norme uniforme sur les compacts ? On commence par énoncer le théorème suivant, dû à Schwartz {Schwartz}~:\\\\ [colframe=blue, colback=white!10, title=] {Schwartz} Si $$ et $$ sont deux fonctions continues à supports compacts, $ 0$, alors $$ est limite uniforme sur tout compact de combinaisons linéaires des translatées de $$. Un énoncé équivalent, où on reconnaît déjà la structure de la sortie d'un réseau de neurones, est le suivant. Soit \\( \\) un compact de \\( \\). Il existe donc des couples \\( (a_j, _j) ^2 \\) pour \\( j \\), tels que~: _{J + } _{x K} (x) -^{J}_{j=1} _j (x-a_j)=0 . \\\\ Soit $:=\\{(x-a), a\\}$. Chaque élément de la somme dans appartenant à $$, la limite de cette somme pour $J +$ est dans l'adhérence $}$",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 11,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_12",
      "text": "de $$. Cette limite $$ étant par ailleurs dans $C^0_c()$ ^0_c()$ l'ensemble des fonctions continues à support compacts}, on déduit du théorème de Schwartz que $$C^0_c() }$$ Tout élément de $$ étant une fonction continue à support compact alors $ C^0_c()$. Il suit que $} C^0_c()$. Cette inclusion réciproque conduit à \\{(x-a); a\\}}= ^0_c() Pour établir un théorème d'approximation dans notre cas spécifique, on va appliquer cette propriété à deux cas distincts~: Les rayons des sphères sont fixés et égaux. Les rayons des sphères ne sont pas fixés (cas général). On commence par le cas des rayons fixés. Pour $ $ donné, on introduit la fonction $_{}$ définie sur $$ par~: $$_{}(x)= (-{2}).$$ Une sortie hypersph\\'erique centr\\'ee en $c $ s'\\'ecrit comme une translat\\'ee de $_$ : $$_{}(x-c) = (-{2})$$ On suppose que $^{0}_{c}()$ et $ 0$, le Th\\'eor\\`eme 1 s'applique et assure l'existence de $(c_j,_j) ^2$, $j $, tels que :\\\\ $$ _{J} _{x K} (x) -^{J}_{j=1} _j (-{2}) =0 $$ ce qui permet de conclure en reprenant un raisonnement analogue à l'obtention de l'équation que~: pour tout $ $ donné _}= ^0_c() où $_ = \\{(-{2});c\\}$. [{ Lien avec les fonctions à bases radiales (RBF)}] Ici on peut faire le l",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 12,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_13",
      "text": "ien entre un réseau à couche hypersphérique (RHS) et un réseau de fonctions de base radiales (RBFN). La sortie d'un RBFN s'exprime comme : y() = _{j}^J _j (_j \\| - _j\\|) La fonction $$ est dite à base radiale car elle ne dépend que de la distance euclidienne entre $$ et $$ dans $^n$. Pour rapprocher les sorties des deux types de réseaux, les sphères doivent être dégénérées (\\( _j = 0 \\)) en points. L'activation qui suit la couche hypersphérique joue un rôle analogue à celui du noyau dans les fonctions à base radiale. La sortie d'un RHS pour l'activation $ (x) = x $ (resp. $ (x) = e^x $) est par exemple proportionnelle à la sortie d'un RBFN en utilisant le noyau quadratique $ (x) = x^2 $ (resp. le noyau $ (x) = e^{-{2}x^2} $). { En prenant $=0$ dans le résultat précédent, on re-prouve donc le fait que le RBFN est également un approximateur universel en dimension 1 (conformément à ).} Passons au cas g\\'enéral où les rayons ne sont pas fixés. Soit $$ une fonction continue à support compact. D'après ce qui précède en choisissant $ =0$ dans , elle peut être utilisée comme fonction d'activation pour approcher $f$ sous la forme $$ f(x) _{j=1}^{J_1} _j ( (x-c_j)^2 ) $$ pour certains $J_1 $",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 13,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_14",
      "text": ", centres $c_j $ et coefficients $_j $, $1 j J_1$. Par ailleurs, $$ peut aussi être approchée { via} des translat\\'ees de l'activation $$ grâce au théorème de Schwartz ( On pose $X_j = (x-c_j)^2 $): $$ (X_j) _{j'=1}^{J_2} _{j'} ( -{2} (X_j - _{j'}^2) )$$ pour certains $J_2 $, rayons $_{j'} $ et coefficients $_{j'} $, $1 j' J_2$. En assemblant ces deux dernières approximations, on obtient $$ f(x) _{j=1}^{J_1} _j _{k=1}^{J_2} _k ( -{2} ((x-c_j)^2 - _k^2) ) $$ qui peut se ré-écrire plus simplement $$ f(x) _{j=1}^{J} _j ( -{2} ((x-c_j)^2 - _j^2) ) .$$ Finalement on a donc établi le théorème suivant~: \\\\ [colframe=blue, colback=white!10, title=] Soit $ _c^{0}()$ une fonction d'activation continue à support compact non identiquement nulle. Alors, $_ = \\{(-{2});c\\}$ pour tout $ $, et $_{c} = \\{(-{2});c, \\, \\}$ sont denses dans $^{0}()$ au sens de la convergence uniforme sur un compact. Le théorème précédent a été démontré pour une activation $ C^0_c() $. Mais cette hypothèse peut être relaxée. Le résultat se généralise par exemple au cas où $$ L^1()$$ pour peu qu'on remplace la norme uniforme par la norme $L^1$. En effet, pour le prouver, il suffit d'utiliser le fait que l'espace de Lebes",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 14,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_15",
      "text": "gue $L^1$ est par définition l'adhérence de l'ensemble des fonctions continues à support compact pour la norme $L^1$ et le résultat du théorème précédent. Les expérimentations menées n'ont pas pour but d'offrir une étude exhaustive des capacités d'approximation des réseaux de neurones, mais plutôt de valider empiriquement les résultats théoriques établis précédemment. Dans cette optique, nous présentons ci-dessous les résultats obtenus pour différentes fonctions tests. Afin de vérifier expérimentalement la conclusion établie dans la partie précédente (Théorème ), l'approximation de fonctions continues à support compact à une variable a été testée. Pour cela, des réseaux de \\( J \\) neurones sur une couche cachée de type hypersphérique ou dense ont été utilisés. Pour chaque réseau, on a testé des valeurs de $J$ de 2 à 64. Comme fonction de perte, l'erreur quadratique moyenne (EQM) a été utilisée, et le jeu de données (composé de 512 points) a été séparé selon les proportions suivantes : \\( {2} \\) pour l'entraînement, \\( {4} \\) pour la validation et \\( {4} \\) pour le test. Les données ont été échantillonnées sur l'intervalle \\( [-0.05, 1.05] \\). L'erreur finale a été évaluée sur le je",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 15,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_16",
      "text": "u de test.\\\\ Dans les tests numériques qui vont suivre, différentes fonctions d'activation ont été testées puis sélectionnées et appliquées aux neurones de la couche cachée (cf. Fig )~: (sans activation)} $$ ~: & \\\\ x & x $$ (Rectified Linear Unit)} $$ ~: & [0, + \\\\ x & (0, (1, {5} + {2})) $$ (fonction de base radiale)} $$ ~: & ]0,1] \\\\ x & e^{-x^2} $$ )} $$ ~: & ]-1,1[ \\\\ x & }{e^x + e^{-x}} $$ } $$ ~: & ]0,2[ \\\\ x & (x) + 1 $$ } $$ ~: & [-1,1] \\\\ x & (x) $$ _{x} \\)} $$ ~: & \\\\ x & ( ._i }{}) e^{-( ._i }{})^2}, . $$ La figure montre l'évaluation des fonctions d'activation sur le produit interne $._i$, en considérant une sphère $$ de centre 0 et de rayon 1 et en faisant varier $x$ sur l'intervalle $[-,]$. En pointillé, sont représentées les fonctions d'activations usuelles. La courbe verte correspondant à l'activation \"linear\" montre le produit interne $._i$, c'est-à-dire la sortie du neurone sans activation. [htpb] $} Les preuves précédentes ont été construites en commençant par utiliser des fonctions d'activation à support compact. Dans le but de se rapprocher au plus près des conditions, l'attention a été portée sur les fonctions \\( L^2 \\) qui s'annulent au bord d'un compact sur",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 16,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_17",
      "text": "lequel on souhaite approcher une fonction. C'est pourquoi la fonction \\( (x) := (x) + 1 \\) a été ajoutée. De même, la fonction \\( _{xd} \\) a été introduite, possédant la propriété de conserver le signe de \\( ._i \\). Enfin, la fonction sinus a été incluse comme non-linéarité, car elle permet de modéliser efficacement des signaux complexes . Dans un premier temps, l'objectif a été d'approcher deux fonctions simples construites pour être à support compact : la fonction Triangle, facilement approchable par une couche dense, et la fonction Omega, correspondant à l'activation d'une couche à un neurone hypersphérique. Dans un deuxième temps, deux fonctions plus complexes ont été étudiées : la fonction \\( \\), qui est la combinaison linéaire (+ ) de 32 sphères aléatoires, et la fonction \\( ({x}) \\), qui oscille fortement autour de 0. Chaque fonction à approcher a été mise à 0 en dehors de l'intervalle \\( K=[0, 1] \\). La figure illustre les fonctions considérées.\\\\ [H] \\\\ La figure montre l'approximation des fonctions Triangle et Omega. La légende indique qu'il est nécessaire de monter à 64 neurones pour bien approximer également pour les intervalles $[-0.05. 0]$ et $[1, 1.05]$, c'est-à-dir",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 17,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_18",
      "text": "e en dehors du compact $K=[0,1]$. [htbp] Il est à noter que le modèle de neurones à couche hypersphérique permet de réduire les erreurs au bord, mais présente quelques oscillations sur les parties purement linéaires de la fonction. Cependant, avec la fonction d'activation \\( _{x} \\), une EQM de \\( 8 10^{-6} \\) est obtenue, inférieure à l'EQM maximale obtenue dans le cas des réseaux à couche dense, qui est égale à \\( 2 10^{-5} \\). \\\\ La difficulté d'entraîner les modèles à couche hypersphérique est également notable. En effet, pour la fonction Omega, construite à partir du produit interne avec une hypersphère suivi d'une activation , on pourrait s'attendre à ce qu'un seul neurone suffise pour approximativement retrouver les paramètres de l'hypersphère. Cependant, cela n'est pas le cas, et il faut au moins deux neurones (deux hypersphères) pour obtenir une approximation efficace.\\\\ Les graphiques et représente les EQM, en fonction du nombres de neurones pour les fonctions plus complexes. Les courbes continues représentent les résultats obtenus avec les modèles de neurones à couche hypersphérique, tandis que les courbes en pointillés représentent les résultats obtenus avec les modèles",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 18,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_19",
      "text": "à couche dense.\\\\ [H] [H] Les EQM restent dans les mêmes ordres de grandeur, quel que soit le modèle utilisé. On peut observer que pour l'approximation de $Sph32$, l'utilisation de la fonction d'activation $sinus$ permet d'obtenir de bons résultats avec peu de neurones. Cependant, que ce soit pour les modèles à couche dense ou hypersphérique, en augmentant suffisamment le nombre de neurones, l'utilisation de la fonction d'activation $Gauss_{x}$ a permis de réduire les erreurs obtenues pour l'ensemble des fonctions que nous avons approximées.\\\\ L'approximation de fonctions a également été testée en fixant le rayon \\( \\) des sphères. Aucun changement significatif n'a été noté sur les résultats obtenus, les EQM restant toujours dans les mêmes ordres de grandeur.\\\\ Dans ce chapitre, on a choisi de faire un focus sur l'approximation de fonctions scalaires. On sait en effet (voir par exemple le Th. 3.2 dans Pinkus ) qu'un résultat d'approximation des fonctions de $$ dans $$ peut être étendu aux fonctions de $^n$ dans $^{n'}$ avec $n,n' $ quelconques. \\\\ Les résultats expérimentaux permettent de confirmer le résultat théorique suivant : un réseau de neurones à une couche hypersphérique e",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 19,
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_20",
      "text": "st capable d'approcher en norme uniforme sur tout compact toute fonction continue de \\( \\) à support compact. Une nouvelle fonction d'activation \\( }_{x} \\) a été proposée. Il a également été vérifié que l'approximation est possible, malgré le fait que les fonctions et ne soient pas à support compact (mais \\( L^2 \\)), ce qui correspond au résultat théorique du paragraphe (on rappelle que la convergence en norme $L^1$ implique la convergence presque partout).\\\\ Les expériences menées comparent aussi les performances des approximations obtenues avec des couches hypersphériques et des couches denses classiques. \\\\ Par rapport à la démonstration présentée dans l’article , nous avons proposé une approche simplifiée en allégeant certains aspects techniques de la preuve initiale. Les résultats théoriques comme les expérimentations ont permis d'illustrer les qualités d'approximation universelle des réseaux hypersphériques.",
      "source_file": "chapitre2.tex.txt",
      "chunk_index": 20,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_0",
      "text": "Après un bref état de l'art sur les méthodes de d\\'etection d'anomalies, ce chapitre explore l'application de couches hypersphériques dans les réseaux neuronaux pour ce type d'application, en mettant l'accent sur les techniques de Support Vector Data Description (SVDD) et de Deep SVDD. Une adaptation du Deep SVDD est introduite en incorporant après la dernière couche linéaire une couche hypersphérique définie dans l'algèbre géométrique conforme. De plus, une nouvelle méthode appelée Deep M-SPH SVDD est proposée afin d'étendre l'approche à des multi-sphères, permettant ainsi au modèle de capturer des groupes distincts de points de données normales.\\\\ De nouvelles fonctions de perte conçues pour éviter l'intersection et l'inclusion des sphères sont également présentées. \\\\ Des expériences préliminaires sont menées sur un ensemble de données synthétiques, ainsi que des évaluations sur les ensembles de données MNIST et CIFAR-10. Ces comparaisons sont utilisées pour évaluer la performance de la méthode proposée et déterminer s'il est préférable, pour un nombre fixe de paramètres, d'utiliser une seule hypersphère de haute dimension ou plusieurs hypersphères de dimension inférieure. La dé",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_1",
      "text": "tection d'anomalies est une \\'etape souvent cruciale dans l'exploration de données, y compris pour la phase de pr\\'eparation d'un \\'eventuel apprentissage. Le but est d'identifier des points de données significativement différents des autres dans un ensemble, souvent appelés valeurs aberrantes ou anomalies. Ces points de données ne se conforment pas au modèle de distribution ``habituel'', ce dernier \\'etant d'ailleurs difficile \\`a d\\'efinir. Dans certains cas, le concept de valeurs aberrantes est distingué de celui des anomalies . En effet, une valeur aberrante englobe à la fois le bruit et l'anomalie. La détection de nouveauté consiste à repérer des motifs ou comportements inhabituels par rapport à la distribution observée des données d'apprentissage. La détection d'anomalies pose des défis notables. En effet, les anomalies sont souvent imprévisibles jusqu'à ce qu'elles se produisent, interdisant d'exploiter le contexte. { A contrario}, la normalité d'une donnée peut varier en fonction du contexte. L'anomalie est ainsi difficile \\`a d\\'efinir intuitivement. Si on ambitionne une d\\'etection automatique, d'autres difficultés s'ajoutent. Par exemple, en raison de leur rareté, faible",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_2",
      "text": "proportion et diversité, il peut parfois être impossible d'étiqueter les anomalies. Le déséquilibre entre les classes complique également la détection.\\\\ Il existe plusieurs approches pour la détection d'anomalies. Les différentes méthodes, y compris celles basées sur l'apprentissage profond, sont passées en revue dans . L'article les classe selon trois approches principales : l'apprentissage profond pour l'extraction de caractéristiques, l'apprentissage de représentations de la normalité et l'apprentissage de scores d'anomalies.\\\\ Dans la catégorie de l'extraction de caractéristiques, on retrouve des méthodes visent à réduire les données de haute dimension en représentations caractéristiques de basse dimension. On suppose que ces représentations extraites préservent les informations discriminantes pour aider à séparer les anomalies des données normales, même si le modèle original n'a pas été entraîné pour détecter les anomalies. Une approche courante consiste à utiliser des réseaux pré-entraînés tels que VGG ou RESNET puis transférer les sorties vers un détecteur d'anomalies tels que SVM ou SVDD .\\\\ L'apprentissage de représentations de la normalité vise à capturer les schémas ré",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_3",
      "text": "guliers suivis par les données, en utilisant des méthodes comme les autoencodeurs et les réseaux antagonistes génératifs (GAN). Par exemple, les méthodes basées sur la reconstruction apprennent à reconstruire les données normales et détectent les anomalies en mesurant la différence entre l'entrée et la sortie. L'apprentissage de scores d'anomalie apprend directement les scores d'anomalie via des réseaux neuronaux, résumant les instances normales avec un modèle discriminant à une classe.\\\\ Les m\\'ethodes d'apprentissage profond doivent permettre de relever les défis de la détection d'anomalies tels que des faibles taux de rappel, des anomalies complexes, la haute dimensionnalité et le déséquilibre des données. Elles ont en particulier l'avantage de pouvoir intégrer des sources de données diverses pour une solution complète. Nous présentons ici un bref état de l'art des méthodes de détection d'anomalies, en mettant l'accent sur la définition d'anomalie propre à chacune des méthodes. Cette méthode de détection d'anomalie repose sur l'hypothèse que les données normales suivent une distribution normale multivariée, tandis que les anomalies s'écartent de cette distribution. Il s'agit don",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_4",
      "text": "c d'une méthode paramétrique qui se base sur une estimation robuste de la matrice de covariance des données (Minimum Covariance Determinant) . Rappelons que la matrice de covariance est une mesure de la dispersion des données autour de leur moyenne, et qu'elle est utilisée pour déterminer la forme de la distribution des données. La distance de Mahalanobis est définie par la formule suivante~: $$D_M(, ) = - )^ ^{-1} ( - )}$$ où $$ est le point à comparer, $$ est la moyenne du groupe de points, et $$ est la matrice de covariance du groupe de points. On remarque que cette distance tient compte de la corrélation entre les variables, contrairement à la distance euclidienne ($ = I$).\\\\ L'algorithme recherche un sous-ensemble de $h$ points qui minimise le déterminant de la matrice de covariance, ce qui revient à identifier la région la plus compacte et donc la plus représentative de la distribution principale des données -- celle où la densité de points normaux est la plus élevée. Pour garantir une estimation robuste et statistiquement valide, la taille $h$ du sous-ensemble doit satisfaire la condition suivante : $$ h ( (1 - )K ,\\; {2} ) $$ où $K$ est le nombre d'observations en dimension",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_5",
      "text": "$m$ et $$ la proportion maximale d'anomalies que l'on souhaite tolérer. Plusieurs sous-ensembles de taille $h$ sont générés aléatoirement, et chacun est raffiné par itérations successives afin d'approcher la configuration minimisant effectivement le déterminant. Pour chaque ensemble: La matrice de covariance et la moyenne sont calculées, et la distance de Mahalanobis est calculée pour chacun des $h$ points. Parmi les $n$ points initiaux, les $h$ points ayant plus petites distances sont choisis pour former un nouveau sous-ensemble. On remarque qu'un algorithme de partitionnement est suffisant pour cette étape; il n'est pas nécessaire d'effectuer un tri complet. Les étapes 1. et 2. sont répétées jusqu'à ce que le déterminant de la matrice de covariance ne diminue plus ou est nul. Le sous-ensemble final est considéré comme robuste. Le sous-ensemble final est celui qui minimise le déterminant de la matrice de covariance.\\\\ Une version rapide de l'algorithme, appelée FastMCD , accélère encore le processus de recherche par l'utilisation de deux heuristiques: \\'Elagage précoce des sous-ensembles les moins prometteurs après deux étapes de l'algorithme précédent. Lorsque le nombre de point",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_6",
      "text": "s est grand (i.e.\\ supérieur à 600 en 1999), une stratégie de type ``diviser pour régner'' a été proposée afin de repartir la recherche de sous-ensembles support avant d'en fusionner les meilleurs et de faire un raffinement final. Le score d'anomalie est calculé à partir de la distance de Mahalanobis. Les points dont la distance dépasse un seuil prédéfini sont considérés comme des anomalies potentielles. Cette méthode est sensible aux données aberrantes et aux violations de l'hypothèse de normalité multivariée. Elle est implémentée dans la librairie scikit-learn sous le nom Elliptic Envelope. L'algorithme Isolation Forest (iForest) est conçu pour détecter les anomalies dans un ensemble de données en utilisant une approche basée sur l'isolement. La méthode commence par construire un ensemble d'arbres d'isolation (iTrees), chacun étant un arbre de décision binaire qui partitionne aléatoirement les données.\\\\ Pour chaque arbre d'isolation, l'algorithme sélectionne aléatoirement un attribut (une caractéristique des données) et une valeur de séparation parmi les valeurs possibles de cet attribut (entre le minimum et le maximum). Ce découpage crée deux sous-groupes de données, que l'on c",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_7",
      "text": "ontinue à partitionner récursivement jusqu'à ce que chaque point de donnée soit isolé. Le nombre de partitions nécessaires pour isoler un point $x$ est égal à la longueur $h(x)$ du chemin de la racine de l'arbre à ce point. Les anomalies, étant plus rares et séparées des autres points, seront en moyenne isolées plus rapidement, ce qui se traduit par des longueurs de chemin plus courtes. À partir d'un certain nombre d'arbres construits, l'algorithme estime la longueur moyenne \\( E(h(x)) \\) du chemin pour chaque point $x$ dont il résulte un score d'anomalie suivant: $$ (x) = 2^{-{c(K)}} $$ où $c(K) = 2H(K-1)-{K}$ représente la longueur moyenne du chemin d'une recherche infructueuse dans un arbre binaire équilibré avec $K$ points avec $H(i)$ le i-ème nombre harmonique.\\\\ Les points dont le score est proche de 1 (ou supérieur à un seuil prédéfini) sont considérés comme des anomalies potentielles (car facilement isolées, i.e. $E(h(x))$ tend vers 0). À l'opposé, les points normaux obtiennent un score bien inférieur à 0.5 (car nécessitant plus de partitions pour être isolés). Si les données sont uniformément réparties dans l'espace des caractéristiques, le score moyen est alors proche de",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_8",
      "text": "0.5 pour peu que le nombre d'arbres soit suffisamment grand. L'algorithme est rapide car hautement parallélisable, efficace et robuste aux valeurs aberrantes. L'algorithme Local Outlier Factor (LOF) est une méthode de détection d'anomalies qui identifie des points aberrants en comparant leur densité locale à celle de leurs voisins. L'idée principale est que les anomalies se trouvent dans des régions de faible densité locale par rapport à leur voisinage immédiat. On doit donc se doter en premier lieu d'une distance $D$ entre les points, puis construire une mesure de densité locale à partir des distances entre les points et leurs $k$ plus proches voisins. Pour cela, on note $D_k(p)$ la distance d'un point $p$ à son $k$-ième le plus proche voisin. La distance d'accessibilité (RD) entre deux points $p$ et $q$ est définie comme $$_k(p, q) = \\{D_k(q), D(p, q)\\}$$ On aura remarqué l'asymétrie de la formule. Ainsi, tous les $k$ plus proches voisins de $q$ seront considérés comme équivalents pour $q$ suivant RD. La densité d'accessibilité locale (LRD) est définie comme l'inverse de la distance moyenne d'accessibilité de $p$ par rapport à ses voisins $q$: $$_{k'}(p) = ({k'} _{q N_k(p)} (p, q",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 8,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_9",
      "text": "))^{-1}$$ où $N_{k'}(p)$ est l'ensemble des $k'$ plus proches voisins de $p$ où $k'$ est un hyper-paramètre désignant le nombre minimal de points pour former un groupement. Cette mesure permet de quantifier la densité locale de $p$ par rapport à ses voisins et sera d'autant plus grande (resp. faible) que $p$ est entouré de points proches (resp. éloignés).\\\\ Le score LOF d'un point $p$ est obtenu en normalisant la densité locale moyenne autour de $p$ par rapport à celle de $p$: $$_{k'}(p) = ( (p)} _{k'}(q)}{k'}) / _{k'}(p)$$ Si le score satisfait $_{k'}(p) > 1$, alors la densité du point $p$ est faible et le label correspondant à une anomalie lui est attribué. Une illustration est donnée dans la figure où le rayon de chaque cercle est proportionnel au score d'anomalie de son centre. On constate que les points les plus éloignés des autres obtiennent un score élevé. [htbp] La méthode One-class SVM (OC-SVM) est une méthode de détection d'anomalies qui repose sur l'apprentissage non supervisé. Elle est basée sur les machines à vecteurs de support (SVM) qui sont classiquement utilisées pour la classification binaire supervisée. En modifiant la fonction à optimiser, on peut adapter les SV",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 9,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_10",
      "text": "M à un problème de détection d'anomalies. Commençons par décrire les machines à vecteurs de support (SVM) . Les machines à vecteurs de support (SVM) sont des modèles d'apprentissage supervisé qui peuvent être utilisés pour la classification ou la régression. Pour la classification binaire, l'objectif des SVM est de trouver un hyperplan qui sépare les données en deux classes. On distingue deux types de SVM~: Les SVM linéaires, qui cherchent à séparer les données par un hyperplan linéaire. Les SVM à noyaux qui permettent de séparer non linéairement les données à l'aide d'une fonction noyau. Le cas du noyau linéaire nous ramène au premier cas. Lorsque les données sont linéairement séparables, il existe une infinité d'hyperplans qui peuvent séparer les données. Parmi ces hyperplans, l'unique plan qui maximise la marge de séparation entre les deux classes est appelé hyperplan optimal. La marge est définie comme la distance entre l'hyperplan et les points les plus proches de chaque classe. Certains points, appelés vecteurs de support, se trouvent sur la marge et définissent l'hyperplan optimal. Nous allons détailler le problème d'optimisation des SVM dans le cas linéairement séparable.\\\\",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 10,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_11",
      "text": "Soit $$ un hyperplan de $^m$ défini par l'équation $ + w_0 = 0$. La fonction de décision est la suivante: y( ) = sgn ( . + w_0) Pour un point générique $ ^m$, on peut le décomposer en deux parties~: sa projection $_{}$ sur l'hyperplan $$ et un vecteur orthogonal à cet hyperplan colinéaire à $$ (voir figure ). Afin de quantifier la marge de séparation, on introduit la distance signée $r$ d'un point $$ à l'hyperplan~: $$r() = . + w_0}{||||}$$ [H] Après normalisation de $$ et $w_0$, les points se situant sur la marge coté positif (resp. négatif) se trouvent à une distance signée de 1 (resp. -1). Maximiser la marge tout en classant correctement l'ensemble des points $\\{(x_k,y_k) \\{1, -1\\})\\}_k$ s'écrit comme un problème de maximisation sous contraintes, à savoir $$,w_0)}{} {||||} y_k (. + w_0) 1 k \\{1,,K\\} $$ On remarquera que les contraintes intègrent naturellement le cas des exemples positifs et négatifs. Ce problème peut se reformuler comme le problème de minimisation sous contraintes suivant: ,w_0)}{} {2} ||||^2 \\\\ & y_k (. + w_0) 1 k \\{1,,K\\} La solution de ce problème est unique en raison de la convexité stricte du premier terme et de la convexité des contraintes . Le lecteur in",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 11,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_12",
      "text": "téressé par la résolution effective peut se reporter à ce même ouvrage. Lorsque les données ne sont pas linéairement séparables dans l'espace initial, on cherche un autre espace de représentation, généralement de plus grande dimension, dans lequel on espère faciliter la séparabilité des données par un hyperplan. Pour effectuer cette transformation, nous utilisons une fonction de plongement \\(\\), illustrée dans la figure .\\\\ En effet, le premier sous-graphique montre un ensemble de données non linéairement séparable dans un espace 2d. Les points rouges et bleus représentent deux classes distinctes. La distribution des données est telle qu'il est impossible de tracer une ligne droite qui sépare les deux classes de manière correcte. Cette disposition montre clairement qu'une séparation linéaire est impossible dans cet espace.\\\\ Le second sous-graphique montre les mêmes données après qu'elles aient été projetées dans un espace de dimension 3 via un plongement explicite. Cela signifie que chaque point \\( (x_1, x_2) \\) est transformé en un vecteur dans l'espace de caractéristiques par la fonction suivante de \\( ^2 ^3 \\) : $$ (x_1, x_2) = (x_1^2, x_2^2, 2 x_1 x_2) $$ Cette projection tran",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 12,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_13",
      "text": "sforme les coordonnées \\((x_1, x_2)\\) de chaque point en une représentation dans un espace à trois dimensions. Comme on peut le voir, le plan en jaune permet une séparation linéaire des données dans cet espace transformé. Le graphique illustre donc le principe des méthodes à noyaux pour traiter des problèmes non linéairement séparables. [htbp] Dans le cadre de ce manuscript, nous nous limitons au cas où $$ est connu explicitement puisque réalisée par un réseau de neurones. Dans ce cadre, l'espace $F$ des caractéristiques qui est l'image de $^n$ par $$ est un sous-ensemble de $^m$. Le problème d'optimisation pass\\'e dans l'espace caractéristique $F$ devient : ,w_0)}{{}} & {2} ||||^2 \\\\ & y_k( . (_k) + w_0) 1 , k \\{1,,K\\} où le vecteur $w$ est maintenant un vecteur de $^m$.\\\\ Le lagrangien associ\\'e \\`a est donn\\'e par : (,w_0,) = {2} ||||^2 - _{k=1}^{K} _k (y_k -1) où les $_k 0$ sont les multiplicateurs de Lagrange. En observant le terme de droite dans cette minimisation, on constate que pour tout point $_k$ qui n'étant pas dans la marge ($y_k -1 > 0$), la solution doit vérifier $_k = 0$ pour être un minimum. La recherche des points critiques annulant la diff\\'erentielle du lagrangi",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 13,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_14",
      "text": "en ($(,w_0,)}{ } = 0$ et $(,w_0,)}{ w_0} = 0$) amène les conditions suivantes :\\\\ = _{k=1}^{K} _k y_k (_k)\\\\ 0 = _{k=1}^{K} _k y_k \\\\ D'après notre remarque précédente, on en déduit que seul les points sur la marge, vérifiant $y_k -1 = 0$, contribuent à la définition de $$. Ils sont appelés vecteurs de support. En remplaçant $$ par $_{k=1}^{K} _k y_k (_k)$ dans le lagrangien, on élimine les variables $$ et $w_0$ pour obtenir le problème d'optimisation (dit dual) suivant:\\\\ () = _{k=1}^{K} _k -{2}_{k, k'=1}^{K} _k_{k'} y_k y_{k'} (_k). (_{k'}) \\\\ \\\\ _{k=1}^{K} _k y_k =0 _k 0 k \\{1,,K\\} La prédiction initiale (équation ) dans l'espace dual s'écrit : y() = sgn (_{k=1}^{K} _k y_k ().(_k)+w_0) = sgn (_{_k SV} _k y_k ().(_k)+w_0) \\\\ où $SV$ désigne l'ensemble des vecteurs de support. variables\")} En pratique, l'existence d'un hyperplan séparateur n'est pas garantie malgré un plongement dans un espace de dimension supérieure, dans le cas d'étiquetage contradictoire, . Afin d'autoriser à violer la contrainte $y_k(. + w_0) 1$, on va introduire des nouvelles variables $ 0$ permettant cela. Le problème d'optimisation ~: ,w_0)}{} {2} ||||^2 \\\\ \\\\ y_k( . _k + w_0) 1 k \\{1,,K\\} \\,. devient ,w_0,",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 14,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_15",
      "text": "_k)}{} {2} ||||^2 + C_{k=1}^{K} _k\\\\ \\\\ y_k( . _k + w_0) 1-_k, _k 0, k \\{1,,K\\} \\,. où $C > 0$ est un hyper-paramètre de la méthode. On notera que: L'exemple est bien classé si et seulement si $_k 0; 1]$ est un paramètre de la $ SVM$ permettant obtenir des garanties statistiques sur la marge. En fixant le paramètre de $$ à 1 dans la $ SVM$, on établit une équivalence entre SVM classique et ce cas particulier. Il suit que : \\(\\) est une borne supérieure sur la fraction des points d'apprentissage qui peuvent être mal classés (i.e., ceux pour lesquels \\(_k > 1\\)). \\(\\) est une borne inférieure sur la fraction des points qui sont des vecteurs supports (i.e., ceux pour lesquels \\(_k > 0\\)). Cette paramétrisation par $$ est ainsi plus naturelle.\\\\ En observant les contraintes dans l'équation , on remarque que soit $_k=0$, soit $_k 1 - y_k( . _k + w_0)$. On peut intégrer directement ces contraintes dans le critère à optimiser: ,w_0)}{} {2} ||||^2 + C_{k=1}^{K} (0, 1 - y_k( . _k + w_0))\\\\ ce qui correspond à l'écriture d'une fonction de perte { hinge}. \\\\ Dans la section précédente, nous avons décrit un algorithme de classification binaire basé sur la maximisation de la marge. On se place",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 15,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_16",
      "text": "maintenant dans le cas où le jeu de données est constitué principalement de données normales. L'objectif de décrire ces données de façon à pouvoir identifier d'autres qui s'en écarteraient. L'algorithme One Class SVM (OC-SVM) repose sur la recherche de l'hyperplan $(w, 0)$ qui sépare les données de l'origine (voir figure ). [htbp] Un nouveau terme $$, assimilable à une distance de l'hyperplan à l'origine apparaît dans la minimisation : , , _k)}{} {2} ||||^2 + C_{k=1}^{K} _k - \\\\ \\\\ ._k -_k, _k 0, k \\{1,,K\\} \\\\ Le paramètre $C = { K}$, avec $ ]0, 1]$, contrôle toujours le taux de points du mauvais coté de l'hyperplan via les variables de tolérance $_k$. La fonction de décision (normal / anormal) met en lumière le lien entre $$ et un potentiel terme $w_0$ (cf. équation ) y() = sgn (_{k=1}^{K} _k ().(_k) - ) On serait tenté donc d'écrire $ = -w_0$ et garder l'optimisation de l'algorithme précédent. Toutefois, on remarque que l'on maximise à la fois la marge (via la minimisation de $||w||^2/2$) et $$ (distance signée à l'origine) (cf. figure ). Comme précédemment, on peut transformer les contraintes par la fonction convexe de perte { hinge} pour en faire un problème d'optimisation non",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 16,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_17",
      "text": "contrainte: , )}{} {2} ||||^2 - + C_{k=1}^{K} (0, - . _k)\\\\ Cette formulation peut être rapprochée de la formulation classique du problème de classification binaire avec SVM (équation ), où un terme de biais est également présent. Dans le cadre des OC-SVM, ce terme de biais \\(w_0\\) permet d'ajuster l'hyperplan de telle sorte qu'il ne passe pas nécessairement par l'origine mais optimise la marge par rapport à l'ensemble des données normales, considérées comme appartenant à une seule classe. Les méthodes de type SVM sont connues pour être qualitativement performantes mais avoir du mal à passer à l'échelle. Afin de remplacer l'optimisation du problème primal ou dual, il est nécessaire d'en adapter la formulation. Dans , le problème d'optimisation est réécrit pour une approche par les réseaux de neurones en réinjectant les contraintes à l'aide des variables { slack}. Le probl\\`eme revient \\`a un problème d'optimisation quasi-quadratique. En effet, en tenant compte de l'inégalité ^t _k + w_0 _k$} (la marge est à $1$ de la frontière de décision) et de la condition $_k 0 $, les contraintes peuvent être retrouv\\'ees en introduisant le maximum entre 0 et $\\{1 - ^t _k + w_0\\}$. La solution",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 17,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_18",
      "text": "du problème minimise donc aussi la fonction de coût suivante : $$(, w_0; \\{_k\\}_k) = {2} ||||^2 + C_{k=1}^{K} \\{0,1- ^t _k+w_0 \\} + w_0$$ Le réseau de neurones consiste en une simple couche linéaire avec un bias. Cela revient à utiliser la fonction de perte de { hinge} (terme de droite) avec un terme de régularisation. Il est naturel de se demander \\`a quel point la solution obtenue par la résolution du problème d'optimisation convexe (via l'implémentation dans ) diff\\`ere de celle obtenue par une descente de gradient pour la fonction de perte ci-dessus. Une illustration est pr\\'esent\\'ee dans la figure pour un jeu de données synthétiques en dimension 2. Après avoir extraites les équations cart\\'esiennes ($y = a x + b$) des droites correspondantes aux frontières de décision pour les solutions trouvées, on observe que la différence entre les valeurs des $a$ (resp. des $b$) est de l'ordre de $1 10^{-3}$ (resp. $1 10^{-2}$). On peut donc conclure à une quasi-equivalence des deux méthodes sur ce jeu de données. [htbp] {0.49} {0.49} \\`A ce stade, et même si nous n'avons pas poursuivi l'idée, on peut imaginer adapter l'idée de la méthode OC-SVM au cas d'une sphère qui sépare le mieux le",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 18,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_19",
      "text": "point euclidien $$ ($e_0$ dans le modèle conforme) des données en maximisant son rayon: {} {2} ^2 - C_{k=1}^{K} _k \\\\ \\\\ _i _k _k, _k 0 k \\{1,,K\\}, \\\\ avec $C={ K} >0$ et le produit int\\'erieur not\\'e ici $_i$. On remarque immédiatement que ne pas fixer le centre aboutit à un problème mal posé; le centre pourrait sans doute être fixé ailleurs qu'en $$. L'optimisation par Adam ne s'effectue que sur le rayon alors que le centre de la sphère sera fixé et minimise la fonction de perte $-{2} ^2 + { K} _k \\{0, _i _k\\}$. La figure montre les résultats obtenus pour différentes valeurs de $$ sur les données précédentes. De manière cohérente, on constate que le taux de couverture des points augmente en même temps de $$. [ht] [b]{0.32} [b]{0.32} [b]{0.32} Les méthodes précédemment décrites cherchaient à séparer par un hyperplan (ou une hypersphère) les données de l'origine. Nous nous intéressons maintenant au cas où l'on cherche à englober les données dans l'hypersphère la plus petite possible avec une marge de tolérance. Cette approche nous est apparue comme la plus à même d'être développée dans le cadre de l'algèbre conforme. Nous allons commencer par décrire la méthode Support Vector Data",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 19,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_20",
      "text": "Description (SVDD) puis allons voir comment elle peut être adaptée à l'apprentissage profond à l'instar de ce que nous avons vu pour la \"Deep One Class SVM\". La méthode Support Vector Data Description (SVDD) vise à trouver les paramètres de centre et de rayon ($c$, $$) de la plus petite hypersphère englobante pour un ensemble de points. La formulation du problème introduit les variables de relaxation qui permettent à certains points de violer ces contraintes et de se retrouver en dehors de l'hypersphère. Comme dans le cas des SVM, le travail est fait apr\\`es plongement par $$ des donn\\'ees dans un espace de caractéristiques. Pour ${ccccc} & : & ^n & & F $ une fonction de plongement associée au noyau $$, une sphère de centre $ F$, de rayon $ $, et $C = 1/ K$ la variable de contrôle avec $ ]0,1]$, le problème d'optimisation associ\\'e \\`a une SVDD s'écrit ainsi~:\\\\ ,, )} ^2 + C_{k}^{} _k\\\\ \\\\ ~||(_k) - ||^2_{F} ^2 + _k, _k 0 k \\{1,,K\\} . où $||.||_{F}$ désigne la norme 2 dans $F$.\\\\ Le Lagrangien associ\\'e \\`a est d\\'efini par~: (,, ,, ) &=& ^2 + C _{k}^{} _k - _{k}^{} _k -_{k}^{} _k _k \\\\ & =& ^2(1-_{k}^{}_k) + _{k}^{} (C-_k-_k) _k + _{k}^{} _k ||(_k)-||^2_{F} où $_k$ et $_k$ sont le",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 20,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_21",
      "text": "s multiplicateurs de Lagrange tels que, pour tout $ k \\{1, , K\\}$, $_k 0$ et $_k 0$. Les points critiques v\\'erifient $}{ } = 0$, $}{ } = 0$ et $}{ _k} = 0$, c'est-\\`a-dire 2 _{k}^{} _k ( - (_k)) = 0 = _{k} _k (_k) \\\\ 2 (1-_{k}^{} _k ) = 0 _{k}^{} _k = 1 \\\\ C - _k - _k = 0 _k = C - _k k \\{1,,K\\} \\'Etant données les positivités de $_k$ et $_k$, on peut éliminer la dépendance en $_k$ en imposant l'encadrement $0 _k C$. À partir des deux premières égalités du système que l'on injecte dans le lagrangien ci-dessus, on obtient la formulation dite duale du problème que l'on va maximiser et qui ne dépend plus que de $_k$: ( ) = _{k}^{} _k (_k).(_k) - _{k,k'}^{}_k _{k'} (_k).(_{k'}) \\\\ sc. 0 _k C _{k}^{} _k =1 \\,. Ceci est à nouveau un problème d'optimisation quadratique sous-contraintes. D'après les conditions de complémentarité de Karush-Kuhn-Tucker (Remarque 7.3, p. 198 ), la solution optimale vérifie pour tout $k \\{1,,K\\}$: _k^{*} = 0 \\\\ ^{*}_k ^{*}_k = 0 \\,. Les vecteurs de support (SV) qui sont l'ensemble des points qui satisfont $_k^{*}> 0$ définissent à eux seuls le centre de la sphère. Géométriquement, il s'agit des points qui sont exactement à la surface de la sphère optimale ($0<",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 21,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_22",
      "text": "_k^{*}< C, ^{*}_k=0$) ou à l'extérieur grâce aux variables de tolérance ($_k^{*} = C, ^{*}_k>0$). Seuls les vecteurs support interviennent dans la définition du centre: $$^* = _{_k SV} ^*_k (_k)$$ Toutefois ce centre n'a pas besoin d'être explicitement calculé. Pour $ ^n$ et $()$ son plongement dans $F$, la distance entre $()$ et le centre s'exprime uniquement par des produits scalaires avec les vecteurs de support: D^2_{}(,):=||()-||^2_{F}= ().()-2_{_k SV}^{} _k^{*} ().(_k) + _{_k, _{k'} SV} _k^{*}_{k'}^{*} (_k).(_{k'}) Afin d'établir un critère d'anomalie, il reste à calculer le rayon de la sphère à partir de l'équation pour un vecteur de support $^<$ strict ($_k^{*}< C$) sur la sphère optimale ${^*}^2 = D^2_{}(^<,)$. Le score d'anomalie associé à la SVDD est: () = D^2_{}(,^*) - {^*}^2 Ainsi, le score est positif pour les points en dehors de l'hypersphère, et seront donc considérés comme une anomalie.\\\\ Avec un effort minimal, ce modèle peut prendre en compte des anomalies connues en imposant que celles-ci soient à l'extérieur de la sphère avec une certaine tolérance. Pour résoudre le problème quadratique à partir de sa formulation duale, il est nécessaire, dans un premier temps,",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 22,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_23",
      "text": "de réécrire le problème sous forme matricielle, afin de procéder ensuite à l'optimisation à l'aide de la librairie .\\\\ En notant $ = (_1, , _K)^T$, $0_K = (0, 0, ..., 0)^T$, $1_K = (1, 1, ..., 1)^T$ et $$ la matrice de Gram dont le terme général est $_{ij}= (_i).(_j)$, }{} ^t - ^t ()\\\\ \\\\ galit: } 1_K^t =1 \\\\ galit: } -Id_K \\\\ Id_K \\\\ 0_K \\\\ C~1_K En général, les $_k$ étant linéairement indépendants, la matrice de Gram est par construction définie positive. La fonction objective à minimiser est dans ce cas strictement convexe. Les contraintes forment de plus un ensemble convexe fermé. On en conclut à l'existence d'une solution unique. Une façon d'adapter la SVDD au contexte des réseaux de neurones et de reproduire la méthodologie qui a permis de passer de la \"One Class SVM\" (voir p. ) à la \"Deep One Class SVM\" (voir p. ): les contraintes sur les variables { slack} sont intégrées dans la fonction de perte charnière ({ hinge loss}) . Le réseau de neurones dont les paramètres sont notés $W$ définit explicitement le plongement $_W: ^n F ^m$, espace dans lequel est calculé la SVDD (cf. figure ). La fonction objective est donc _{, W} ^2 + { K} _k (0, ||_W(_k) - ||^2_F - ^2) + {2} ||W||^",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 23,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_24",
      "text": "2 Le plongement $_W$ sera pénalisé pour tout point $_W(_k)$ à l'extérieur de la sphère. Le réseau de neurones est régularisé par une pénalité $L_2$ afin de rendre le plongement dans $F$ plus robuste. On remarque que le centre ne fait pas partie de l'optimisation afin d'éviter une solution triviale. En effet, si on laisse $$ libre, la sphère va s'effondrer sur le cas dégénéré $=_0$, $=0$ où $_0$ est la sortie du réseau pour $W=0$ (tous les poids du réseau sont nuls). Le centre $$ doit donc être fixé et différent de $_0$.\\\\ Les auteurs démontrent que deux conditions supplémentaires sont nécessaires pour éviter cet effondrement(appelé également \"collapse\"). Il n'existe pas de terme de biais apprenable dans les couches cachées; le réseau serait capable de trouver un $W^*$ tel que $_W() = $ pour tout $$ et $^*=0$. Les fonctions d'activations ne doivent pas être bornées. Cette condition découle de la précédente car si une fonction d'activation sature un neurone en dehors de 0 quelque soit son entrée, ce neurone pourra jouer le rôle d'un terme de biais dans la couche suivante. Il est donc possible d'utiliser l'activation mais pas l'activation sigmoïde. Le score d'anomalie pour cette métho",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 24,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_25",
      "text": "de est: $$() = ||_{W^*}() - ||^2_F - {^*}^2$$ Afin de faciliter l'interprétation et la comparaison entre modèles, on peut adimensionner ce score en le divisant par ${^*}^2$. [htbp] {0.9} {0.33} {60} in {1,...,15} { {rand*360} { rand*1.5} (:) circle (1pt); } in {1,...,} { {rand*360} {rand*1.5} (:) circle (1pt); } {0.33} [shorten >=1pt,->,draw=black!50, node distance=2cm] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0.5,--0.5) {$x_{}$}; / in {1,...,3} (J-) at (1.5,-) {$h_{}$}; / in {1,...,3} (K-1) edge (J-); / in {1,...,3} (K-2) edge (J-); [decorate,decoration={brace,amplitude=10pt},yshift=-0.5cm] (-2,0) -- (2,0) node [black,midway,yshift=0.6cm] {$_W()$}; {0.33} (0,0) circle (0.5*); {60} in {1,...,15} { {rand*360} { + rand*0.5} (:) circle (1pt); } in {1,...,} { {rand*360} {rand*0.5} (:) circle (1pt); } )$ et SVDD} Si l'on est dans le cas où la (quasi-)totalité des données sont normales, les mêmes auteurs ont proposé une version simplifiée du précédent algorithme appelée \"One Class Deep SVDD\" pour laquelle la notion",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 25,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_26",
      "text": "de sphère disparait en prenant $ = 0$. Par extension, on peut dire que tous les points sont considérés comme à l'extérieur de la sphère et on cherche de minimiser la variance des données normales autour d'un centre $ F$: _W {K} _k ||_W(_k) - ||^2_F + {2} ||W||^2 En cherchant la transformation $_W$ qui contracte les données normales vers $$, on espère que les anomalies en seront éloignées. Il est évident que le réseau doit être suffisamment régularisé pour maintenir une \"certaine\" quantité de variance et éviter l'effondrement sur $$. Comme précédemment, il convient de fixer $$ différent de $_0$ obtenu pour $W=0$. Nos tests ont montré qu'il est possible d'utiliser des biais apprenables et des activations sur toutes les couches hormis la dernière. \\\\ Le score d'anomalie pour cette méthode est simplement la distance à $$: $$() = ||_{W^*}() - ||^2_F$$ Cependant, en pratique, il est nécessaire de fixer un seuil à ce score. Pour cela, nous avons utilisé la méthode de qui détermine un seuil \"optimal\" en maximisant le score AUC-ROC (décrit dans la section ). Ce post-traitement appliqué au score d'anomalie pour chacune des méthodes décrites ne garantit pas que la frontière de décision de l'",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 26,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_27",
      "text": "hypersphère (généralement le seuil est à 0) coïncide avec le seuil optimal trouvé.\\\\ Une amélioration a été proposée dans afin de préserver suffisamment d'information et de structure. L'idée est de pré-entraîner un auto-encodeur pour lequel $_{W}$ serait l'encodeur. Le vecteur $$ serait alors défini comme la sortie moyenne de $_{W}$. La poursuite de l'entraînement consiste à minimise la perte auquel on a rajouté l'erreur de reconstruction de l'auto-encodeur en norme $L_2$. Les fonctions à base radiale sont des fonctions dont la valeur dépend de la distance par rapport au centre $ ^n$ de la fonction. Une fonction à base radiale $()$ est de la forme~: (, \\{, \\}) = (||-||) où $ ^n$ est le vecteur d'entrée, $$ est le centre de la fonction, $$ un paramètre de contrôle sur la largeur de la fonction et $$ une fonction donn\\'ee. On considérera $||||$ comme la norme euclidienne. Une des fonctions à base radiale les plus couramment utilisées est la fonction Gaussienne, définie par~: (, \\{, \\}) = e^{--||^2}{2^2}} Les fonctions à base radiale sont souvent utilisées, tr\\`es efficacement, dans le cadre de méthodes d'approximation ou de classification. Elles servent de fonctions de base pour repr",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 27,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_28",
      "text": "ésenter des fonctions plus complexes (cf. approximation universelle par les fonctions à base radiale dans le chapitre précédent). Elles sont aussi structurellement assez proches des fonctions de sortie hypersph\\'eriques. On d\\'etaille donc la m\\'ethode Deep Radial Basis Function Data Descriptor (D-RBFDD) pour que le lecteur la différencie bien des outils que nous introduirons par la suite. La sortie d'une couche RBF s'écrit~: (, \\{w_j, _j, _j\\}) = _{j=1}^J w_j ( _j|| -_j|| ) . où $J$ est le nombre de fonctions radiales. Dans la littérature, les points centraux $_j$ des fonctions radiales sont initialisés par clustering et les facteurs d'échelle fixés à 1.\\\\ Le principe de la méthode RBFDD est d'utiliser une couche RBF puis une activation $g$ pour régresser la valeur de normalité de $1$. Afin d'éviter une activation qui sature pour la valeur $1$, les auteurs utilisent la fonction recommandée dans , à savoir $1.7159~(2x/3)$. Dès lors qu'un module $_W$ d'extraction de caractéristiques est mis en amont de la couche RBF, les auteurs appellent ce modèle Deep RBFDD. La fonction de coût à minimiser est: L(\\{_k\\}, W, \\{w_j, _j, _j\\}) = {2K} _k ^2+{2}||W||^2_2 Comme précédemment, un seuil au",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 28,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_29",
      "text": "tomatique est calculé en sortie de l'activation $g$ par la procédure automatique liée au critère AUC-ROC (cf. ). Les anomalies seront typiquement les données pour lesquelles la sortie est inférieure à ce seuil. Les couches hypersphériques sont définies dans le cadre de l'algèbre géométrique conforme, permettant de représenter des points et des hypersphères dans un espace étendu. Les méthodes D-RBFDD ou Deep SVDD et ses variantes décrites précédemment, sont adaptées pour utiliser des couches hypersphériques et effectuer une optimisation. Dans cette section sont d\\'efinis tous les algorithmes que nous proposons, en s'appuyant sur leur construction \\`a partir de probl\\`emes d'optimisation sous contraintes et en d\\'etaillant le fonctionnement des fonctions de co\\^ut dirigeant l'apprentissage. Cela donne des premiers \\'el\\'ements de comparaison avec les m\\'ethodes Deep SVDD. L'analyse des propri\\'et\\'es des algorithmes propos\\'es et les exp\\'erimentations num\\'eriques seront d\\'evelopp\\'ees aux paragraphes et . L'espace des caract\\'eristiques $F$ dans lequel sont projet\\'ees les donn\\'ees est suppos\\'e de dimension $m$. Une hypersph\\`ere dans $^m$ de centre $=(c_1,,c_m)$ et de rayon $$",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 29,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_30",
      "text": "correspond \\`a $ $, dont les $m+2$ coordonn\\'ees sont $(1,, ( ^2 - ^2)/2)$, avec $ ^2=_{i=1}^m c_i^2$, dans l'alg\\`ebre conforme $(m+1,1)$. Dans toute la suite, pour \\'eviter la confusion avec le produit scalaire, le produit interne de l'alg\\`ebre conforme est not\\'e $_i$. On rappelle en particulier que pour tout point $ =(x_0,,x_)$ de $(m+1,1)$, on a~:\\\\ $$ _i = - {2} ( - ^2 -^2 ) .$$ On note en particulier que seules les coordonn\\'ees de $$ correspondant aux coordonn\\'ees dans l'espace des caract\\'eristiques sont utilis\\'ees. Cette approche s'inspire du principe que la méthode RBFDD décrite précédemment. Le réseau de neurones est constitué d'une seule couche composée de plusieurs hypersphères. La sortie du réseau correspond à une somme pondérée de sigmoïdes (fonctions $g$) appliquées au produits conformes entre les hypersph\\`eres et les points testés. (, \\{ w_j, _j \\}) = _{j=1}^{J} w_j g(_j _i ) La figure illustre deux points principaux: Elle approxime une fonction porte à bords lisses sur l'intervalle $[-, ]$. Quelque soit la valeur de $$, son maximum qui est atteint pour $=$ appartient à l'intervalle $]0.5, 1[$. [H] _i $ pour $=0$} Le réseau SPH Anomaly cherche par une régressi",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 30,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_31",
      "text": "on au sens des moindes carrés à ajuster les sphères pour obtenir une sortie à 1:\\\\ (\\{_k\\}, \\{w_j, _j\\}) = {2K} _k (1- (_k, \\{w_j, _j\\}) )^2 \\\\ \\'Etant donnée une hypersphère de rayon $$ \"suffisant\" grand ($>3$), la sortie $g(_j _i )$ pour les points $$ situés à une distance inférieure à $$ de $$ est proche de 1. On appellera Deep SPH Anomaly, un réseau de neurones tel que SPH Anomaly suit un extracteur de caractéristiques. Dans nos observations, nous avons constaté qu'il est préférable que ce dernier soit pré-entrainé (via un auto-encodeur). {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0,-+1/2) {$x_{}$}; (K-3) at (0,-3*+1/2) {$1$}; (K-4) at (0,-4*+1/2) {${2}$}; / in {1,...,3} (J-) at (0,-) {$_{}_i $}; / in {1,...,3} (K-1) edge (J-); / in {1,...,3} (K-2) edge (J-); / in {1,...,3} (K-3) edge[dashed,->,red] (J-); / in {1,...,3} (K-4) edge[dashed,->,red] (J-); (I) at (0.5cm,-2*) {$ w_{j=1}^3 g(_j _i _k)$}; in {1,...,3} (J-) edge (I); L'idée de la méthode SPH SVDD",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 31,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_32",
      "text": "est de reformuler le problème la SVDD (cf. équation ) en utilisant une hypersphère dans l'espace géométrique conforme. En d'autres termes, utiliser une couche hypersphérique pour résoudre le problème d'optimisation en utilisant l'algèbre géométrique conforme (cf. figure ). Comme ce sont les paramètres d'une hypersphère d'encadrement qui sont recherchés, le réseau contient une couche hypersphérique à un seul neurone (une sphère) et les entrées $ ^m$ sont d'abord plongés dans l'espace conforme pour donner $ (m+1, 1)$.\\\\ [h!] {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!120]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0,-+1/2) {$x_{}$}; (K-3) at (0,-3+1/2) {$1$}; (K-4) at (0,-4+1/2) {${2}$}; / in {0} (J-) at (0,-1.75) {$y_i = _i $}; / in {0} (K-1) edge (J-); / in {0} (K-2) edge (J-); / in {0} (K-3) edge[dashed,->,red] (J-); / in {0} (K-4) edge[dashed,->,red] (J-); Par conséquent, la sortie du réseau est le produit conforme entre un point $ (m+1, 1)$ et une hypersphère $ (m+1, 1)$ de centre $$ et de rayon $$. Ra",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 32,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_33",
      "text": "ppelons que ce produit est\\\\ _i = -{2} \\\\ On rappelle que le problème d'optimisation pour la SVDD se formule comme\\\\ {} & ^2 + { K}_{k=1}^{K} _k\\\\ \\\\ & ||_k - ||^2-^2 _k, _k 0, k \\{1,,K\\} \\\\ peut se reformuler à l'aide du produit conforme ($||_k - ||^2-^2 = - 2~ _i _k$) comme\\\\ ,)}{} & ^2 + { K}_{k=1}^{K} _k\\\\ \\\\ & -2_i_k _k, _k 0, k \\{1,,K\\} . \\\\ En utilisant une fonction de coût charnière, on obtient une formulation sur une ligne de SPH SVDD comme\\\\ }{}\\, ^2 + { K} _{k=1}^{K} \\{0, -2 _i _k \\} \\\\ où $K$ est le nombre total de points dans l'ensemble des données. On peut préciser que $^2$ peut également s'écrire $^2$ (produit géométrique) ou encore $ _i $ car $ = 0$.\\\\ La $$-propriété de la SVDD qui permet de contrôler la proportion des points à l'intérieur de la sphère est perdue ici. L'hyper-paramètre $$ reste toutefois un moyen de contrôler la tolérance du modèle à l'erreur. En pratique, différents codes de la littérature montrent que la $$-propriété peut-être rétablie après optimisation en modifiant le rayon pour couvrir $100~\\ Si les données ne peuvent pas être séparées par une hypersph\\`ere, une ou plusieurs couches cachées (extraction de caractéristiques) sont ajoutées au rés",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 33,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_34",
      "text": "eau en amont (cf. figure ). Les paramètres $W$ de ce module du réseau permettent caractériser la fonction de plongement $_W: ^n ^m$ dans l'espace des caractéristiques $F$. La sortie $_W()$ de $_W$ est ensuite plongée dans l'algèbre géométrique conforme $(m+1,1)$. La suite du réseau correspond à la partie SPH SVDD décrite plus haut, l'ensemble est appelé Deep SPH SVDD.\\\\ [h!] {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0,--2.5) {$x_{}$}; / in {1,...,5} (J-) at (0,--1) {$h_{}$}; / in {1,...,5} (K-1) edge (J-); / in {1,...,5} (K-2) edge (J-); / in {1,...,3} (L-) at (2.5,-2) {$_{W}(h)$}; / in {1,...,3} (J-1) edge (L-); / in {1,...,3} (J-2) edge (L-); / in {1,...,3} (J-3) edge (L-); / in {1,...,3} (J-4) edge (L-); / in {1,...,3} (J-5) edge (L-); (L-4) at (2.5,-7.75) {$1$}; (L-5) at (2.5,-9) {${2}$}; (I) at (2,-3.75) {$y_i = _i $}; in {1,...,3} (L-) edge (I); / in {4,...,5} (L-) edge[dashed,->,red] (I); La recherche des paramètres de l'hypersphère englobante est eff",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 34,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_35",
      "text": "ectuée dans l'espace de caractéristiques en dimension $m$. La fonction de coût est relativement similaire à celle de la SPH SVDD puisque seul un terme de régularisation est rajouté: }{}\\, ^2 + { K} _{k=1}^{K} \\{0, -2 _i (_k) \\} + {2} ||W||^2 Contrairement à la méthode Deep SVDD, tous les paramètres de l'hypersphère, y compris le centre, sont appris dans Deep SPH SVDD. Nous verrons par la suite la raison qui fait que cela est possible sans effondrement. On d\\'efinit le score d'anomalie comme () = -2^* _i }(z). Ce score est si et seulement si $}(z)$ est dans la sphère $^*$. Cela indique que le réseau de la méthode Deep SPH SVDD produit directement $ ^* _i }(z)$ qui est un score de normalité. Comme nous l'avons déjà évoqué, un seuil automatique du score est calculé par la procédure automatique liée au critère AUC-ROC (cf. équation ). Cela revient à ajuster le rayon de la sphère trouvée.\\\\ Les étapes d'implémentation sont résumées à travers le pseudo-code suivant:\\\\ [ colback=gray!5, colframe=blue!75!black, fonttitle=, title={Deep SPH SVDD} ] [H] _1, , _K\\}$\\\\ Hyperparamètres $ ]0, 1[$, $> 0$, $$, nombre maximal d'itérations $N_{}$, $$} $ et rayon $^*$ de l'hypersphère, fonction de dét",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 35,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_36",
      "text": "ection d'anomalie} \\\\ \\\\ Initialiser aléatoirement les poids $W$ de $$ (méthode He)\\; { Initialiser $$ avec $ = $ et $$ leur écart-type\\; } $ avec $ (0,1)$ et $ = 1$\\; } \\\\ 1$ $N_{}$}{ Mélanger aléatoirement $X$ \\; { _k B$}{ Extraction de caractéristiques: $_k ^n _W (_k) ^m$\\; Passage en conforme: $_W (_k) ^m (_k) (m+1,1)$\\; Calculer le score d'anomalie $-2 _i (_k)$; } $ = ^2 + { |B|} _{x_k B} (0, -2 _i (_k))) + {2} ||W||_2^2$\\; Mettre à jour $W$, $$ à partir de $$ avec la méthode ADAM\\; } $}{ Arrêter l'étape 2\\; } } $ = 0$\\; \\\\ {un jeu de validation $V$ est disponible}{ _k V$}{ Calculer le score d'anomalie $-2^* _i }(_k)$; } Obtenir un seuil automatique $$ via l'équation ; } $$() := -2^* _i }() > $$ \\\\ Certains des \\'el\\'ements ci-dessous seront établis dans les paragraphes et . Nous pr\\'ef\\'erons les rassembler aussi ci-dessous pour le confort du lecteur. [H] {|p{0.9}|} {|c|}{} \\\\ .\\\\ L'hyper-paramètre $$ reste un coefficient de régularisation. Un taux de couverture peut être obtenu en aval par l'ajustement du rayon de la sphère sur un jeu de données de validation. \\\\ [H] {|p{0.9}|} {|c|}{} \\\\ .\\\\ Le nombre de paramètres entraînables est $m+1$ dans les deux cas. Pour cela, Deep S",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 36,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_37",
      "text": "PH SVDD fixe le coefficient de $e_0$ à 1 pendant l'entraînement. \\\\ .\\\\ Par définition, les anomalies sont les points situés à l'extérieur de la sphère (score(z) $> 0$). En pratique, ce seuil peut être ajusté automatiquement sur un jeu de validation afin de maximiser la performance ROC. Si l'on considère que le centre de l'hypersphère est fixé par l'entraînement, cela correspond à en ajuster le rayon.\\\\ [H] {|p{0.435}|p{0.435}|} {|c|}{} & {c|}{} \\\\ Le centre $$ de l'hypersphère afin éviter la dégénerescence de celle-ci. Par exemple, on peut choisir $$ comme le barycentre des points ${(_k)}$ pour le premier batch. & Le centre $$ est libre, pas de dégénérescence observée. \\\\ Le rayon $$ est libre hormis pour la OC-Deep SVDD qui le fixe à $0$ (requiert une hypothèse de normalité). & Le rayon $$ est libre. \\\\ La sortie du réseau fournit la projection des données dans l'espace des caractéristiques \\( F \\). & La sortie du réseau donne directement le score de normalité. \\\\ Les termes de biais et les activations bornées sont interdites sous peine de dégénérescence de l'hypersphère & L'encodeur $$ peut contenir des termes de biais et utiliser des fonctions d'activation bornées.\\\\ Dans cette",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 37,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_38",
      "text": "section, nous étendons la méthode Deep SPH SVDD au cas de plusieurs sphères. L'idée première est d'envelopper efficacement des groupes spécifiques de points normaux dans l'espace des caractéristiques par un groupe de sphères. Par ce choix, nous désirons améliorer la finesse de d\\'etection et l'interprétabilité du modèle notamment dans le cas de données multimodales.\\\\ Les paramètres à optimiser sont toujours les poids $W$ de la fonction de projection $$ qui envoie les données vers l'espace des caract\\'eristiques, ainsi que les centres $$ et les rayons $_j$ des hypersphères. Afin d'adapter la formulation au cadre multisph\\'erique, deux questionnements doivent être tranchés: Comment définir une anomalie dans le cas de plusieurs hypersphères ? Comment calculer le volume des données normales couvertes par le modèle ? Pour répondre à la première question, rappelons qu'un point $$ est à l'intérieur (couvert) par la sphère $_j$ si et seulement si $-2_j _i < 0$. La règle de couverture la plus naturelle est de décréter qu'un point est couvert si et seulement si il existe au moins une sphère qui le couvre. Cela revient alors à attribuer à chaque point le score d'anomalie minimal parmi toute",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 38,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_39",
      "text": "s les sphères: () = _j \\{ -2_j _i \\} et espérer qu'il soit négatif.\\\\ Ce choix est justifié par le fait que si un point est bien inclus dans au moins une sphère, alors son score d'anomalie sera faible. À l'inverse, un point qui est éloigné de toutes les sphères recevra un score élevé, reflétant son caractère atypique. Ainsi, cette définition permet d'assurer une cohérence entre la formulation du score d'anomalie et la règle de couverture adoptée pour la classification des données normales.\\\\ Dans toutes les méthodes présentées jusqu'à présent, la minimisation du volume d'une sphère qui englobe les données normales est obtenue en minimisant de manière auxiliaire le carré de son rayon. Dans le cas de plusieurs hypersphères, la minimisation du volume couvert nécessite de prendre en compte les relations spatiales entre elles. Par exemple, si deux sphères se chevauchent fortement le volume de leur union est supérieur à celui d'une unique sphère optimisée couvrant les mêmes points, ce qui peut être inefficace en termes de représentation des données normales. De plus, même s'il existe une formule analytique pour le volume de l'intersection entre deux sphères, gérer plus de deux sphères se",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 39,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_40",
      "text": "mble inextricable.\\\\ En conséquence, une stratégie naturelle consiste à imposer une contrainte d'exclusion mutuelle entre les hypersphères, garantissant ainsi qu'elles ne s'intersectent pas. Cette contrainte permet de s'assurer que chaque sphère capture une région distincte de l'espace, évitant ainsi les redondances et améliorant la séparation des groupes de données. De plus, en éliminant les intersections, on simplifie considérablement la modélisation du volume couvert, qui devient simplement la somme des volumes individuels des sphères (ou de manière auxiliaire la somme des carrés des rayons). Ce choix garantit ainsi une optimisation cohérente et contrôlable, en adéquation avec l'objectif de minimisation du volume global des données normales. On notera que cette question n'\\'etait pas prise en compte dans l'approche avec la m\\'ethode SPH Anomaly. Pour garantir la distinction et la non-intersection entre les sphères, on va introduire deux termes supplémentaires dans la fonction de co\\^ut. Le théorème suivant de Hestenes et al. (voir théorème 2.6.1 in ) inspire la construction (adaptation des notations).\\\\ [colframe=blue, colback=white!10, title=Critère pour l'intersection de deux",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 40,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_41",
      "text": "sphères] Deux sphères $_1$, $_2$ se coupent, sont tangents ou parallèles, ou ne se coupent pas, si et seulement si $(_1 _2)^2$ est inférieur, égal ou supérieur à $0$, respectivement. Ainsi le d\\'eveloppement d'une expression de la forme $(_{1} _2)^2$, où $$ désigne le produit externe, permet d'obtenir une relation entre les centres et les rayons des sphères, dont le signe indique si deux sphères s'intersectent.\\\\ Par exemple en dimension 2, soit $_1$ et $_2$ sont deux hypersphères caract\\'eris\\'ees par les coordonn\\'ees\\\\ {l} _1 = e_0 + x_1 e_1 + y_1 e_2 + (- {2} + {2} ) e_, \\\\ \\\\ _2 = e_0 + x_2 e_1 + y_2 e_2 + (- {2} + {2} ) e_, dont les centres sont donc $_1 = (x_1, y_1)$ et $_2 = (x_2, y_2)$ et les rayons $_1$ et $_2$. On calcule le scalaire: (_1 _2)^2 &= {4} (-_1^2 - 2_1_2 - _2^2 + x_1^2 - 2x_1x_2 + x_2^2 + y_1^2 - 2y_1y_2 + y_2^2) \\\\ \\\\ & (-_1^2 + 2_1_2 - _2^2 + x_1^2 - 2x_1x_2 + x_2^2 + y_1^2 - 2y_1y_2 + y_2^2) \\\\ \\\\ &= {4} \\\\ \\\\ & \\\\ \\\\ &= {4} \\\\ Il s'avère que ce d\\'eveloppement se généralise en dimension quelconque~: \\\\ (_1 _2)^2 &= {4} (-_1^2 - 2_1_2 - _2^2 + _{i=1}^m (_{1i} - _{2i})^2) \\\\ \\\\ & (-_1^2 + 2_1_2 - _2^2 + _{i=1}^m (_{1i} - _{2i})^2) \\\\ \\\\ &= {4} \\\\ La figure",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 41,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_42",
      "text": "illustre le signe de $(_1 _2)^2$ suivant la relation spatiale entre les deux sphères.\\\\ [htbp] {0.2} [thick, scale=0.6] (0,0) circle (1.5); (1.8,0) circle (1.5); _1 _2)^2 < 0$} {0.37} [thick, scale=0.6] (0,0) circle (1.5); (2.5,0) circle (1); (3.75,-1.5) -- (3.75,1.5); (5.5,0) circle (1.5); (6,0) circle (1); _1 _2)^2 = 0$} {0.37} [thick, scale=0.6] (0,0) circle (1.5); (2.5,0) circle (0.75); (3.65,-1.5) -- (3.65,1.5); (5.5,0) circle (1.5); (6,0) circle (0.75); _1 _2)^2 > 0$} _1 _2)^2$} On remarque plusieurs éléments: L'expression est bien entendu symétrique~: $ (_1 _2)^2 =(_2 _1)^2$ Une sphère est tangente à elle-même: $( )^2 = 0$ Si la sphère $_1$ est totalement incluse dans $_2$, $(_1 _2)^2 > 0$ Réduire l'intersection des sphères revient à pénaliser le terme $- (_1 _2)^2$ s'il est positif. Dans le cas de $J$ hypersphères, on est amené à considérer ce terme de pénalité pour chaque couple d'hypersphères et ainsi définir le coût:\\\\ oss_{}(_1, , _J) = _{j < j'} \\{0, - (_j _{j'})^2 + \\} \\\\ où $$ est une petite constante positive introduite pour \\'eviter que deux hypersphères soient tangentes.\\\\ Cependant, comme indiqué dans la troisième remarque et le dernier cas de la figure , le sign",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 42,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_43",
      "text": "e de $(_1 _2)^2$ n'est pas suffisant pour garantir l'exclusion mutuelle. Pour cela, un terme de non-inclusion est rajouté pour favoriser la situation où\\\\ _1 _i _2 < 0 _2 _i _1 < 0 \\\\ Cela revient à faire en sorte que le centre de chaque sphère soit perçu comme une anomalie par l'autre. On imagine bien que ce critère contribue également à la non-intersection de deux sphères. Comme précédemment, on définit un terme global pour les $J$ sphères:\\\\ oss_{}(_1, , _J) = _{j, j' j} \\{0, _j _i _{j'} + \\} où $$ est une petite constante positive introduite pour éviter que les centres soient trop proches des bords des sphères. Conformément à l'objectif poursuivi, nous avons retiré le terme $j = j'$ dans la double sommation de l'équation . Toutefois, de manière amusante, on constate que\\\\ $\\{0, _j _i _{j} + \\} = _j^2/2 + $, ce qui conduit de minimiser $_j^2$ déjà présent dans la fonction de coût finale. Pour une raison de lisibilité et de lien entre les algorithmes, nous avons fait le choix de supprimer le terme $j=j'$ de la double sommation.\\\\ Finalement, en regroupant l'ensemble des consid\\'erations pr\\'ec\\'edentes, la formulation du problème de la Deep M-SPH SVDD s'écrit comme\\\\ ,, , W}{} \\{",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 43,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_44",
      "text": "& _{j} ^2 + { K} _{k} \\{ 0, {}\\{ -2_j _i _W(_k) \\} \\} \\\\ \\\\ & + oss_{}(_1, , _J) + oss_{}(_1, , _J) + {2} ||W||^2 \\} \\\\ Pour mieux illustrer les étapes d'implémentation de la méthode Deep SPH SVDD, la représentation algorithmique ci-dessous, sous forme de pseudo-code, synthétise les étapes essentielles de sa mise en œuvre.\\\\ [ colback=gray!5, colframe=blue!75!black, fonttitle=, title={Deep M-SPH SVDD} ] [H] _1, , _K\\}$, nb. hypersphères $J$\\\\ Hyperparamètres: $ ]0, 1[$, $> 0$, $$, nombre max. d'itérations $N_{}$, $$} $ et rayons $_j^*$ des hypersphères, fonction de détection d'anomalie} \\\\ \\\\ Initialiser aléatoirement les poids $W$ de $$ (méthode He)\\; { Clustering des $\\{_W(x) | x X\\}$ en $J$ groupes\\; Initialiser les $_j$ à partir des centres $_j$ et des écart-types $_j$ des clusters\\; } _j$ avec $_j (0,1)$; $_j$ = 1\\; } \\\\ 1$ $N_{}$}{ Mélanger aléatoirement $X$ \\; { _k B$}{ Extraction de caractéristiques: $_k ^n _W (_k) ^m$\\; Passage en conforme: $_W (_k) ^m (_k) (m+1,1)$\\; _j$}{Calculer les scores d'anomalie $-2_j _i (_k)$;} } $_{} = 0$; $_{} = 0$ \\; ^2, j < j'$} { $_{} = _{} + \\{0, - (_j _{j'})^2 + \\}$\\; $_{} = _{} + \\{0, _j _i _{j'} + \\} + \\{0, _{j'} _i _{j} + \\}$\\; } $ = _j",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 44,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_45",
      "text": "_j^2 + { |B|} _{x_k B} _j \\{ -2_j _i _{W}(_k)\\} + _{} + _{} + {2} ||W||_2^2$\\; Mettre à jour $W, _1, , _J$ à partir de $$ avec la méthode ADAM\\; } $}{ Arrêter l'étape 2\\; } } $ = 0$\\; \\\\ {un jeu de validation $V$ est disponible}{ _k V$} { Calculer le score d'anomalie $_j \\{ -2^*_j _i _{W^*}(_k)\\}$ } Obtenir un seuil automatique $$ via l'équation ; } $$() := _j \\{ -2^*_j _i _{W^*}(z) \\} > $$ } La méthode Deep M-SPH SVDD généralise Deep SPH SVDD, car lorsque le nombre de sphères est fixé à $J=1$, les deux algorithmes deviennent équivalents. L'algorithme le plus proche de Deep M-SPH SVDD est la variante Deep Multi-Sphere SVDD (DMSVDD) développée par Ghafoori et Leckie dans . Avant de comparer point par point les deux algorithmes, donnons la fonction de coût suivi par DMSVDD: _{W, } {J} _j ^2_j + { K} _k (0, ||_W(_k) - c(_W(_k))||^2 - (_W(_k))^2) + {2} ||W||^2_2 où $c(_W(_k))$ (resp. $(_W(_k))$) désigne le centre (resp. le rayon) du centre le plus proche de $(_W(_k)$ dans $F$. Le score d'anomalie ainsi construit est clairement identique au nôtre dans l'équation (). Pour faciliter la comparaison, diff\\'erents \\'el\\'ements sont list\\'es dans l'encadr\\'e ci-dessous. [H] {|p{0.48}|p{0.48}",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 45,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_46",
      "text": "|} & } \\\\ $k$-means dans l’espace des caractéristiques pour initialiser les centres et les rayons des sphères. & $k$-means dans l’espace des caractéristiques pour fixer les centres.\\\\ Rétropropagation conjointe des paramètres de l’encodeur et des sphères via Adam. & Optimisation alternée : mise à jour séparée des rayons et des paramètres de l’encodeur. \\\\ Centres ajustés dynamiquement pendant l’entraînement via rétropropagation. & Fixés après l’initialisation, restent constants tout au long de l’optimisation. \\\\ Rayons ajustés dynamiquement pendant l’entraînement via rétropropagation. & Déterminés empiriquement comme les $(1-)$-quantiles des distances aux centres. \\\\ La $$-propriété n'est pas respectée lors de l'optimisation. & La $$-propriété forcée par le choix empirique des rayons. \\\\ Introduit un coût d’intersection et de non-inclusion pour éviter le recouvrement excessif des sphères. & Les sphères peuvent s'intersecter; la somme des carrés des rayons ne mesure donc pas le volume de données couvert.\\\\ Le nombre de sphères $J$ est fixé. Les sphères peuvent avoir un rayon nul lors de l'optimisation & Ajusté dynamiquement en supprimant les sphères avec un effectif trop faible. \\\\",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 46,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_47",
      "text": "Le score d'anomalie est celui de la distance à la plus proche sphère. & Le score d'anomalie est celui de la distance à la plus proche sphère. \\\\ Au final, on voit que Deep M-SPH SVDD adopte une approche plus flexible grâce à une optimisation conjointe qui permet une adaptation fine des sphères aux données et qui modélise explicitement les relations géométriques entre les sphères. L'approche de DMSVDD est plus rigide avec des centres fixés et des rayons \"optimisés\" pour la $$-propriété. Cela favorise sans doute une répartition stable des sphères, mais la répartition des données dans les sphères dépend largement de la régularité de $$. Dans les modèles classiques de Deep SVDD (ex. One-Class Deep SVDD), l'encodeur est associé à un centre $c$ et un rayon $$ fixés. Dans les modèles Deep SPH SVDD proposés, \\`a une ou plusieurs sph\\`eres, les paramètres caract\\'eristiques des sphères, rayon et centre, sont libres \\`a l'initialisation puis appris. C'est un int\\'er\\^et de ces nouvelles m\\'ethodes. C'est \\'egalement surprenant au regard du contexte classique. En effet, il est connu (voir ) que si les centre et rayon des sph\\`eres sont laiss\\'es libres dans les algorithmes classiques de Deep",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 47,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_48",
      "text": "SVD, alors ceux-ci convergent vers la solution triviale $(W_0, , 0)$, o\\`u $W_0$ d\\'esigne le r\\'eseau dont tous les poids sont nuls. En particulier, le rayon de la sph\\`ere limite est nul, elle est d\\'eg\\'en\\'er\\'ee, r\\'eduite \\`a un point not\\'e $c_0$ : c'est le phénomène de collapse. Les algorithmes Deep SPH SVDD propos\\'es ont pu appara\\^ tre jusqu'ici comme des r\\'e-\\'ecritures d'algorithmes classiques profitant du formalisme de l'alg\\`ebre conforme. Il s'av\\`ere qu'ils ont un comportement tr\\`es diff\\'erent vis \\`a vis du risque de collapse. On a constat\\'e apr\\`es de nombreuses exp\\'erimentations que, bien que les centres et les rayons soient laiss\\'es libres, les rayons ne tendent pas toujours vers z\\'ero. Le pr\\'esent paragraphe est donc consacr\\'e \\`a ce comportement { a priori} \\'etonnant. On introduit quelques notations pour uniformiser le raisonnement entre les diff\\'erentes m\\'ethodes pr\\'esent\\'ees dans ce manuscrit combinant r\\'eseau de neurones et SVDD. Soit d'abord $$ _W \\, : \\, ^n F ^m.$$ la fonction associ\\'ee au r\\'eseau $W$ qui effectue le plongement des donn\\'ees de $^{n}$ à un espace de caractéristiques $F$. Soit $W_0$ le r\\'eseau dont tous les poids sont nu",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 48,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_49",
      "text": "ls (avec les notations du manuscrit, $^l = 0$ pour tout $^l W_0$). La fonction $_{W_0}$ associ\\'ee est donc constante. Soit $ ^m$ cette constante, $\\{ \\} = _{W_0}(^n)$. On suppose qu'on dispose de $K$ donn\\'ees not\\'ees $_k$ pour $1 k K$. Toutes les m\\'ethodes m\\^elant apprentissage et SVDD pr\\'esent\\'ees ont une structure commune dans la fonction de perte associ\\'ee. On peut les \\'ecrire sous la forme d'une somme de fonctions de la forme J (W,, ) = ^2 + { K} _{ k}^{} ||_W(_k) - ||^2 _{F} + (_W) + (,) avec && 0, \\ 0,\\ 0, \\ 0, \\\\ && \\, : \\, ^{m+1} _+ (,0) = 0 \\ ^m. Dans la suite, on va supposer pour all\\'eger les notations qu'on ne cherche qu'une sph\\`ere englobant les donn\\'ees, si bien que la fonction de perte est exactement donn\\'ee par . Si les param\\`etres \\`a apprendre sont les param\\`etres du r\\'eseau, le rayon et le centre de la sph\\`ere, le but de l'algorithme est donc de d\\'eterminer (W^*,^*,^*) = _{(W,,)} J (W,, ) , que le centre et le rayon soient appris directement ou appris { via} l'hypersph\\`ere conforme $^* = (1, c_1^*,,c_m^*, ( ^* ^2 - (^*)^2)/2)$. On \\'enonce le r\\'esultat suivant. ^*,^*)$ du probl\\`eme est } $$ W^*= W_0, ^* = _0, ^* = 0.$$ On v\\'erifie facilement",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 49,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_50",
      "text": "que $J(W_0,_0,0)=0$. Comme la fonction $J$ est positive, $(W_0,_0,0)$ r\\'ealise n\\'ecessairement le minimum de $J$ est est une solution du probl\\`eme . S'il existe une autre solution $(W^*,^*,^*)$ au probl\\`eme , elle v\\'erifie n\\'ecessairement aussi $J(W^*,^*,^*)=0$. Chaque terme dans la somme d\\'efinissant $J$ \\'etant une fonction \\`a valeur positive, cela implique que $$ (^*)^2 =0 ,\\ (_{W^*}) =0,\\ (^*,^*) =0 , \\ ||_{W^*}(_k) - ^*||^2 _{F} =0 \\ 1 k K.$$ La premi\\`ere relation implique $^*=0$, la seconde $W^*=W_0$, si bien que la quatri\\`eme implique alors $^*=_{W^*}(_k) = _{W_0}(_k) =_0$. $$ Un tel r\\'esultat a d\\'ej\\`a \\'et\\'e d\\'emontr\\'e par les auteurs de pour le cadre Deep SVDD. Ils en concluent que l'algorithme en question risque de converger vers cette solution d\\'eg\\'en\\'er\\'ee. Pour l'\\'eviter ils pr\\'econisent de fixer rayon et centre et de ne plus chercher qu'\\`a apprendre les poids du r\\'eseau $W$ : pr\\'ecis\\'ement, pour $ 0$ donn\\'e, pour $ _0$ donn\\'e, r\\'esoudre $$W^* = _{W} J (W,, ).$$ C'est un appauvrissement du mod\\`ele dont il est cependant bien connu qu'il est n\\'ecessaire~: l'algorithme Deep SVDD a tendance \\`a collapser lorsque le centre et le rayon ne sont",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 50,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_51",
      "text": "pas fix\\'es. Pour autant, comme on va l'illustrer dans le paragraphe suivant, les algorithmes Deep SPH SVDD, bien que soumis aussi au r\\'esultat de la Proposition ci-dessus, ne conduisent pas au collapse. Le fait que les Deep SPH SVDD ne soient pas sensibles au collapse va entra\\^iner une cascade de bonnes propri\\'et\\'es. L'article mentionne aussi le fait que le r\\'eseau associ\\'e \\`a la m\\'ethode Deep SVDD ne doit pas contenir de biais. D'abord, en toute rigueur, ce n'est vrai que si la fonction de perte ne contient pas le terme de régularisation $(_W)$ (voir la d\\'emonstration de la Prop. 2 dans ). Ensuite, et surtout, c'est une cons\\'equence du fait que le centre est fix\\'e pour la Deep SVDD. Les Deep SPH SVDD qui ne collapsent pas quand le centre est laiss\\'e libre peuvent donc s'appuyer sur un r\\'eseau dont l'architecture est enrichie par des biais. D\\`es lors (voir Prop. 3 dans ), les Deep SPH SVDD ne sont pas limit\\'es \\`a l'utilisation de fonctions d'activation non born\\'ees. Le but de ce paragraphe est de comprendre pourquoi comment l'approche Deep SPH SVDD se comporte par rapport au phénomène de collapse ({ i.e.} ne produit pas une sph\\`ere d\\'eg\\'en\\'er\\'ee en un point)",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 51,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_52",
      "text": "et pourquoi il n'est pas necessaire comme dans le cas des methodes Deep SVDD et ses variantes de laisser libres le rayon et le centre de la sph\\`ere calcul\\'ee. Afin de simplifier les calculs, nous allons faire le raisonnement dans un cas extr\\^emement simple. Supposons donc que nous n'avons qu'une seule donn\\'ee, not\\'ee $$. On suppose aussi qu'il n'y a qu'une couche et pas de fonction d'activation dans les r\\'eseaux impliqu\\'es. On peut facilement v\\'erifier que tout ce qui est dit ci-dessous s'adapte \\`a des configurations plus r\\'ealistes, cela rend simplement les notations plus lourdes. Pour comparer m\\'ethodes Deep SVDD et Deep SPH SVDD, on introduit deux fonctions de perte caract\\'erisant respectivement ces deux m\\'ethodes tout en se limitant \\`a ce qui les diff\\'erencie vraiment. Les probl\\`emes associ\\'es sont~:\\\\ && (W^*,^*,^*) = J_{cl}(W,,), J_{cl}(W,,) = ^2 + _W() - ^2, \\\\ && (W^*,^*) = J_{sph}(W,) , \\\\ \\\\ && J_{sph}(W,) = {} _W() - ^2 + {} ( _{i=1}^m c_i^2 - 2 s_{m+1} ) , \\ = (1, c_1,,c_m, s_{m+1}). \\\\ En effet, les coordonn\\'ees d'une sph\\`ere $$ dans l'espace conforme sont $$ = (1, c_1,, c_m,(\\|\\|^2 - ^2)/2)$$ o\\`u $(c_1,,c_m)$ sont les coordonn\\'ees du centre dans $",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 52,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_53",
      "text": "^m$ et $$ est le rayon de la sph\\`ere. Dans l'algorithme Deep SPH SVDD les paramètres de l'hypersph\\`ere sont donc intégrés dans un vecteur $$ qui encapsule à la fois le centre et une expression du rayon. On va voir que le couplage induit par cette structuration des inconnues a une influence sur la convergence de l'algorithme et explique l'absence de collapse observ\\'e avec Deep SPH SVDD. Pour reproduire l'apprentissage, on d\\'ecrit la r\\'esolution des probl\\`emes et par descente de gradient. On construit donc une suite $(y_n)_{n }$ selon l'algorithme suivant\\\\ \\{ {l} y_0 ,\\\\ y_{n+1} = y_n - J(y_n) \\ n 0, . \\\\ o\\`u $ >0$ est le taux d'apprentissage et :\\\\ -- pour le probl\\`eme , $J=J_{cl}$, $y ^{ W + m +1}$ contient les inconnues et peut s'\\'ecrire $y=(W,,)$ ; $ J = (_W J_{cl}, _{} J_{cl}, _{} J_{cl})$ ; \\\\ -- pour le probl\\`eme , $J=J_{sph}$, $y ^{ W + m +1}$ contient les inconnues et peut s'\\'ecrire $y=(W,,s_{m+1} )$ ; $ J = (_W J_{sph}, _{} J_{sph}, _{s_{m+1}} J_{sph})$.\\\\ Le r\\'eseau n'ayant qu'une couche cach\\'ee et pas de fonction d'activation, on consid\\`ere que $_W(x)=Wx$ et on calcule facilement les gradients~:\\\\ && _W J_{cl}(W,,) = 2 (_W()-) , \\\\ && _{} J_{cl}(W,,) = - 2",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 53,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_54",
      "text": "(_W()-) ,\\\\ && _{} J_{cl} (W,,) = 2 \\, ; \\\\ && _W J_{sph}(W,) = {} (_W()-) , \\\\ && _{} J_{sph}(W,) = - {} (_W()-) +2 {} = - {} ( _W() - ) , \\\\ && _{s_{m+1}} J_{sph} ( W,) = -2 {} . \\\\ Dans une descente de gradient telle que , la vitesse de la convergence est contr\\^ol\\'ee par la norme du gradient $ J$. Ici, la solution du problème est connue : $W^*=W_0$, $^*=(W_0)()=_0$, $^*=0$ (voir paragraphe ), c'est-\\`a-dire $^*=(1,_0, _0^2/2)$. Or, gr\\^ace aux calculs des gradients ci-dessus, on remarque que $$ _{(W,,) (W_0,_0,0 )} J_{cl}(W,,) =0$$ $$ _{(W,) (W_0,1,_0, _0^2/2 )} _W J_{sph}(W,) =0$$ mais _{(W,) (W_0,1,_0, _0^2/2 )} _{} J_{sph}(W,) = {} _0 0 _0 0_{^m}, \\\\ _{(W,) (W_0,1,_0, _0^2/2 )} _{s_{m+1}} J_{sph} ( W,) = {} 0 car le travail de d\\'etection d'anomalie n'a de sens que si $ <1$.\\\\ Ces limites montrent : Pendant que $y_n$ construit par et converge vers la solution $ (W_0,_0,0 )$, le gradient $ J(y_n)$ qui dirige la descente tend en norme vers z\\'ero : les pas sont donc de plus en plus petits, \\'evitant ainsi des oscillations autour de la solution. \\`A l'inverse, si $y_n$ est construit par et , seule la partie du gradient li\\'ee \\`a la convergence des param\\`etres $W$ du r\\'eseau",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 54,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_55",
      "text": "converge en norme vers z\\'ero ; Deep SVDD et Deep SPH SVDD traitent le m\\^eme probl\\`eme de minimisation mais Deep SPH SVDD est bas\\'e sur une formulation non strictement convexe du probl\\`eme~; En pratique, dans le cas de Deep SPH SVDD, les bonnes propri\\'et\\'es de convergence de la partie r\\'eseau, de $W$, permettent au terme $ _W()-$ de d\\'ecro\\^ tre et de se stabiliser rapidement, tandis que l'instabilit\\'e par rapport \\`a $s_{m+1}$ fait osciller le couple centre-rayon (surtout le rayon qui n'est pas stabilis\\'e par $W$), \\'evitant ainsi d'atteindre la solution optimale tout en convergeant vers cette dernière. Le caract\\`ere oscillant de $s_{m+1}$ ne peut pas \\^etre compens\\'e par une diminution du taux d'apprentissage (on ne peut pas choisir un taux inf\\'erieur \\`a la constante de Lipschitz des gradients par exemple, puisque cette constante est nulle pour une fonction constante). C'est le fait d'avoir transform\\'e la formulation du probl\\`eme sous la forme d'une minimisation non convexe qui permet d'apprendre le centre et le rayon. Ces consid\\'erations th\\'eoriques sont illustr\\'ees dans le paragraphe ci-dessous. Les exp\\'erimentations suivantes sont r\\'ealis\\'ees avec le jeu",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 55,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_56",
      "text": "de donn\\'ees { `easy'} d\\'ecrit au paragraphe On peut v\\'erifier que sur ce jeu de donn\\'ees, un algorithme Deep SVDD o\\`u le centre et le rayon sont appris conduit au collapse, l'hypersph\\`ere calcul\\'ee \\'etant de rayon nul et centr\\'ee en $_0= _{W_0}()$ o\\`u $W_0$ est le r\\'eseau dont tous les poids sont nuls. Le collapse est illustr\\'e en annexe \\`a la fin du chapitre, voir page . Comme annonc\\'e, une telle d\\'eg\\'en\\'erescence n'est pas observ\\'ee avec Deep SPH SVDD. Des illustrations sont donn\\'ees ci-dessous pour appuyer les r\\'esultats th\\'eoriques du paragraphe . Un exemple est donn\\'e dans la figure . On a utilis\\'e une fonction d'activation lin\\'eaire. On a donc $_0=_{W_0}()=0_{^m}$ (voir les notations au paragraphe ). Pour v\\'erifier que le r\\'eseau ne tend pas vers celui associ\\'e \\`a l'hypersph\\`ere d\\'eg\\'en\\'er\\'ee il suffit donc de v\\'erifier que la norme des $_W(_k)$, $_k$ d\\'esignant les donn\\'ees, ne tend pas vers z\\'ero. On voit aussi que les rayons ne tendent pas vers z\\'ero non plus. [H] {0.445} {0.445} _k)||$} L'algorithme Deep SPH SVDD est construit pour avoir de bonnes propri\\'et\\'es de convergence pour le r\\'eseau qui apprend en particulier l'espace des",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 56,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_57",
      "text": "caract\\'eristiques mais une convergence instable pour le rayon (voir paragraphe ). L'\\'evolution des diff\\'erents termes de la fonction de perte au fil des it\\'erations est d\\'etaill\\'ee dans la figure et . [H] [H] [H] On note dans la figure que le r\\'eseau envoie rapidement les donn\\'ees sur le bord de l'hypersph\\`ere et corrige de ce point de vue les oscillations du rayon. [H] ||$ et le rayon $$} Ainsi le r\\'eseau structure-t-il les donn\\'ees en les envoyant sur le bord de l'hypersph\\`ere plut\\^ot que dans l'hypersph\\`ere (du moins pour un choix pertinent de $$ pour la d\\'etection, voir aussi la figure au paragraphe suivant). Pour illustrer que le r\\'eseau envoie bien les donn\\'ees sur un espace pertinent de caract\\'eristiques, on peut mener l'exp\\'erimentation suivante : on force le r\\'eseau \\`a r\\'epartir de fa con uniforme les donn\\'ees sur le bord de ou dans l'hypersph\\`ere. Pour se faire, on définit deux distributions uniformes: Sur la sphère unité, $(^{m-1})$ où $^{m-1} = \\{\\, ^m : \\|\\| = 1 \\}$. Dans la boule unité, $(^m)$ où $^m = \\{\\, ^m : \\|\\| 1 \\}$. Pour chacune des distributions que l'on notera génériquement $$, on ajoutera à $J$ la pénalité suivante: J_{KL} = C~~D_{}",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 57,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_58",
      "text": "( \\| (_W() - ) / ) o\\`u $D_{}$ désigne la divergence de Kullback-Leibler. Le coefficient $C$ sert \\`a donner plus ou moins d'importance \\`a cette partie du loss. Divers r\\'esultats sont donn\\'es dans les figures , et . En premier lieu, on constate que le score AUC-ROC reste constant à partir de l'epoch 10200 (cf. figure ). En forçant la distribution des $_W()$ à suivre les deux distributions sus-cités, les scores AUC-ROC sont dégradés voire très dégradés et oscillent en permanence.\\\\ Au regard de la distribution des scores d'anomalies (cf. figure ), on constate que le seuil optimal de décision est déplacé du bord de la sphère unité (où le seuil devrait être à $0$), et la distribution apparaît plus étalée. \\\\ Dans le premier cas (avec ajout du terme \\( J_{KL} \\) pour forcer les points à se répartir uniformément dans la \\( ^m \\)), on observe que l'hypersphère laisse beaucoup de points normaux à l'extérieur, car la frontière est déplacée vers l'intérieur. \\\\ Dans le cas où l'on ajoute le terme \\( J_{KL} \\) pour forcer les points à se répartir sur la \\( ^{m-1} \\), l'hypersphère rejette également de nombreux points normaux à l'extérieur tout en incluant parfois des anomalies à l'intérie",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 58,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_59",
      "text": "ur. {0.3125} {0.3125} {0.3125} 6$.} {0.3125} {0.3125} {0.3125} {0.3125} {0.3125} {0.3125} L'absence de collapse observ\\'e est li\\'e (voir paragraphe ) \\`a une forme d'instabilit\\'e dans le calcul du rayon. Il est important de confirmer en pratique que cette instabilit\\'e perdure m\\^eme si le taux d'apprentissage est diminu\\'e (le r\\'esultat th\\'eorique l'affirme). Le comportement est v\\'erifi\\'e dans la figure , o\\`u l'on voit l'absence de tendance vers un collapse lorsque l'on diminue drastiquement le taux d'apprentissage. [H] 5$.} Les derni\\`eres illustrations concernent le remarque juste avant le paragraphe : contrairement \\`a la Deep SVDD, la Deep SPH SVDD n'est pas limit\\'ee \\`a des r\\'eseaux sans biais ou/et \\`a des fonctions d'activation born\\'ee placée après la dernière couche cachée.\\\\ Suivant la fonction d'activation, la figure illustre les résultats, toujours sur le même jeu de données, avec les fonctions d'activation , et . La fonction est définie comme suit : \\( (x) }{e^x + e^{-x}} \\), qui ramène les valeurs dans l'intervalle [-1, 1]. De même, la fonction est définie par \\( (x) {1 + e^{-x}} \\), qui ramène les valeurs dans l'intervalle [0, 1].\\\\ On constate tout d'abord",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 59,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_60",
      "text": "que les scores AUC-ROC sont largement amélior\\'es par rapport celui du modèle sans biais ($ 0.653$) pour les cas et , ce dernier produisant de l'instabilité (cf. figure ). Les distributions des scores d'anomalies sont comme attendues proches de la frontière de décision pour les données normales (cf. figure ). Dans le cas de l'activation , on constate un resserrement de la zone de décision autour de 0. Pour les frontières de décision induites, les résultats sont conformes à ce que l'on pouvait attendre (cf. figure ).\\\\ [htbp] {0.3125} } {0.3125} } {0.3125} } [htbp] {0.3125} } {0.3125} } {0.3125} } [htbp] {0.3125} } {0.3125} } {0.3125} } Pour finir, l'apport du biais a été testé. Cet enrichissement a donn\\'e de meilleurs r\\'esultats comme on peut le voir sur les figures . Avec biais, le rayon converge en oscillant autour de zéro (cf. figure ). En observant la moyenne sur les $_k$ des $||_W(_k)||$ que la solution optimale n'est pas atteinte car les poids de la couche cachée sont non nuls (ie. $W^* W_0$). \\\\ [H] {0.425} {0.425} [H] {0.425} {0.425} [H] {0.425} {0.425} Pour obtenir une vue d'ensemble visuelle simple, un réseau de neurones avec une seule couche hypersphérique (pour SPH S",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 60,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_61",
      "text": "VDD) et un modèle avec une couche cachée suivie d'une couche hypersphérique (pour Deep SPH SVDD) ont été testés sur des ensembles de points dans $^2$. Ensuite, la méthode Deep M-SPH SVDD est testée sur des ensembles de données de dimensions supérieures, à savoir MNIST et CIFAR-10. Les méthodes d'initialisation sont variées pour examiner leur impact sur les performances des modèles. La fonction de coût dans l'équation dépend du paramètre $ ~]0,1[$, qui contrôle le nombre de points en dehors de l'hypersphère. Pour vérifier la sensibilité de ce paramètre, la méthode SPH SVDD a été testée sur des ensembles de données synthétiques en dimension 2 ({ pt anomaly blob circle}) en faisant varier $$ (cf. figure ). À mesure que $$ augmente, l'algorithme rejette plus de points lors de l'apprentissage en les plaçant à l'extérieur de la sphère.\\\\ Les jeux de données utilisés sont les suivants : : 300 points normaux (en noir) et 30 points d'anomalie (en blanc) disposés en cercle autour de l'origine. : 300 points normaux (en noir) et 30 points d'anomalie (en blanc) disposés un peu plus loin de l'origine. Le premier jeu de données est plus simple que le second, car les points d'anomalie sont plus él",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 61,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_62",
      "text": "oignés des points normaux.\\\\ La figure montre la pré-image de sphère englobante (en rouge) au seuil 0 pour les deux jeux de données (sous-ensemble d'entraînement) trouvée par SPH SVDD avec $ = 0.001$ et $ = 0.5$. Les résultats montrent que pour un $$ petit, tous les points sont à l'intérieur de la sphère, tandis que pour un $$ plus grand, une proportion similaire de points est à l'extérieur de la sphère.\\\\ [htpb] {1} [H] {|c|c|c|} & $ = 0.001$ & $ = 0.5$ \\\\ {} & & \\\\ {} & & \\\\ L'évaluation des performances d'un modèle de classification peut être effectuée sur la base de trois critères : l'aire sous la courbe ROC (AUC-ROC), l'aire sous la courbe Précision-Rappel (AUC-PR) et le score F1. Typiquement, le critère le plus couramment utilisé est l'aire sous la courbe ROC.\\\\ Le tableau de confusion est un outil essentiel pour évaluer les performances d'un modèle de classification binaire. Le tableau de confusion présente les prédictions du modèle par rapport aux véritables classes cibles. Il permet de calculer diverses métriques comme la précision, le rappel, et le taux de faux positifs, qui sont utilisées pour tracer la courbe ROC et la courbe de Rappel-Précision. La structure d'un table",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 62,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_63",
      "text": "au de confusion se présente ainsi~: [H] {|c|c|c|} & & \\\\ & Vrai Positif (VP) & Faux Négatif (FN) \\\\ & Faux Positif (FP) & Vrai Négatif (VN) \\\\ Si l'on adapte le discours à la détection d'anomalies, les termes du tableau de confusion sont les suivants~:\\\\ : Nombre d'anomalies correctement identifiées par le modèle. : Nombre de fois où une anomalie n'a pas été détectée par le modèle. : Nombre de fois où le modèle a prédit une anomalie, mais la classe réelle est normale. : Nombre de fois où le modèle a prédit une classe normale et que la classe réelle est normale. On en déduit les métriques suivantes~: [Taux de Vrais Positifs (TVP)] ou ou encore : Il concerne la capacité du modèle à identifier les anomalies (à maximiser). $$ = }{ + }$$ : Il traduit la capacité du modèle à identifier les points normaux comme étant des anomalies (à minimiser). $$ = }{ + }$$ : Elle est définie comme le rapport des vrais positifs par rapport à l'ensemble des prédictions positives (vrais positifs + faux positifs). Elle permet de mesurer la qualité des prédictions positives (à maximiser). $$ = }{ + }$$ : Elle est définie comme le rapport des vrais négatifs par rapport à l'ensemble des prédictions négatives",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 63,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_64",
      "text": "(vrais négatifs + faux négatifs). Elle permet de mesurer la qualité des prédictions négatives (à maximiser). $$ = }{ + }$$ : Il s'agit de la moyenne harmonique entre la précision et le rappel. Il permet de trouver un compromis entre ces deux métriques en un seul score en favorisant les modèles qui ont des valeurs équilibrées de précision et de rappel (à maximiser). $$ = 2 }{ + }$$ En classification binaire (et donc en détection d'anomalies), il est nécessaire de trouver un compromis entre tous les critères sus-mentionnés: le rappel, la précision, la spécificité... (Ex.: score F1). D'autant que ces métriques dépendent du seuil de décision. Dans notre cas, le seuil de décision par défaut est 0, ce qui correspond à positionner la frontière de décision sur la sphère englobante.\\\\ Pour différentes valeurs de seuil, les métriques évoluent. On peut donc tracer des courbes pour visualiser ces évolutions. On peut ainsi tracer les courbes ROC et Précision-Rappel.\\\\ La courbe ROC, correspond dont à la courbe tracée par les points qui ont pour abscisse les TFP et pour ordonnée les TVP. L'AUC-ROC est l'aire sous la courbe de ROC et permet de quantifier la capacité du modèle à différencier les c",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 64,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_65",
      "text": "lasses positives des classes négatives. Sa valeur est comprise entre 0.5 et 1, où 1 correspond à un modèle parfait. La recherche d'un seuil de décision optimal peut être effectuée en cherchant le point le plus proche du coin supérieur gauche de la courbe ROC parmi les seuils testés $\\{_i\\}$: ^* = _{_i} ( (_i)^2 + (1 - (_i))^2 ) Nous avons appliqué cette méthode en post-traitement pour obtenir le seuil optimal dans l'ensemble de nos algorithmes dans le cas où un jeu de données de validation est disponible. En conséquence, cela améliore les performances de nos modèles mais ne fait plus correspondre la frontière de décision avec la sphère englobante. Cela impacte également la $$-propriété. L'AUC-PR (Aire sous la courbe rappel-précision) mesure également la performance du modèle en termes de précision et de rappel à différents seuils de décision. La courbe est tracée en représentant la précision en fonction du rappel. D'après la littérature , ce critère semble plus approprié dans le cas d'un problème à classes déséquilibrées. Cependant, elle peut mener à une interprétation trop optimiste des modèles.\\\\ [H] La figure , montre un exemple de tracé des courbes ROC et PR (Rappel-Précision).",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 65,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_66",
      "text": "Une régression logistique est utilisée et les prédictions sont seuillées avec différents seuils (en rouge). ^2$} Dans cette section, les méthodes présentées sont testées et comparées sur des jeux de données synthétiques, incluant la SVDD reprogrammée à partir des éléments décrits dans la section , les méthodes utilisant les réseaux de neurones à couche hypersphérique, ainsi que les méthodes classiques de la bibliothèque . Chaque jeu de données se compose de 300 points. Les points bleus représentent les données considérées comme normales, tandis que les points rouges indiquent les anomalies. Les jeux de données sont divisés en deux parties : l'une pour l'entraînement et l'autre pour le test, constitués donc de 150 points chacun. Les anomalies sont présentes uniquement dans l'ensemble de test, avec un taux de 15\\ [H] [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} Les types de données considérés sont des configurations variées d'anomalies, telles que des anomalies ponctuelles en forme de cercle, des blobs, et des données avec différentes structures comme les blobs, les données variées, anisotropes, lunaires, ou sans structure spécifique. Dans les modè",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 66,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_67",
      "text": "les testés, les centres sont initialisés à l'aide d'une distribution normale standard et les rayons sont initialisés à 1. L'optimisation est effectuée en utilisant la méthode Adam, avec un taux d'apprentissage initial de 0,001 qui diminue en utilisant la méthode de réduction de plateau avec un facteur de 0,1 et une patience de 50.\\\\ La Deep SVDD et la Deep SPH SVDD étant des méthodes d'apprentissage profond, chacune est composée d'un modèle de réseau de neurones : Deep SVDD : L'architecture du modèle comprend un encodeur séquentiel qui prend en entrée un vecteur et le traite à travers trois couches linéaires. La première couche réduit la dimension à 64, la seconde maintient cette dimension à 64, et la troisième couche réduit la dimension à celle des centres de l'hypersphère englobante. Chaque couche linéaire est suivie d'une activation LeakyReLU. Deep SPH SVDD : L'architecture du modèle comprend deux parties principales : un encodeur et une couche hypersphérique. L'encodeur est constitué d'une séquence de couches, débutant par une couche linéaire qui réduit la dimension de l'entrée à 64, suivie d'une activation , puis d'une seconde couche linéaire qui réduit à nouveau la dimension",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 67,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_68",
      "text": "à celle correspondant aux centres des hypersphères, suivie d'une autre activation . ( la méthode SPH SVDD est seulement composé de la couche hypersphérique). } Le tableau ci-dessous montre les résultats des scores AUC-ROC pour les modèles Enveloppe elliptique, OC-SVM, isolation forest, LOF et SVDD pour les différents jeux de données (cf. Section ). [H] {|l|c|c|c|c|c|} &[c]{@{}c@{}}Enveloppe \\\\ elliptique& [c]{@{}c@{}}OC-SVM \\\\ (RBF)& [c]{@{}c@{}}Isolation \\\\ Forest& LOF & SVDD\\\\ pt anomly circle & 0.86 & 0.91 & 0.86 & 0.80 & \\\\ pt anomaly blob circle & 0.90 & 0.89 & 0.90 & & 0.92 \\\\ point anomaly & 0.95 & 0.82 & 0.90 & & 0.83 \\\\ make blob & 0.93 & 0.92 & 0.93 & & 0.95 \\\\ make varied & 0.88 & 0.91 & 0.89 & 0.93 & \\\\ make aniso & 0.75 & & 0.86 & 0.85 & 0.85 \\\\ make moo, & 0.76 & 0.81 & 0.86 & 0.76 & \\\\ make no structure & 0.49 & 0.42 & 0.43 & 0.45 & \\\\ Globalement, la méthode SVDD a obtenu les meilleurs résultats dans plusieurs scénarios expérimentaux. En particulier, dans la détection des anomalies ponctuelles en forme de cercle avec un score de 0.99, ainsi que pour les données de type lune, avec un scores de 0.98. Ces résultats suggèrent que SVDD est efficace pour détecter des anom",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 68,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_69",
      "text": "alies dans des structures de données plus complexes et variées. Cette section présente les performances de plusieurs méthodes de détection d'anomalies : SVDD, Deep SVDD, SPH SVDD et Deep SPH SVDD. Les résultats sont évalués à l'aide de différentes métriques, notamment l'AUC-ROC, l'AUC-PR et le score F1 dans le but de comparer l'efficacité de chaque méthode sur divers jeux de données synthétiques. [H] {|c|c|c|c|c|} & & & & \\\\ & 0.99 & 0.90 & 0.99 & \\\\ & & 0.90 & 0.70 & \\\\ &0.83 & 0.77 & 0.89 & \\\\ & 0.95 & 0.99 & 0.95 & \\\\ & 0.95 & 0.96 & 0.76 & \\\\ & 0.85 & 0.86 & 0.66 & \\\\ & 0.93 & 0.93 & 0.58 & \\\\ & 0.67 & 0.58 & 0.47 & \\\\ [H] {|c|c|c|c|c|} & & & & \\\\ &0.99& 0.98 & 0.99 & 0.99 \\\\ &0.99 & 0.99 & 0.97 & 0.98 \\\\ &0.98 & 0.97 & 0.98 & 0.98 \\\\ &0.99 & 0.99 & 0.99 & 0.99 \\\\ &0.99 & 0.99 & 0.97 & 0.98 \\\\ &0.98 & 0.98 & 0.96 & 0.97 \\\\ &0.99 & 0.99 & 0.96 & 0.97 \\\\ &0.96 & 0.96 & 0.95 & 0.95 \\\\ [H] {|c|c|c|c|c|} & & & & \\\\ &0.99 & 0.88 & 0.99 & 0.97 \\\\ &0.91 & 0.89 & 0.97 & 0.96 \\\\ &0.79 & 0.70 & 0.98 & 0.99 \\\\ &0.99 & 0.99 & 0.99 & 0.99 \\\\ &0.99 & 0.99 & 0.97 & 0.98 \\\\ &0.98 & 0.98 & 0.96 & 0.97 \\\\ &0.99 & 0.99 & 0.96 & 0.97 \\\\ &0.96 & 0.96 & 0.92 & 0.95 \\\\ Le critère AUC-ROC est celui pou",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 69,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_70",
      "text": "r lequel les différences entre les méthodes sont les plus significatives. C'est donc sur ce critère que l'analyse va pouvoir être la plus pertinente pour différencier l'efficacité des méthodes sur les différents jeux de données. Dans l'ensemble, les méthodes SVDD et Deep SVDD sont similaires. On rappelle que nous sommes dans un cas où la SVDD est applicable car il y a peu de points dans chaque jeu de données. La méthode SPH SVDD ne contenant pas de couche cachée, cela peut expliquer la différences des résultats obtenus puisque la fonction de décision ne dépend que d'un seul neurone contenue dans la couche hypersphérique. Les résultats montrent que la méthode Deep SPH SVDD offre les meilleures performances globales dans la plupart des scénarios notamment avec les jeux de données \"make aniso\", \"make moon\" et même \"no structure\" où les données sont plus complexes par leur structure. Dans cette section, l'approche utilisant les réseaux de neurones basés sur les fonctions à bases radiales (RBF) est comparée à une approche similaire utilisant des couches hypersphériques. Les performances sont évaluées selon trois mêmes critères que précédemment. [H] {|c|c|c|c|c|c|c|} & {|c|}{ AUC-ROC} &",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 70,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_71",
      "text": "{|c|}{ AUC-PR} & {|c|}{score F1}\\\\ & & & & & & \\\\ && & 1.00 & 1.00 & 1.00 & 1.00\\\\ && 0.80 & 0.99 & 0.98 & 0.97 & 0.98 \\\\ &0.56& & 0.95 & 1.00 & 0.92 & 1.00 \\\\ &0.9& & 0.99 & 0.99 & 0.99 & 0.99\\\\ && 0.83 & 0.98 & 0.98 & 0.98 & 0.98 \\\\ && 0.63 & 0.97 & 0.96 & 0.96 & 0.96\\\\ && 0.67 & 0.80 & 0.96 & 0.97 & 0.93 \\\\ && 0.44 & 0.48 & 0.92 & 0.93 & 0.73 \\\\ Comme précédemment, les scores F1 et AUC-PR sont très proches pour les deux méthodes, avec des différences plus prononcées pour le score AUC-ROC. Hormis pour le jeu de données \"point anomaly\", où la méthode SPH Anomaly est beaucoup plus performante, elle ne se montre pas très convaincante par rapport à l'approche classique. Ce qui laisse suggérer que la methode proposée n'est pas efficace dans certaines configurations de données. Le tableau reprend les résultats obtenus entre les deux méthodes proposées Deep SPH SVDD et SPH Anomaly . Les scores AUC-ROC, AUC-PR et F1 sont présentés pour chaque jeu de données avec un focus particulier pour le score AUC-ROC.\\\\ [htbp] {|c|c|c|} & {|c|}{ AUC-ROC} \\\\ & & \\\\ & & 1.00 \\\\ & & 0.8 \\\\ & & 1.00 \\\\ & & 0.93 \\\\ & & 0.83 \\\\ & & 0.63 \\\\ & & 0.67 \\\\ & & 0.44 \\\\ Les résultats montrent que la méthode Deep",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 71,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_72",
      "text": "SPH SVDD est plus performante que la méthode SPH Anomaly pour tous les jeux de données. Ces résultats suggèrent que la méthode Deep SPH SVDD est plus efficace pour détecter des anomalies dans des structures de données plus complexes et variées.\\\\ Afin d'illustrer cela, nous avons tracé les frontières de décisions des sphères pour chaque méthode sur quelques jeux de données (cf. figure ). La méthode SPH Anomaly est évaluée avec deux types d'initialisation des paramètres : d’une part, une initialisation où le centre suit une loi normale $(0,1)$ avec un rayon fixé à 1, et d’autre part, une approche d'initialisation analogue à celle de la méthode -means++ (décrite dans la section ).\\\\ On constate immédiatement que les couches cachées permettent de mieux capturer la structure des données avec une seule sphère alors SPH Anomaly correspond \"simplement\" à un assemblage de sphères en dimension 2.\\\\ Ici on peut pointer le principal défaut de la méthode SPH Anomaly: la méthode contraint le rayon des sphères à être suffisamment grand pour que la sigmoïde soit proche de 1. La figure montre que le rayon doit être au minimum de 3. Dans le cas du jeu de données \"make aniso\", on observe ce forcage",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 72,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_73",
      "text": "du rayon sur la partie inférieure gauche. On déduit qu'il est nécessaire de faire précéder la dernière couche de la méthode SPH Anomaly d'une ou plusieurs couches cachées (vers une Deep SPH Anomaly ...).\\\\ On peut également observer que, pour des ensembles de données comme ou , la valeur de sortie obtenue dépasse 1 à certains endroits où il n'y a pas de données normales. En effet, comme l’illustre la figure , selon la position des centres et les rayons des sphères, celles-ci peuvent se chevaucher. Cela a pour conséquence d’additionner le signal, ce qui augmente la valeur de sortie. La méthode nécessite donc une réflexion supplémentaire afin de contraindre les sphères à ne pas s’intersecter. On observe que le déplacement des centres lors de l’initialisation améliore les résultats.\\\\ [htpb] {1} {|c|c|c|c|} &Deep SPH SVDD & SPH Anomaly & SPH Anomaly ($k$-means++)\\\\ { } & & & \\\\ { } & & & \\\\ \\\\ {1} {|c|c|c|c|} &Deep SPH SVDD & SPH Anomaly & SPH Anomaly ($k$-means++)\\\\ { } & & & \\\\ { } & & & \\\\ Dans cette section, les expériences menées visent à observer le comportement des réseaux de neurones dans le cas de l'utilisation de plusieurs hypersphères en utilisant le modèle Deep M-SPH SVDD",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 73,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_74",
      "text": "proposé. On testera d'abord sur un ensemble de données de points dans $^2$ pour une évaluation visuelle, puis sur les ensembles de données MNIST et CIFAR10. Cette expérimentation dans $^2$ a également pour ambition d'améliorer la compréhension du phénomène de collapse décrit dans . Le jeu de données synthétiques (nommé ) est composé de 350 points normaux générés à partir de trois groupes distincts et de 98 anomalies générées à partir d'une distribution uniforme tronquée pour être majoritairement à l'extérieur des points normaux; il s'agit d'une version modifiée des jeux de données de . Cette configuration est choisie pour être suffisamment simple afin de vérifier le comportement de la méthode: on s'attend à ce que les hypersphères englobent chaque groupe de points normaux tout en excluant les anomalies. L'ensemble de données est divisé en un ensemble d'entraînement de 175 points et un ensemble de test de 175+98 points, qui inclut les anomalies. Le modèle utilisé est constitué d'un encodeur qui joue le rôle de la fonction $_W$. Dans le modèle classique de Deep SVDD, l'encodeur comprend deux couches cachées linéaires sans biais suivies d'une BatchNorm et d'une activation LeakyReLU, t",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 74,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_75",
      "text": "andis le modèle proposé Deep M-SPH SVDD, utilise un à la place du LeakyReLU, contient des biais pour les couches linéaires et ajoute une couche hypersphérique en sortie.\\\\ Contrairement au modèle classique, où la sortie est de dimension $d$ (correspondant à la dimension du centre $$ de l'hypersphère), la sortie du modèle Deep M-SPH SVDD est un scalaire représentant directement le score d'anomalie $-2_j _i _k$ (cf. équation ).\\\\ Étant donné que les fonctions d'activation des couches cachées de l'encodeur sont des , l'initialisation des poids des couches linéaires se fait via la méthode par défault He uniforme. Les centres des hypersphères sont initialisés selon une distribution normale $(0,1)$, tandis que le rayon est fixé à 1.\\\\ Comme mentionné précédemment, le seuil de l'AUC-ROC est ajusté en fonction des scores d'anomalie sur un ensemble de validation et ne coïncide donc pas avec les frontières des hypersphères ajustées sur l'ensemble d'entraînement. Les autres hyperparamètres de la méthode ($$,$$) sont également définis à partir de l'ensemble de validation.\\\\ L'optimisation est effectuée en utilisant la méthode Adam, avec un taux d'apprentissage initial de 0.001 qui diminue en u",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 75,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_76",
      "text": "tilisant la méthode de réduction de plateau avec un facteur de 0.1 et une patience de 100. [htpb] _1$ et $_2$ de centre: $ 3.85$ et de rayon: $ = 4$ modifiées par des sigmoïdes, ainsi que la somme des deux termes} Cette section analyse l'application du modèle Deep M-SPH SVDD sur le jeu de données \"easy\", afin de visualiser son comportement. Trois configurations sont testées : avec 1, 3 et 10 hypersphères. Selon la méthode de détermination des paramètres de l'hypersphère, incluant le centre et le rayon, ces derniers peuvent être soit appris, soit fixés. Dans ce dernier cas, les gradients sont maintenus à donc fixés à 0.\\\\ modèle 1 : [{$J$: 1, $m$: 64, $$: 0.1, $$ : 0.0003321558199348189}] \\\\ modèle 2 : [{$J$: 3, $m$: 64, $$: 0.001, $$: 1.711799308608884e-6}] \\\\ modèle 3 : [{$J$: 10, $m$: 64, $$: 0.01, $$: 0.004318232518633555}] {6.5cm} [H] {0.45} {0.45} {0.99} {7cm} [H] {0.475} {non} appris.} {0.475} {0.475} {non} appris.} {0.475} {0.475} {non} appris.} {0.475} {8.9cm} [H] {0.495} {0.495} {non} appris.} {0.495} {non} appris.} {0.495} {0.495} {non} appris.} {0.495} \\\\ [H] {|c|c|c|} & rayon et centres {non} appris. & rayon et centres appris. \\\\ Modèle 1 & $0.85$ & $0.86$ \\\\ Modèle 2 &",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 76,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_77",
      "text": "$0.79$ & $0.91$ \\\\ Modèle 3 & $0.76$ & $0.88$ \\\\ La figure montrent les pré-images des hypersphères (c. à d.~ $ _i_W(x)=0$) (ligne pleine), tandis que la ligne pointillée représente la frontière de décision optimale _i_W(x)$, une valeur négative correspond à une valeur positive du score d'anomalie}. Étant donné que $$ est fixé à 0.001, l'hypersphère englobe toutes les données.\\\\ Pour le modèle 1 (avec les paramètres de $s$ non fixés, donc gradients non fixés), la frontière de décision est à l'intérieur de l'hypersphère puisque le seuil optimal est positif et égal à 9.9. L'AUC-ROC est a 0.86.\\\\ Pour le modèle 2 (toujours avec les paramètres de $s$ non fixés, donc rayons et centres appris), il peut être observé que les pré-images des trois sphères couvrent la majorité des points d'entraînement et ne s'intersectent pas. Cette dernière propriété est favorisée par la perte d'intersection. Il est à noter que les sphères finales correspondent approximativement aux trois groupes de points normaux. Le score AUC-ROC est amélioré à 0,91 avec un seuil de 0,004. De plus l'observation des distribution de sortie ( cf figure ), montre que dans cette configuration, les distributions sont moin écra",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 77,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_78",
      "text": "sées en zéro. \\\\ Lorsque plusieurs hypersphères sont rajoutées (avec 10 hypersphères et gradient toujours libre), voir le modèle 3 dans la figure , on observe que les rayons de nombreuses sphères convergent vers 0, ce qui signifie que ces hypersphères dégénèrent en points. Bien que l'observation des frontières de décisions correspondant à ce modèle présente la frontière de trois hypersphères, quatre rayons sont non nulles. En effet, le score d'anomalie donnée par cette quatrième sphère étant négatif, il n y a pas de frontière en zéro. Un inspection minutieuse de $_j$.$_W(x_k)$ a montré que cette hypersphère ne couvre aucun point. \\\\ Il est constaté que la liberté laissée aux paramètres améliore les résultats, en particulier le score AUC-ROC (cf. tableau ). De plus, les distributions (cf. ) montrent que les points normaux obtiennent des scores positifs, contrairement au cas où les gradients sont fixés. Cela permet d'établir une frontière de décision proche du seuil, comme illustré dans la figure . Les ensembles de données MNIST et CIFAR10 sont largement utilisés pour évaluer les méthodes de détection d'anomalies. Selon la méthodologie couramment adoptée, les expériences comparent un",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 78,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_79",
      "text": "e classe à toutes les autres (\"OneVsAll\"). L'ensemble d'entraînement contient 5000 images de la classe normale, tandis que l'ensemble de test contient 1000 images de chaque classe, soit 10000 images de test avec 90 \\ Les figures et illustrent les architectures des modèles Deep SVDD appliquées aux ensembles de données MNIST et CIFAR-10 qui ont été utilisés dans . Ces réseaux de neurones convolutifs sont conçus pour détecter les anomalies en apprenant une représentation compacte des données normales vers un espace de caractéristiques. Chaque architecture comprend plusieurs couches de convolution, de normalisation, et d'activation LeakyReLU présenté dans les figures. S'enchaînent ensuite une couche pour mettre à plat le tenseur et une couche linéaire finale. Les paramètres de centre et de rayon de l'hypersphere $[C, ]$ sont simplement déclarés comme des tenseurs () et non en tant que paramètres d'une couche linéaire. Ils ne seront pas mis à jour lors de l'optimisation. \\\\ 1{1.5} [h] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(1, 28, 28)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(8, 28, 28)$}; (bn1) [rectangle, draw, right of=conv1, rot",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 79,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_80",
      "text": "ate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(8, 14, 14)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(4, 14, 14)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(4, 7, 7)$}; (flatten) [rectangle, draw, right of=pool2, rotate=90] { Flatten (196)}; (linear) [rectangle, draw=blue, right of=flatten, rotate=90] { Linear $(32)$}; (sph) [rectangle, draw=blue, below of=linear, rotate=90, node distance=2cm] { $[C, ]$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (sph) -- (linear); [h] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(3, 32, 32)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(32, 32, 32)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { Ba",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 80,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_81",
      "text": "tchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(32, 16, 16)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(64, 16, 16)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(64, 8, 8)$}; (conv3) [rectangle, draw, right of=pool2, rotate=90] { Conv $(128, 4, 4)$}; (bn3) [rectangle, draw, right of=conv3, rotate=90] { BatchNorm}; (lrelu3) [rectangle, draw, right of=bn3, rotate=90] { LeakyReLU}; (flatten) [rectangle, draw, right of=lrelu3, rotate=90] { Flatten (2048)}; (linear) [rectangle, draw=blue, right of=flatten, rotate=90] { Linear $(128)$}; (sph) [rectangle, draw=blue, below of=linear, rotate=90, node distance=2cm] { $[C, ]$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (conv3); [->] (conv3) -- (bn3); [->] (bn3) -- (lrelu3); [->] (lrelu3) -- (flatten); [->] (flat",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 81,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_82",
      "text": "ten) -- (linear); [->, draw=red] (sph) -- (linear); Les modèles suivent une architecture similaire à celle utilisée pour la méthode Deep SVDD mais ajoutent une couche sphérique à la fin. Contrairement au modèle Deep SVDD, ici les paramètres d'hypersphère sont intégrés dans la couche sphérique, ce qui transforme la sortie linéaire en une représentation unidimensionnelle par hypersphère. Cela signifie que pour $m$ hypersphères utilisées, le tenseur de sortie est de taille $m$ et pour le modèle Deep SPH SVDD, il est de taille 1.\\\\ Concernant les paramètres d'apprentissage, L'hyperparamètre $$ est fixé à 1e-4 pour MNIST et à 0,01 pour CIFAR-10. Le taux d'apprentissage est initialisé à 1e-4 et réduit d'un facteur de 0,1 lorsque le plateau est atteint (patience = 5). Le nombre d'epochs est limité à 2500. Le terme de régularisation est la somme des normes $L_2$ des poids du réseau avec $ = 1-6$. Chaque noyau de convolution a une taille de (5,5). 1{1.5} [H] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(1, 28, 28)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(8, 28, 28)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1)",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 82,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_83",
      "text": "[rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(8, 14, 14)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(4, 14, 14)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(4, 7, 7)$}; (flatten) [rectangle, draw, right of=pool2, rotate=90] { Flatten (196)}; (linear) [rectangle, draw=red, right of=flatten, rotate=90] { Linear $(32)$}; (sph) [rectangle, draw=red, right of=linear, rotate=90] { Spherical $[C, ]$ $ (m)$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (linear) -- (sph); [H] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(3, 32, 32)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(32, 32, 32)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 83,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_84",
      "text": ", right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(32, 16, 16)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(64, 16, 16)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(64, 8, 8)$}; (conv3) [rectangle, draw, right of=pool2, rotate=90] { Conv $(128, 4, 4)$}; (bn3) [rectangle, draw, right of=conv3, rotate=90] { BatchNorm}; (lrelu3) [rectangle, draw, right of=bn3, rotate=90] { LeakyReLU}; (flatten) [rectangle, draw, right of=lrelu3, rotate=90] { Flatten (2048)}; (linear) [rectangle, draw=red, right of=flatten, rotate=90] { Linear $(128)$}; (sph) [rectangle, draw=red, right of=linear, rotate=90] { Spherical $[C, ]$ $(m)$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (conv3); [->] (conv3) -- (bn3); [->] (bn3) -- (lrelu3); [->] (lrelu3) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (linear",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 84,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_85",
      "text": ") -- (sph); où $m$ est le nombre d'hypersphères utilisées dans la couche finale. Pour l'initialisation de la première partie du réseau (encodeur), nous avons testé deux stratégies: Un autoencodeur pré-entraîné sur les données normales (AE) (mentionné par AE dans les tableaux ). Une initialisation des poids selon la méthode de Glorot (mentionné comme $$ dans les mêmes tableaux). Dans les modèles testés, si le rayon est initialisé à zéro, les paramètres d'hypersphère sont fixes. Cependant, dans le second cas, l'initialisation du rayon est déterminée par la méthode d'initialisation des centres.\\\\ Les centres sont initialisés soit en suivant une distribution normale standard (désignée par $(0,1)$ dans les figures et ), avec un rayon initialisé à 1, soit en utilisant la méthode $k$-means++ , suivie d'une étape d'assignation au plus proche voisin, puis d'un calcul de la distance moyenne aux centres pour déterminer les rayons (notés dist. $_J$ dans les tableaux de résultats, où $m$ est le nombre d'hypersphères utilisées). Dans les deux cas, les paramètres des hypersphères sont entraînés par le modèle.\\\\ Les tableaux ci-dessous présentent les résultats pour les différentes méthodes utilisé",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 85,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_86",
      "text": "es (Deep M-SPH SVDD, Deep M-SPH SVDD ainsi que Deep SVDD et les références de l'article ) correspondant à l'aire sous la courbe de ROC en pourcentage (AUC-ROC). Les résultats sont analysés en fonction des paramètres d'initialisation du centre $$, de la fixation ou non des paramètres de rayon et de centre (dans , , et , si les rayons et les centres sont appris, il sont mentionnés dans la ligne ), de la valeur initiale du rayon ($0$, $1$ ou $_m$), et de l'initialisation des poids des couches précédentes avec ceux de l'autoencodeur pour la couche hypersphérique (avec $m$ le nombre d'hypersphères considéré). Cela implique que pour la Deep SPH SVDD, m=1.\\\\ Les résultats observés dans les figures et révèlent des tendances contrastées en fonction des méthodes et des ensembles de données utilisés. L'effet du pré-entraînement avec un autoencodeur (AE) semble bénéfique pour les modèles appliqués à MNIST mais moins favorable pour ceux appliqués à CIFAR10.\\\\ Concernant l'impact des paramètres, les modèles basés sur Deep OC-SVDD montrent généralement de meilleures performances par rapport à Deep soft-bound SVDD sur les deux ensembles de données. Il est donc préférable d'entraîner le modèle avec",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 86,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_87",
      "text": "un rayon fixé à zéro plutôt que d'utiliser une hypersphère de rayon 1 pour les modèles classiques.\\\\ Pour les modèles proposés comme Deep M SPH-SVDD, les performances sont meilleures lorsque les paramètres ne sont pas fixés, alignant ainsi ces modèles plus près de la méthode Deep soft-bound SVDD. Cependant, une exception notable se produit lorsque les centres sont initialisés en utilisant une distribution normale réduite centrée ($c ( 0, 1)$) pour les données CIFAR10.\\\\ En termes d'impact de l'initialisation, pour MNIST, initialiser les centres avec la méthode $k$-means++ donne de meilleures performances par rapport à l'initialisation avec une distribution normale réduite centrée. Ce qui est l'inverse pour les modèles traitant CIFAR10, hormis le cas où les paramètres de centre et de rayon sont fixés. Dans ce cas, les résultats sont supérieurs lorsque les centres sont initialisés avec $k$-means++ et les rayons sont initialisés en fonction des paramètres $_m$.\\\\ En résumé, ces observations soulignent l'importance critique de l'initialisation des paramètres du modèle adaptée à la complexité et à la nature des données pour la détection d'anomalies. L'ensemble de données CIFAR10 présen",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 87,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_88",
      "text": "te une tâche de détection d'anomalies plus difficile comparée à MNIST, ce qui se reflète dans des scores AUC-ROC généralement plus bas. De plus , les résultats montrent que la méthode Deep M-SPH SVDD fonctionne mieux lorsque les autres méthodes échouent, mais est moins performante lorsque les autres méthodes réussissent. La classe \"oiseau\" est connue pour être particulièrement difficile, comme en témoigne la large dispersion de son encodage en deux dimensions (voir ). Pour cette classe, la méthode Deep M-SPH SVDD avec tous les paramètres fixés obtient constamment le score AUC-ROC le plus élevé avec une marge significative. [H] {!}{ {|c|c|c|c|c||c|c|c|c||c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{{@{}c@{}}Soft Bound \\\\ Deep SVDD} & {c|}{{@{}c@{}}Deep \\\\ OC SVDD} \\\\ Paramètres appris & $(W)$ & $(W,,)$ & $(W)$ & $(W,,)$ & $(W)$ &$(W,,)$ & $(W)$ & $(W,,)$ & $(W)$ & $(W,)$\\\\ Initialisation $_W$ &{c|}{$$} & {c||}{AE} & {c|}{$$} & {c||}{AE} & {c|}{AE} \\\\ centre init &{c||}{$c ( 0, 1)$ } &{c||}{$k$-means++ } & - & -\\\\ rayon init & 0 & 1 & 0 & 1& 0&dist. $_1$& 0 & dist. $_1$& 0 & 1\\\\ 0 & 0.9389 & 0.9742 & 0.9609 & 0.9839 & 0.9384 & & 0.9682 & 0.9811 & 0.9800 & 0.9780 \\\\ 1 & 0.9894 & 0.9941 & 0.",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 88,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_89",
      "text": "9696 & 0.9946 & 0.9937 & & 0.9937 & 0.9951 & 0.9970 & 0.9960 \\\\ 2 & 0.7580 & 0.7817 & 0.8002 & 0.8608 & 0.8332 & 0.8777 & 0.9035 & & 0.9170 & 0.8950 \\\\ 3 & 0.8420 & 0.8741 & 0.8729 & 0.9297 & 0.8735 & 0.8978 & 0.8803 & 0.8689 & & 0.9030 \\\\ 4 & 0.8945 & 0.9353 & 0.9123 & 0.9441 & 0.9125 & 0.9369 & 0.9081 & 0.9371 & & 0.9380 \\\\ 5 & 0.8128 & 0.8457 & 0.8583 & 0.8563 & 0.8229 & 0.8578 & 0.8192 & & 0.8850 & 0.8580 \\\\ 6 & 0.9309 & 0.9652 & 0.9400 & 0.9872 & 0.9752 & 0.9743 & 0.9819 & & 0.9830 & 0.9800 \\\\ 7 & 0.8777 & 0.9286 & 0.8602 & 0.9285 & 0.9474 & 0.9378 & 0.9178 & 0.9280 & & 0.9270 \\\\ 8 & 0.7846 & 0.8925 & 0.9218 & 0.9263 & 0.9138 & 0.9045 & 0.9210 & & 0.9390 & 0.9290 \\\\ 9 & 0.8818 & 0.9513 & 0.8919 & 0.9528 & 0.9605 & 0.9598 & 0.9638 & & 0.9650 & 0.9490 \\\\ } : MNIST, 1-sphère, dimension 32.} [H] {!}{ {|c|c|c|c|c||c|c|c|c||c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{{@{}c@{}}Soft Bound \\\\ Deep SVDD} & {c|}{{@{}c@{}}Deep \\\\ OC SVDD} \\\\ paramètres appris & $(W)$ & $(W,,)$ & $(W,,)$ & $(W)$ & $(W)$ &$(W,,)$ &$(W,,)$ & $(W)$ & $(W)$ & $(W,)$\\\\ Initialisation $_W$ & {c|}{$$} & {c||}{AE} & {c|}{$$} & {c||}{AE} & {c|}{AE} \\\\ centre init &{c||}{$c ( 0, 1)$ } &{c||}{$k$-means++} & - & -\\\\ rayon",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 89,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_90",
      "text": "init & 0 & 1 & 0 & 1 & 0 & dist. $_1$ & 0 & dist. $_1$ & 0 & 1 \\\\ 0 & 0.6087 & 0.6360 & 0.6789 & 0.6662 & & 0.6477 & 0.6583 & 0.6458 & 0.617 & 0.617 \\\\ 1 & 0.5838 & 0.5711 & 0.4743 & 0.4762 & 0.5106 & 0.4991 & 0.5338 & 0.4993 & & 0.648 \\\\ 2 & 0.6530 & 0.6641 & 0.6467 & 0.6429 & & 0.6503 & 0.6427 & 0.6474 & 0.508 & 0.495 \\\\ 3 & 0.5652 & 0.5766 & 0.5274 & 0.5304 & & 0.5608 & 0.5344 & 0.5483 & 0.591 & 0.56 \\\\ 4 & 0.7246 & 0.7306 & 0.7276 & 0.7224 & & 0.7041 & 0.7404 & 0.7206 & 0.609 & 0.591 \\\\ 5 & 0.6093 & 0.6283 & 0.5257 & 0.5409 & & 0.5704 & 0.5168 & 0.5191 & 0.657 & 0.621 \\\\ 6 & 0.7184 & 0.7160 & 0.7610 & 0.7626 & 0.6843 & 0.7276 & 0.7594 & & 0.677 & 0.678 \\\\ 7 & 0.5736 & 0.5638 & 0.5204 & 0.5229 & 0.5603 & & 0.5380 & 0.5261 & 0.673 & 0.652 \\\\ 8 & 0.6794 & 0.7241 & 0.6409 & 0.6590 & 0.7574 & 0.7315 & 0.6429 & 0.6353 & & 0.756 \\\\ 9 & 0.5554 & 0.5700 & 0.4987 & 0.5362 & 0.5430 & 0.5902 & 0.5757 & 0.5411 & & 0.71 \\\\ } : CIFAR-10, 1-sphère, dimension 128. } La prochaine étape consiste à analyser s'il est bénéfique d'utiliser davantage de neurones avec des dimensions plus petites. Pour maintenir un nombre approximatif de paramètres, la dimension des hypersphères est divisée par le nombr",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 90,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_91",
      "text": "e d'hypersphères utilisées dans la couche. Pour MNIST, cela implique de passer d'une seule sphère de dimension 64 à quatre sphères de dimension 16, et pour CIFAR10, cela signifie passer d'une sphère de dimension 128 à quatre sphères de dimension 32. Cela augmente le nombre de paramètres de 3. Les observations faites précédemment ne varient pas avec les changements dans le nombre de sphères à travers les modèles. [H] {!}{ {|c|c|c|c|c||c|c|c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{Deep M-SPH SVDD } \\\\ m, dim & {c|}{1, 64} & {c||}{4, 16} & {c|}{1, 64} & {c|}{4, 16} \\\\ paramètres appris & {c||}{$(W)$}& {c|}{$(W,,)$}\\\\ Initialisation $_W$ & {c|}{AE }\\\\ centre init & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++\\\\ rayon init & 0 & dist. $_1$ & 0 & dist. $_m$ & 1 & dist. $_1$ & 1 & dist. $_m$ \\\\ 0 & 0.9764 & & 0.8715 & 0.9742 & & 0.9824 & 0.9751 & 0.9730 \\\\ 1 & 0.9864 & 0.9932 & 0.9541 & 0.9940 & 0.9961 & 0.9956 & & 0.9949 \\\\ 2 & 0.8654 & & 0.8019 & 0.9008 & 0.8965 & 0.9117 & 0.8624 & 0.8857 \\\\ 3 & 0.8930 & 0.8909 & 0.7385 & 0.8791 & 0.9036 & & 0.8932 & 0.8855 \\\\ 4 & & 0.8964 & 0.9018 & 0.9085 & 0.9288 & 0.9381 & 0.9205 & 0.9221 \\\\ 5 &",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 91,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_92",
      "text": "& 0.8518 & 0.8209 & 0.8356 & 0.8739 & 0.8921 & 0.8185 & 0.8413 \\\\ 6 & 0.9824 & 0.9824 & 0.9028 & 0.9755 & & 0.9894 & 0.9726 & 0.9779 \\\\ 7 & 0.9182 & 0.9098 & 0.8561 & 0.9175 & & 0.9335 & 0.9023 & 0.9102 \\\\ 8 & 0.9422 & 0.9219 & 0.8810 & 0.9122 & 0.9225 & & 0.9279 & 0.9005 \\\\ 9 & 0.9430 & 0.9611 & 0.8321 & 0.9546 & & 0.9676 & 0.9517 & 0.9536 \\\\ } : MNIST.} [H] {!}{ {|c|c|c|c|c||c|c|c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{Deep M-SPH SVDD } \\\\ m, dim & {c|}{1, 128} & {c||}{4, 32} & {c|}{1, 128} & {c|}{4, 32} \\\\ paramètres appris & {c||}{$(W)$}& {c|}{$(W,,)$}\\\\ Initialisation $_W$ & {c|}{$$ }\\\\ centre init & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++\\\\ rayon init & 0 & dist. $_1$ & 0 & dist. $_m$ & 1 & dist. $_1$ & 1 & dist. $_m$ \\\\ 0 & 0.6087 & & 0.5709 & 0.6367 & 0.6360 & 0.6341 & 0.6124 & 0.5956 \\\\ 1 & 0.5838 & 0.5127 & 0.5027 & 0.5094 & & 0.5538 & 0.4729 & 0.5004 \\\\ 2 & 0.6530 & & 0.6288 & 0.6640 & 0.6641 & 0.6590 & 0.6527 & 0.6477 \\\\ 3 & 0.5652 & & 0.5510 & 0.5603 & 0.5766 & 0.5711 & 0.5469 & 0.5349 \\\\ 4 & 0.7246 & 0.7498 & 0.6931 & & 0.7306 & 0.7071 & 0.6987 & 0.7041 \\\\ 5 & 0.6093 & & 0.5392 & 0.5547 & 0.6283 & 0.5832 & 0.",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 92,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_93",
      "text": "5731 & 0.5400 \\\\ 6 & & 0.6843 & 0.7012 & 0.7078 & 0.7160 & 0.7149 & 0.7005 & 0.7181 \\\\ 7 & 0.5736 & 0.5603 & 0.5378 & 0.5333 & & 0.5638 & 0.5322 & 0.5398 \\\\ 8 & 0.6794 & & 0.6474 & 0.6970 & 0.7241 & 0.7247 & 0.6769 & 0.6543 \\\\ 9 & 0.5554 & 0.5430 & 0.4944 & 0.5619 & 0.5700 & & 0.5195 & 0.5284 \\\\ } : CIFAR-10.} Comme le montrent les courbes de fonction de perte, elles diminuent avec les modèles utilisant davantage de neurones de dimensions plus petites. Cependant, cela ne garantit pas de meilleures performances. En effet, pour tous les modèles, les performances diminuent selon ce critère. Plus précisément, le calcul des scores moyens d'évaluation des méthodes par classe d'anomalies révèle une diminution de l'AUC-ROC de 2\\ Ces expériences se concluent par une comparaison de la méthode Deep M-SPH SVDD avec celle de sur un sous-ensemble de CIFAR10 ({Automobile, Truck} vs \"animaux\" := {Bird, Horse, ...}). Les auteurs de semblent avoir utilisé la même architecture que Ruff et al. et ont obtenu un AUC-ROC de 0,663. La méthode proposée atteint un score de 0,71 en utilisant Deep M-SPH SVDD avec 1 hypersphère de dimension 128, paramètres non fixés. La différence notable entre les deux méthod",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 93,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_94",
      "text": "es réside dans le fait que, pour Deep M-SPH SVDD, les paramètres de l'encodeur sont appris par le réseau. Cependant, une diminution des performances est observée avec un score de 0,65 lors de l'utilisation de 4 hypersphères en dimension 32. Ce phénomène est similaire à celui observé dans les ensembles de données de points dans \\(^2\\), ce qui peut expliquer cette baisse de performance. En effet, les courbes montrant les rayons (cf. figure et ) des hypersphères en fonction du nombre d'epochs selon le modèle utilisé indiquent qu'une seule hypersphère ne dégénère pas en un point lorsque ce modèle utilise plusieurs hypersphères. Cela implique que sur des données de haute dimension, le réseau utilise uniquement une sphère dont la dimension est par conséquent réduite.\\\\ L'observation des courbes de fonctions de pertes (cf. figure et ), montre que les modèles composé de plusieurs hypersphères convergent d'avantage vers 0. Mais cette convergence ne garantit pas une meilleure performance du modèle, si l'on compare les scores AUC-ROC respectifs des modèles. En revanche, on peut observer que pour les modèles dont le rayon est plus petit, c'est-à-dire ceux contenant plusieurs hypersphères, la {",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 94,
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_95",
      "text": "loss} est également plus petite. [H] [b]{0.475} [b]{0.475} (0,1)$ )} [H] [b]{0.475} [b]{0.475} La figure présente les histogrammes des distributions de sortie de Deep M-SPH SVDD pour différentes configurations de paramètres. Ici encore, nous avons cherché des configurations avec un nombre de paramètres comparable. Comme les histogrammes présentés dans la section , les distributions de sortie ne correspondent évidement pas à une gaussienne, mais on tendance à former un pic autour de 0. Cela indique que l'encodeur envoie les données d'entrée vers le bord de l'hypersphère.\\\\ Pour les données MNIST, les histogrammes entre les données d'entraînement et les données de test, se chevauchent, mais sur un petit intervalle. Ce n'est pas le cas avec les histogrammes des données CIFAR10, ce qui montre la difficulté à définir un seuil optimal pour des données plus complexes. (0,1)$} =0 =1 =2 =3 =4 =5 =6 =7 =8 =9 [H] [b]{0.45} =1$, , } [b]{0.45} =1$, ,} [b]{0.45} =1$, , } [b]{0.45} =1$, ,}",
      "source_file": "chapitre3.tex.txt",
      "chunk_index": 95,
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_0",
      "text": "{chapter}{Conclusion} Ce manuscrit a exploré une variante des couches classiques de réseaux de neurones en remplaçant les hyperplans par des hypersphères. Cette approche, basée sur l'algèbre géométrique conforme, offre un cadre mathématique puissant pour la modélisation de structures complexes dans des espaces de données.\\\\ L'objectif principal de cette thèse était d'étudier le potentiel des couches hypersphériques pour la détection d'anomalies. La motivation derrière cette approche réside dans la capacité des hypersphères à définir des frontières de décision non linéaires, ce qui les rend particulièrement adaptées à la modélisation de données complexes.\\\\ Pour résumer les principaux résultats et contributions de ce travail, on peut énoncer les points suivant~: : A travers ce manuscrit, nous avons présenté en détail le formalisme mathématique des couches hypersphériques, en s'appuyant sur l'algèbre géométrique conforme pour représenter les hyperplans et les hypersphères de manière unifiée. Des formules explicites pour les couches denses et convolutives ont été décrites, permettant une implémentation pratique de ces couches dans des architectures de réseaux neuronaux.\\\\ : L'initiali",
      "source_file": "conclusion.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_1",
      "text": "sation des paramètres des couches hypersphériques est un élément important. Des méthodes plus robustes et adaptatives ont été développées pour garantir une convergence stable et optimale de l'apprentissage.\\\\ : Un théorème d'approximation a été établi, démontrant que les réseaux de neurones à couches hypersphériques peuvent approximer des fonctions continues définies sur un compact. Ce résultat théorique important confirme la capacité des couches hypersphériques à modéliser une large gamme de fonctions et justifie leur utilisation dans des tâches d'apprentissage automatique.\\\\ : Nous avons proposé deux nouveaux algorithmes de détection d'anomalies basés sur des couches hypersphériques : Deep sph-SVDD et Deep M sph-SVDD. Ces algorithmes, inspirés de la méthode Support Vector Data Description (SVDD), apprennent les paramètres du centre et du rayon d'une ou plusieurs hypersphères dans un espace transformé, permettant une meilleure séparation des données normales et des anomalies.\\\\ : Le manuscrit a analysé en détail le phénomène de collapse observé dans les algorithmes Deep SVDD classiques, où le rayon de l'hypersphère tend vers zéro. Il a été démontré que les algorithmes Deep sph-SVD",
      "source_file": "conclusion.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_2",
      "text": "D proposés sont immunisés contre ce problème, grâce à une formulation non convexe du problème d'optimisation. Cette robustesse permet d'apprendre le centre et le rayon de l'hypersphère et d'utiliser des biais et des fonctions d'activation bornées dans le réseau.\\\\ : Des expériences approfondies ont été menées sur des ensembles de données synthétiques et réels, notamment MNIST et CIFAR-10, pour valider les algorithmes proposés et les comparer aux méthodes classiques. Les résultats ont montré que les couches hypersphériques peuvent pour certains jeux de données améliorer les performances de détection d'anomalies et offrir une meilleure interprétabilité des modèles. Malgré les contributions significatives de cette thèse, certaines limites persistent et ouvrent des perspectives pour des recherches futures~:\\\\ : Dans le cas des modèles Deep M sph-SVDD, la détermination du nombre optimal d'hypersphères à utiliser reste une question ouverte. Des recherches supplémentaires sont nécessaires pour développer des méthodes permettant de choisir ce paramètre de manière automatique et efficace.\\\\ : Les couches hypersphériques ont un potentiel d'application au-delà de la détection d'anomalies. Exp",
      "source_file": "conclusion.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_3",
      "text": "lorer leur utilisation dans d'autres tâches d'apprentissage automatique, telles que la classification, la régression et la segmentation, constitue une perspective prometteuse. Cette thèse a visé à démontrer le potentiel des couches hypersphériques pour l'apprentissage automatique, en particulier pour la détection d'anomalies. Les contributions de ce travail ouvrent des perspectives prometteuses pour le développement de modèles plus performants et interprétables, capables de capturer la complexité des données et de détecter les anomalies de manière efficace. Des recherches futures sont nécessaires pour relever les défis restants et étendre l'application des couches hypersphériques à d'autres domaines de l'apprentissage automatique.",
      "source_file": "conclusion.tex.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_0",
      "text": "# Corpus fondamental pour Mia — version thématique robuste",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_1",
      "text": "## RH - Apport en entreprise\n\nJulien peut apporter un prototype fonctionnel, des solutions de détection d'anomalies, la validation de méthodes innovantes et contribuer à la mise en production de projets IA.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_2",
      "text": "## RH - Bénéfice produit/service\n\nJulien apporte la détection d'anomalies, la supervision intelligente et l'industrialisation de solutions IA pour améliorer la qualité des produits et services.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_3",
      "text": "## RH - Disponibilité\n\nLa disponibilité de Julien est précisée dans son CV. Pour tout contact, veuillez utiliser l'email ou le formulaire de contact.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_4",
      "text": "## RH - Références\n\nJulien a travaillé dans des laboratoires comme MIA, XLIM, et a été encadré par des directeurs de thèse reconnus. Ses encadrants et laboratoires sont mentionnés dans son CV.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_5",
      "text": "## RH - Travail en équipe non-technique\n\nJulien pratique la vulgarisation scientifique, sait travailler en équipe et communiquer avec des profils non techniques.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_6",
      "text": "## RH - Management\n\nJulien préfère se concentrer sur la technique, la mission et la résolution de problèmes complexes plutôt que d'évoluer vers le management.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_7",
      "text": "## Technique - Qui est Julien ?\n\nJulien est docteur en informatique appliquée, spécialisé en réseaux neuronaux hypersphériques et détection d'anomalies.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_8",
      "text": "## Technique - Compétences IA\n\nJulien maîtrise la théorie, les CNN, RNN, GAN, Transformers, auto-encodeurs et l'optimisation bayésienne.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 8,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_9",
      "text": "## Technique - Publications\n\nVoici les principales publications de Julien :\n- J. de Saint Angel, C. Saint-Jean, C. Choquet – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, chapitre de livre, Recent Applications in Deep Learning, 2025.\n- J. de Saint Angel, C. Saint-Jean – Couches Dense et Conv2d sphériques via l'algèbre géométrique conforme, ORASIS, 2021.\n- J. de Saint Angel, C. Saint-Jean – Théorème d'approximation pour neurones hypersphériques, GRETSI, 2023.\n- J. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 9,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_10",
      "text": "## Qualité - Fiabilité\n\nOui, Julien est fiable et possède de solides compétences scientifiques et techniques.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 10,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_11",
      "text": "## RH - Encadrement\n\nJulien a été enseignant fonctionnaire au lycée (secondaire). Il n'a jamais encadré d'étudiants au niveau universitaire.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 11,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_12",
      "text": "## RH - Gestion de la pression et délais\n\nJulien est très organisé et anticipe les obstacles grâce à une planification rigoureuse. Son statut RQTH lui permet d'adapter son environnement de travail pour mieux gérer la pression et la fatigabilité.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 12,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_13",
      "text": "## RH - Collaboration internationale\n\nJulien n'a jamais collaboré à l'international.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 13,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_14",
      "text": "## Technique - Données sensibles\n\nJulien a travaillé sur des jeux de données publics (synthétiques, MNIST, CIFAR-10) pour valider ses méthodes, mais pas sur des données sensibles ou confidentielles.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 14,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_15",
      "text": "## Distinctions et prix\n\nJulien a publié à l'international (ICMLA 2024) et a contribué à un chapitre de livre (2025). Il n'a pas reçu de prix prestigieux, mais a été reconnu pour la qualité de ses travaux.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 15,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_16",
      "text": "## Hackathon\n\nJulien a participé et remporté un hackathon en deuxième année de thèse.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 16,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_17",
      "text": "## Ateliers/conférences\n\nJulien n'a jamais animé d'atelier ou de conférence scientifique.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 17,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_18",
      "text": "## Industrie\n\nJulien n'a pas d'expérience en industrie.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 18,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_19",
      "text": "## Anti-hallucination - Informations privées\n\nCette information n'est pas disponible dans les sources. Les réponses sont limitées aux informations présentes dans le corpus ou le CV.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 19,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_20",
      "text": "## Parcours - Parcours académique\n\nJulien a un doctorat, un master, une licence en mathématiques à La Rochelle, et a enseigné les mathématiques.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 20,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_21",
      "text": "## Parcours - Expérience professionnelle\n\nJulien a effectué des stages en laboratoire, a été enseignant et professeur, et a travaillé sur des projets de recherche appliquée.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 21,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_22",
      "text": "## Parcours - Centres d'intérêt\n\nJulien s'intéresse à la vulgarisation scientifique, l'astrophotographie et la modélisation.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 22,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_23",
      "text": "## Parcours - Langues\n\nJulien parle anglais (niveau minimum requis pour le travail et les conférences), écrit maîtrisé.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 23,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_24",
      "text": "## Parcours - Contact\n\nPour contacter Julien, utilisez l'email indiqué dans le CV ou le formulaire de contact sur son site web.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 24,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_25",
      "text": "# Corpus fondamental pour Mia — version 1:1 certification",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 25,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_26",
      "text": "## Q4. Aménagements RQTH\n\nJulien est reconnu RQTH. Il a besoin d’un environnement sain, calme, stable, avec prise en compte de la fatigabilité. Pour plus de détails, il est possible de le contacter via le formulaire de contact sur son site web.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 26,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_27",
      "text": "## Q7. Management\n\nJulien ne souhaite pas évoluer vers le management. Il préfère se concentrer sur la mission, la résolution de problèmes techniques complexes et l’expertise technique.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 27,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_28",
      "text": "## Q9. Définition exacte de SVDD, SPH SVDD et Deep SPH SVDD\n\nLa méthode Support Vector Data Description (SVDD) vise à trouver les paramètres de centre et de rayon (c, ρ) de la plus petite hypersphère englobante pour un ensemble de points. Appliquée à la détection d'anomalies, il s'agit de laisser les points normaux à l'intérieur de la sphère et les anomalies à l'extérieur. Il existe deux approches en machine learning : la Deep soft-boundary SVDD et la One Class Deep SVDD, toutes deux basées sur ce principe.\nDans le cadre de la thèse de Julien, deux méthodes sont proposées : SPH SVDD et Deep SPH SVDD. L’idée de la méthode SPH SVDD est de reformuler le problème SVDD en utilisant une hypersphère dans l’espace géométrique conforme, c’est-à-dire d’utiliser une couche hypersphérique pour résoudre le problème d’optimisation avec l’algèbre géométrique conforme. Le réseau contient alors une couche hypersphérique à un seul neurone (une sphère) et les entrées x. La méthode Deep SPH SVDD est une extension au cas de plusieurs sphères. L’idée première est d’envelopper efficacement des groupes spécifiques de points normaux dans l’espace des caractéristiques par un groupe de sphères. Par ce choix,",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 28,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_29",
      "text": "nous désirons améliorer la finesse de détection et l’interprétabilité du modèle notamment dans le cas de données multimodales.\nMots-clés : SVDD, hypersphère, centre, rayon, anomalie, intérieur, extérieur, soft-boundary, one class, SPH SVDD, Deep SPH SVDD, algèbre géométrique conforme, multi-sphères, groupes de points, caractéristiques, interprétabilité, données multimodales.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 29,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_30",
      "text": "## Q16. Parcours académique\n\nJulien a étudié les mathématiques depuis le lycée, puis à l’université de La Rochelle (Licence, Master, CAPES). Il a enseigné les mathématiques au lycée Saint-Exupéry à La Rochelle. Il a effectué des stages en laboratoire (marégraphe automatique au LIENSs, analyse trajectographique à XLIM/MIA, interpolation d’orbites au SYRTE). Il a soutenu une thèse de doctorat en informatique appliquée à l’intelligence artificielle, où il a publié et exposé ses travaux dans différentes conférences.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 30,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_31",
      "text": "## Q19. Anglais\n\nJulien parle anglais au niveau minimum requis pour travailler et échanger lors de conférences internationales. L’écrit est maîtrisé.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 31,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_32",
      "text": "## Méthode d'initialisation\n\nUne bonne méthode d'initialisation des couches hypersphériques est essentielle pour garantir la convergence et la stabilité de l'apprentissage. La méthode proposée dans la thèse permet d'enchaîner plusieurs couches hypersphériques, ce qui n'est pas possible avec une initialisation standard (type Glorot). Elle repose sur des propriétés asymptotiques des lois de type Gamma généralisées et des calculs de covariance adaptés à la structure géométrique des sphères.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 32,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_33",
      "text": "## Jeux de données utilisés\n\nLes jeux de données principaux utilisés pour valider la méthode Deep M-SPH SVDD sont :\n- Jeux de points synthétiques (pour illustrer le comportement des couches hypersphériques)\n- MNIST\n- CIFAR-10",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 33,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_34",
      "text": "## Composante exploratoire/théorique de la thèse\n\nLa thèse comporte une part importante d'étude théorique, notamment :\n- Un théorème d'approximation pour les réseaux à couches hypersphériques\n- L'étude mathématique des propriétés de convergence et de stabilité\n- L'analyse du phénomène de collapse dans les modèles Deep SVDD",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 34,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_35",
      "text": "## Personnes de référence\n\n- Pr. Catherine Choquet (directrice du laboratoire MIA)\n- Christophe Saint-Jean (directeur de thèse)",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 35,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_36",
      "text": "## Besoins RQTH\n\nJulien est reconnu RQTH. Il a besoin d’un environnement sain, calme, stable, avec prise en compte de la fatigabilité. Pour plus de détails, il est possible de le contacter via le formulaire de contact sur son site web.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 36,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_37",
      "text": "## Préférence technique / Management\n\nJulien ne souhaite pas évoluer vers le management. Il préfère se concentrer sur la mission, la résolution de problèmes techniques complexes et l’expertise technique.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 37,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_38",
      "text": "## Parcours académique (structuré)\n\nJulien a étudié les mathématiques depuis le lycée, puis à l’université de La Rochelle (Licence, Master, CAPES). Il a enseigné les mathématiques au lycée Saint-Exupéry à La Rochelle. Il a effectué des stages en laboratoire (marégraphe automatique au LIENSs, analyse trajectographique à XLIM/MIA, interpolation d’orbites au SYRTE). Il a soutenu une thèse de doctorat en informatique appliquée à l’intelligence artificielle, où il a publié et exposé ses travaux dans différentes conférences.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 38,
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_39",
      "text": "## Anglais\n\nJulien parle anglais au niveau minimum requis pour travailler et échanger lors de conférences internationales. L’écrit est maîtrisé.",
      "source_file": "corpus_fondamental.txt",
      "chunk_index": 39,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_0",
      "text": "# Fundamental Corpus for Mia — Robust Thematic Version",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_1",
      "text": "## HR - Contribution to the Company\n\nJulien can deliver a functional prototype, anomaly detection solutions, validation of innovative methods, and contribute to the production deployment of AI projects.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_2",
      "text": "## HR - Product/Service Benefit\n\nJulien brings anomaly detection, intelligent monitoring, and the industrialization of AI solutions to improve product and service quality.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_3",
      "text": "## HR - Availability\n\nJulien's availability is specified in his CV. For any contact, please use the email or contact form.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_4",
      "text": "## HR - References\n\nJulien has worked in laboratories such as MIA and XLIM, and has been supervised by recognized thesis directors. His supervisors and labs are listed in his CV.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_5",
      "text": "## HR - Teamwork with Non-Technical Profiles\n\nJulien practices science outreach, knows how to work in a team, and can communicate with non-technical profiles.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_6",
      "text": "## HR - Management\n\nJulien prefers to focus on technical work, the mission, and solving complex problems rather than moving into management.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_7",
      "text": "## Technical - Who is Julien?\n\nJulien holds a PhD in applied computer science, specialized in hyperspherical neural networks and anomaly detection.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_8",
      "text": "## Technical - AI Skills\n\nJulien masters theory, CNNs, RNNs, GANs, Transformers, autoencoders, and Bayesian optimization.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 8,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_9",
      "text": "## Technical - Publications\n\nHere are Julien's main publications:\n- J. de Saint Angel, C. Saint-Jean, C. Choquet – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, book chapter, Recent Applications in Deep Learning, 2025.\n- J. de Saint Angel, C. Saint-Jean – Spherical Dense and Conv2d Layers via Conformal Geometric Algebra, ORASIS, 2021.\n- J. de Saint Angel, C. Saint-Jean – Approximation Theorem for Hyperspherical Neurons, GRETSI, 2023.\n- J. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 9,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_10",
      "text": "## Quality - Reliability\n\nYes, Julien is reliable and has strong scientific and technical skills.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 10,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_11",
      "text": "## HR - Supervision\n\nJulien was a secondary school teacher. He has never supervised university-level students.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 11,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_12",
      "text": "## HR - Stress and Deadlines Management\n\nJulien is very organized and anticipates obstacles through rigorous planning. His RQTH status allows him to adapt his work environment to better manage stress and fatigue.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 12,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_13",
      "text": "## HR - International Collaboration\n\nJulien has never collaborated internationally.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 13,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_14",
      "text": "## Technical - Sensitive Data\n\nJulien has worked on public datasets (synthetic, MNIST, CIFAR-10) to validate his methods, but not on sensitive or confidential data.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 14,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_15",
      "text": "## Distinctions and Awards\n\nJulien has published internationally (ICMLA 2024) and contributed to a book chapter (2025). He has not received prestigious awards but has been recognized for the quality of his work.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 15,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_16",
      "text": "## Hackathon\n\nJulien participated in and won a hackathon in the second year of his PhD.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 16,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_17",
      "text": "## Workshops/Conferences\n\nJulien has never led a scientific workshop or conference.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 17,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_18",
      "text": "## Industry\n\nJulien has no industrial experience.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 18,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_19",
      "text": "## Anti-hallucination - Private Information\n\nThis information is not available in the sources. Answers are limited to information present in the corpus or CV.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 19,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_20",
      "text": "## Background - Academic Background\n\nJulien holds a PhD, a master's, and a bachelor's degree in mathematics from La Rochelle, and has taught mathematics.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 20,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_21",
      "text": "## Background - Professional Experience\n\nJulien has completed laboratory internships, taught as a teacher and professor, and worked on applied research projects.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 21,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_22",
      "text": "## Background - Interests\n\nJulien is interested in science outreach, astrophotography, and modeling.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 22,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_23",
      "text": "## Background - Languages\n\nJulien speaks English (minimum required for work and conferences), with proficient writing skills.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 23,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_24",
      "text": "## Background - Contact\n\nTo contact Julien, use the email provided in the CV or the contact form on his website.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 24,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_25",
      "text": "# Fundamental Corpus for Mia — 1:1 Certification Version",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 25,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_26",
      "text": "## Q4. RQTH Accommodations\n\nJulien is recognized as RQTH. He needs a healthy, calm, stable environment, with consideration for fatigue. For more details, he can be contacted via the contact form on his website.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 26,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_27",
      "text": "## Q7. Management\n\nJulien does not wish to move into management. He prefers to focus on the mission, solving complex technical problems, and technical expertise.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 27,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_28",
      "text": "## Q9. Exact Definition of SVDD, SPH SVDD, and Deep SPH SVDD\n\nThe Support Vector Data Description (SVDD) method aims to find the center and radius (c, ρ) parameters of the smallest enclosing hypersphere for a set of points. Applied to anomaly detection, the goal is to keep normal points inside the sphere and anomalies outside. There are two machine learning approaches: Deep soft-boundary SVDD and One Class Deep SVDD, both based on this principle.\nIn Julien's thesis, two methods are proposed: SPH SVDD and Deep SPH SVDD. The idea of SPH SVDD is to reformulate the SVDD problem using a hypersphere in conformal geometric space, i.e., to use a hyperspherical layer to solve the optimization problem with conformal geometric algebra. The network then contains a hyperspherical layer with a single neuron (one sphere) and the inputs x. The Deep SPH SVDD method is an extension to the case of multiple spheres. The main idea is to efficiently enclose specific groups of normal points in feature space with a group of spheres. This aims to improve detection accuracy and model interpretability, especially for multimodal data.\nKeywords: SVDD, hypersphere, center, radius, anomaly, inside, outside, soft",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 28,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_29",
      "text": "-boundary, one class, SPH SVDD, Deep SPH SVDD, conformal geometric algebra, multi-spheres, groups of points, features, interpretability, multimodal data.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 29,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_30",
      "text": "## Q16. Academic Background\n\nJulien studied mathematics from high school, then at the University of La Rochelle (Bachelor's, Master's, CAPES). He taught mathematics at Saint-Exupéry High School in La Rochelle. He completed laboratory internships (automatic tide gauge at LIENSs, trajectory analysis at XLIM/MIA, orbit interpolation at SYRTE). He defended a PhD in applied computer science for artificial intelligence, where he published and presented his work at various conferences.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 30,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_31",
      "text": "## Q19. English\n\nJulien speaks English at the minimum level required to work and communicate at international conferences. His writing is proficient.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 31,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_32",
      "text": "## Initialization Method\n\nA good initialization method for hyperspherical layers is essential to ensure convergence and learning stability. The method proposed in the thesis allows chaining several hyperspherical layers, which is not possible with standard initialization (like Glorot). It relies on asymptotic properties of generalized Gamma distributions and covariance calculations adapted to the geometric structure of spheres.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 32,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_33",
      "text": "## Datasets Used\n\nThe main datasets used to validate the Deep M-SPH SVDD method are:\n- Synthetic point sets (to illustrate the behavior of hyperspherical layers)\n- MNIST\n- CIFAR-10",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 33,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_34",
      "text": "## Exploratory/Theoretical Component of the Thesis\n\nThe thesis includes a significant theoretical component, notably:\n- An approximation theorem for hyperspherical layer networks\n- Mathematical study of convergence and stability properties\n- Analysis of the collapse phenomenon in Deep SVDD models",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 34,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_35",
      "text": "## Reference Persons\n\n- Prof. Catherine Choquet (MIA lab director)\n- Christophe Saint-Jean (thesis advisor)",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 35,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_36",
      "text": "## RQTH Needs\n\nJulien is recognized as RQTH. He needs a healthy, calm, stable environment, with consideration for fatigue. For more details, he can be contacted via the contact form on his website.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 36,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_37",
      "text": "## Technical Preference / Management\n\nJulien does not wish to move into management. He prefers to focus on the mission, solving complex technical problems, and technical expertise.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 37,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_38",
      "text": "## Academic Background (Structured)\n\nJulien studied mathematics from high school, then at the University of La Rochelle (Bachelor's, Master's, CAPES). He taught mathematics at Saint-Exupéry High School in La Rochelle. He completed laboratory internships (automatic tide gauge at LIENSs, trajectory analysis at XLIM/MIA, orbit interpolation at SYRTE). He defended a PhD in applied computer science for artificial intelligence, where he published and presented his work at various conferences.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 38,
      "embedding": []
    },
    {
      "id": "corpus_fondamental_en.txt_chunk_39",
      "text": "## English\n\nJulien speaks English at the minimum level required to work and communicate at international conferences. His writing is proficient.",
      "source_file": "corpus_fondamental_en.txt",
      "chunk_index": 39,
      "embedding": []
    },
    {
      "id": "couverture.tex.txt_chunk_0",
      "text": "{0mm}{15mm}{15mm}{0mm}{0mm}{0mm}{0mm} { p{3cm} p{12cm}} {3cm} & {12cm} DE LA ROCHELLE}} {16cm} }} $\\ $ [LABORATOIRE : XXXXXX] {16cm} pr\\'esent\\'ee par : \\\\ \\\\ soutenue le : {16cm} pour obtenir le grade de : } Discipline : {16cm} \\\\ } { depth 0pt height 0.4pt width 16cm}} } $\\ $ \\\\ {l p{2cm} p{9cm}} } & $\\ $ & Professeur, Université xxxxxx, Président du jury\\\\ } & $\\ $ & Directeur de recherche CNRS, Université xxxx, Rapporteur\\\\ } & $\\ $ & Professeur, Université xxxxxxx, Rapporteur\\\\ } & $\\ $ & Professeur, Université de La Rochelle, Directeur de thèse\\\\ } & $\\ $ & Professeur, Université xxxxxxxxxxx\\\\ } & $\\ $ & Maître de conférences, Université xxxxxxxxxxxx\\\\ } & $\\ $ & Titre, établissement\\\\ {0pt}",
      "source_file": "couverture.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_0",
      "text": "Julien de Saint Angel – Ingénieur IA & Mathématiques appliquées\nDocteur en informatique appliquée – spécialisé en réseaux neuronaux hypersphériques et détection d'anomalies\n\n📍 17000 La Rochelle, France\n✉️ julien.desaintangel@gmail.com\n\n🎓 Formation\n\n2020–2025 – Doctorat en Informatique Appliquée, Université de La Rochelle, Laboratoire MIA\nThèse : \"Réseaux de neurones à couches hypersphériques pour la détection d'anomalies\".\n\n2017–2019 – Master Mathématiques et Applications MIX (Mention AB), Université de La Rochelle\nSpécialité : mathématiques appliquées, optimisation, équations différentielles.\n\n2014–2015 – Maîtrise d'Astronomie et Physique, Observatoire de Paris-Meudon\n\n2012–2014 – Master Mathématiques et Métiers de l'Éducation + CAPES, Université de La Rochelle\n\n2009–2012 – Licence de Mathématiques, Université de La Rochelle\n\n💼 Expérience Professionnelle\nContributions Scientifiques (2021–2024)\n\nDeep M-SPH SVDD : Méthode multi-sphères hypersphériques pour la détection d'anomalies.\n\nInitialisation : Méthode d'initialisation dédiée aux réseaux à couches hypersphériques (optimisation numérique).\n\nApplications : Détection d'anomalies en séries temporelles, marégraphe visuel, analyse sp",
      "source_file": "cv_julien_texte.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_1",
      "text": "ortive à haute fréquence.\n\nFév.–Mai 2019 – Stage aux laboratoires XLIM (UMR 7252) et MIA (EA 3165)\n\nEncadrants : B. Tremblais et R. Pétéri\nCaractérisation du geste sportif par caméras rapides utilisant l'analyse trajectographique de points critiques.\n\nMai 2018 – Stage au laboratoire LIENSs (UMR 7266)\n\nEncadrants : E. Poirier (IGR) et L. Testud\nRéalisation d'un marégraphe visuel : lecture automatisée de l'image d'une échelle de marée.\n\nMars–Juin 2015 – Stage au laboratoire SYRTE (UMR 8630), Observatoire de Paris\n\nEncadrant : J.-Y. Richard\nDéveloppement d'algorithmes d'interpolation pour les orbites de satellites artificiels (équations différentielles, simulation numérique).\n\n2015–2017 – Enseignant fonctionnaire second degré, Lycées Saint-Exupéry, La Rochelle\n\nProfesseur de mathématiques. Développement de compétences en communication, vulgarisation scientifique et travail en équipe.\n\n🧠 Compétences Techniques\n\nIA : CNN, RNN/LSTM, GAN, Transformers, auto-encodeurs, modèles surrogate, optimisation.\nFrameworks : TensorFlow / PyTorch, Scikit-Learn, OpenCV, PyTorch Lightning, Keras.\nTraitement du signal : Segmentation, filtrage fréquentiel, ondelettes, transformée de Fourier, analyse temps",
      "source_file": "cv_julien_texte.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_2",
      "text": "-fréquence.\nProgrammation : Python (Numpy, Pandas, SciPy, Matplotlib), Java, C, C++, Fortran, MATLAB, Scilab.\nOutils : Git, Docker, Jupyter, Linux, LaTeX, Suite Office (Excel, Word, PowerPoint).\nMaths avancées : Optimisation, EDP, géométrie conforme, approximation universelle.\n\n📚 Publications, articles\n\nJ. de Saint Angel, C. Saint-Jean, C. Choquet – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, chapitre de livre, Recent Applications in Deep Learning, 2025.\n\nJ. de Saint Angel, C. Saint-Jean – Couches Dense et Conv2d sphériques via l'algèbre géométrique conforme, ORASIS, 2021.\n\nJ. de Saint Angel, C. Saint-Jean – Théorème d'approximation pour neurones hypersphériques, GRETSI, 2023.\n\nJ. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.\n\n🌍 Langues\n\nFrançais : natif\nAnglais : B2 – courant\nEspagnol & Roumain : B1\n\n🎨 Centres d'Intérêt\n\nVulgarisation scientifique, astrophotographie, ornithophotographie, modélisation 3D, animation vidéo.",
      "source_file": "cv_julien_texte.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "cv_julien_texte_en.txt_chunk_0",
      "text": "Julien de Saint Angel – AI Engineer & Applied Mathematics\nPhD in Applied Computer Science – specialized in hyperspherical neural networks and anomaly detection\n\n📍 17000 La Rochelle, France\n✉️ julien.desaintangel@gmail.com\n\n🎓 Education\n\n2020–2025 – PhD in Applied Computer Science, University of La Rochelle, MIA Laboratory\nThesis: \"Hyperspherical Neural Networks for Anomaly Detection.\"\n\n2017–2019 – Master’s in Mathematics and Applications MIX (Honors: AB), University of La Rochelle\nSpecialization: applied mathematics, optimization, differential equations.\n\n2014–2015 – Master’s in Astronomy and Physics, Paris-Meudon Observatory\n\n2012–2014 – Master’s in Mathematics and Education + CAPES, University of La Rochelle\n\n2009–2012 – Bachelor’s in Mathematics, University of La Rochelle\n\n💼 Professional Experience\nScientific Contributions (2021–2024)\n\nDeep M-SPH SVDD: Multi-hypersphere method for anomaly detection.\n\nInitialization: Dedicated initialization method for hyperspherical neural networks (numerical optimization).\n\nApplications: Anomaly detection in time series, visual tide gauge, high-frequency sports analysis.\n\nFeb.–May 2019 – Internship at XLIM (UMR 7252) and MIA (EA 3165) laboratori",
      "source_file": "cv_julien_texte_en.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "cv_julien_texte_en.txt_chunk_1",
      "text": "es\n\nSupervisors: B. Tremblais and R. Pétéri\nCharacterization of sports gestures using high-speed cameras and trajectory analysis of critical points.\n\nMay 2018 – Internship at LIENSs laboratory (UMR 7266)\n\nSupervisors: E. Poirier (IGR) and L. Testud\nDevelopment of a visual tide gauge: automated reading of tide staff images.\n\nMar.–Jun. 2015 – Internship at SYRTE laboratory (UMR 8630), Paris Observatory\n\nSupervisor: J.-Y. Richard\nDevelopment of interpolation algorithms for artificial satellite orbits (differential equations, numerical simulation).\n\n2015–2017 – Secondary School Teacher, Lycées Saint-Exupéry, La Rochelle\n\nMathematics teacher. Developed skills in communication, science outreach, and teamwork.\n\n🧠 Technical Skills\n\nAI: CNN, RNN/LSTM, GAN, Transformers, autoencoders, surrogate models, optimization.\nFrameworks: TensorFlow / PyTorch, Scikit-Learn, OpenCV, PyTorch Lightning, Keras.\nSignal Processing: Segmentation, frequency filtering, wavelets, Fourier transform, time-frequency analysis.\nProgramming: Python (Numpy, Pandas, SciPy, Matplotlib), Java, C, C++, Fortran, MATLAB, Scilab.\nTools: Git, Docker, Jupyter, Linux, LaTeX, Office Suite (Excel, Word, PowerPoint).\nAdvanced Math:",
      "source_file": "cv_julien_texte_en.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "cv_julien_texte_en.txt_chunk_2",
      "text": "Optimization, PDEs, conformal geometry, universal approximation.\n\n📚 Publications, Articles\n\nJ. de Saint Angel, C. Saint-Jean, C. Choquet – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, book chapter, Recent Applications in Deep Learning, 2025.\n\nJ. de Saint Angel, C. Saint-Jean – Spherical Dense and Conv2d Layers via Conformal Geometric Algebra, ORASIS, 2021.\n\nJ. de Saint Angel, C. Saint-Jean – Approximation Theorem for Hyperspherical Neurons, GRETSI, 2023.\n\nJ. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.\n\n🌍 Languages\n\nFrench: native\nEnglish: B2 – fluent\nSpanish & Romanian: B1\n\n🎨 Interests\n\nScience outreach, astrophotography, bird photography, 3D modeling, video animation.",
      "source_file": "cv_julien_texte_en.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_0",
      "text": "{chapter}{Introduction} Cette thèse explore une variante des couches classiques de réseaux de neurones, remplaçant les hyperplans par des hypersphères. Contrairement aux couches denses classiques, qui sont généralement définies par des équations linéaires des formes $W + = 0$, les couches hypersphériques reposent sur des frontières définies par des hypersphères, permettant une partition non linéaire de l'espace $^n$. \\ [H] {valign=t,minipage=} Ce changement de représentation ouvre de nouvelles perspectives pour la modélisation de structures complexes dans des espaces de données en utilisant l'algèbre géométrique conforme. Cette approche permet de représenter de manière unifiée hyperplans et hypersphères sous forme de vecteurs, facilitant ainsi l'intégration de cette approche dans des architectures de réseaux neuronaux traditionnels.\\\\ Les couches hypersphériques s'appliquent aussi bien aux couches denses qu'aux couches convolutives. En remplaçant les hyperplans par des hypersphères, ces couches permettent de définir des frontières de décision non linéaires, adaptées aux tâches de partitionnement et de classification complexes. Les hypersphères peuvent ainsi offrir une nouvelle faço",
      "source_file": "introduction.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_1",
      "text": "n de capturer la complexité des relations entre les données et permettent de mieux modéliser les structures sous-jacentes.\\\\ Cependant, l'intégration de ces couches dans des architectures de réseaux de neurones soulève plusieurs défis pratiques. L'un des plus importants concerne l'initialisation des paramètres des couches hypersphériques. En effet, cette question est un défi particulièrement complexe en raison de la non-linéarité de la transformation et de la difficulté d'assurer une convergence stable lors de l'apprentissage. La thèse explore différentes stratégies d'initialisation des poids, notamment l'adaptation des méthodes classiques comme celle de Glorot et Bengio. Il est essentiel de bien paramétrer l'initialisation des neurones hypersphériques pour éviter des divergences numériques, garantir une convergence efficace des modèles et offrir la possibilité d'enchaîner et d'intégrer ces couches dans des architectures plus complexes.\\\\ La détection d'anomalies est une tâche cruciale dans de nombreux domaines, tels que la sécurité informatique, la surveillance industrielle, la finance, et le diagnostic médical. Elle vise à identifier les points de données qui s'écartent significa",
      "source_file": "introduction.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_2",
      "text": "tivement des modèles de distribution attendus, souvent appelés valeurs aberrantes ou anomalies. Cette problématique devient particulièrement complexe dans des contextes où les données sont de grande dimension, non linéaires, ou lorsque les distributions des données ne suivent pas des lois statistiques simples comme la loi normale. Ainsi, l'amélioration des méthodes de détection d'anomalies reste un défi de taille, surtout dans les situations complexes où les modèles classiques échouent à capturer des structures sous-jacentes complexes.\\\\ Les approches classiques de détection d'anomalies, telles que les méthodes basées sur la distance (k-plus proches voisins), la densité (Local Outlier Factor) ou le clustering (k-means), se confrontent à des limitations face à des données complexes, non linéaires, ou avec des distributions non gaussiennes. Ces méthodes se heurtent souvent aux problèmes liés aux données qui sont souvent hétérogènes, déséquilibrées et de grande dimension. Il devient ainsi nécessaire de développer des approches plus robustes et interprétables, capables de mieux gérer ces complexités. Ce sont les problématiques auxquelles les approches par apprentissage profond vont pou",
      "source_file": "introduction.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_3",
      "text": "voir répondre.\\\\ L'une des applications principales de cette thèse concerne la détection d'anomalies. En exploitant les couches hypersphériques, cette thèse propose une nouvelle méthode, le Deep sph-SVDD, qui revisite l'approche du Support Vector Data Description (SVDD) en l'intégrant dans le cadre de l'algèbre géométrique conforme. Ce modèle apprend directement les paramètres du centre et du rayon de l'hypersphère dans un espace transformé, permettant ainsi une meilleure séparation des données normales et des anomalies. De plus, une extension de cette méthode, nommée Deep M sph-SVDD, introduit plusieurs hypersphères pour mieux capturer des groupes distincts de données normales, renforçant ainsi la capacité du modèle à détecter des anomalies dans des ensembles de données complexes et variés.\\\\ Afin d'explorer le potentiel de ces couches hypersphériques en détection d'anomalies, cette thèse adopte une méthodologie combinant des approches théoriques, algorithmiques et expérimentales~:\\\\ Étude théorique : Un théorème d'approximation est établi, démontrant que les réseaux de neurones à couches hypersphériques peuvent approximer des fonctions continues définies sur un compact. Cette app",
      "source_file": "introduction.tex.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_4",
      "text": "roche repose sur des résultats issus du théorème de Schwartz pour les fonctions d'une seule variable. Développement algorithmique : La thèse propose de nouveaux algorithmes pour la détection d'anomalies, incluant le Deep sph-SVDD et le Deep M sph-SVDD, qui apprennent directement les paramètres associés aux hypersphères englobantes (centre, rayon, etc.) à partir des données. Implémentation et expérimentation : Les algorithmes sont implémentés à l'aide de bibliothèques spécialisées pour l'apprentissage profond, telles que TensorFlow et PyTorch, puis évalués sur des ensembles de données comme MNIST et CIFAR-10. Les résultats expérimentaux montrent que les méthodes proposées sont compétitives par rapport aux méthodes classiques en termes de performance (AUC-ROC, AUC-PR, score F1) et d'interprétabilité. En résumé, cette thèse propose une exploration approfondie des réseaux de neurones à couches hypersphériques, alliant avancées théoriques et applications pratiques dans le domaine de la détection d'anomalies. Les résultats démontrent non seulement que les modèles développés restent compétitifs et complémentaires par rapport aux modèles classiques, mais aussi leur potentiel à améliorer l'",
      "source_file": "introduction.tex.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_5",
      "text": "interprétabilité des modèles. Néanmoins, des défis demeurent, notamment en ce qui concerne la stabilisation de l'apprentissage, ouvrant ainsi des pistes pour des recherches futures visant à rendre ces modèles plus robustes et applicables à une plus large gamme de tâches.",
      "source_file": "introduction.tex.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_0",
      "text": "0 {Lemme} {Th\\'eor\\`eme} {Remarque}[section] {arg\\,max} {arg\\,min} {}} {Tableau}} [t]{0.49} [t]{0.49} {0.9} \\\\ {0.9} {0.9} { r @{} l @{} l @{} l } {c}{} \\\\[0.8cm] Présentée par & : & & \\\\[0.2cm] Pour obtenir le grade de & : & & \\\\[0.2cm] Discipline & : & & \\\\[0.5cm] } {2pt} {15pt} {0.9} } {2pt} Soutenue publiquement le , à {1.5em} \\= \\= Christophe SAINT-JEAN \\> La Rochelle Université \\> Directeur de thèse \\\\ Yannick BERTHOUMIEU \\> Université de Bordeaux \\> Rapporteur \\\\ Hoel LE CAPITAINE \\> Nantes Université \\> Rapporteur \\\\ Laetitia CHAPEL \\> Laboratoire IRISA \\> Examinatrice \\\\ Catherine CHOQUET \\> La Rochelle Université \\> Examinatrice \\\\ \\\\ La réalisation de cette thèse marque l’aboutissement d’un long chemin, parsemé de défis et de moments d’apprentissage. Je tiens à exprimer toute ma gratitude envers les nombreuses personnes qui ont contribué, de près ou de loin, à cette aventure scientifique et humaine.\\\\ Je tiens tout d'abord à adresser mes remerciements les plus sincères aux membres du jury pour l’attention portée à mon travail, leurs lectures attentives et leurs retours constructifs. Je remercie également Yannick Berthoumieu (université de Bordeaux) et Hoel Le Capitaine (",
      "source_file": "main.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_1",
      "text": "université de Nantes), rapporteurs de cette thèse, pour leur évaluation rigoureuse et leurs commentaires enrichissants. Je remercie également Laetitia Chapel (Laboratoire IRISA) et Catherine Choquet (MIA), examinatrices, pour leur intérêt pour ce travail.\\\\ Je souhaite également remercier chaleureusement mon directeur de thèse, Christophe Saint-Jean, pour m’avoir offert l’opportunité de mener à bien ce projet de recherche. Son accompagnement et son investissement ont été essentiels tout au long de ce parcours. Réaliser une thèse est un parcours complexe, souvent ponctué de défis. Je suis profondément reconnaissant de sa patience, de sa rigueur et de son aide précieuse qui m’ont permis d’aller jusqu’au bout de ce projet ambitieux.\\\\ Je tiens également à remercier Mme Catherine Choquet pour son soutien inestimable et son humanité. En sa qualité de directrice du laboratoire, elle a veillé, avec Christophe, à ce que je puisse bénéficier de conditions optimales pour travailler, malgré les difficultés liées à mes handicaps. Son engagement et son attention ont joué un rôle déterminant dans ma réussite.\\\\ Je tiens également à remercier l'ensemble des enseignants-chercheurs du laboratoire M",
      "source_file": "main.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_2",
      "text": "IA, et en particulier Michel Berthier, Laurent Mascarilla et Renaud Péteri, pour leur enseignement, leur soutien et leur confiance en mes capacités depuis le début de mes études. Leur foi en mon potentiel m'a été constante source d'inspiration tout au long de ce projet.\\\\ Je tiens également à remercier chaleureusement Patrick Motillon et François Geoffriau qui m’ont accompagné durant mes études. J’aimerais également dédier une pensée particulière à Frédéric Testard, enseignant pédagogue et passionné.\\\\ Un immense merci à ma famille pour son soutien indéfectible. Je tiens à remercier particulièrement mes parents, Olivier et Corinne de Saint Angel, ma sœur Marylou, ma grand-mère Nicolle Métayer, Émeric de Monteynard et tous les autres membres de ma famille : merci de m’avoir accompagné tout au long de ma vie, de croire en moi et de m’avoir donné la force et la motivation nécessaires pour réaliser ce rêve. Votre amour et votre confiance m’ont permis de devenir la personne que je suis aujourd’hui.\\\\ Je tiens également à exprimer ma reconnaissance envers mes amis et camarades de recherche, dont Chloé Marchand, Abdoulrazack Mohamed Abdi, Valérie Garcin, ainsi que les doctorants du labora",
      "source_file": "main.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_3",
      "text": "toire. Ensemble, nous avons partagé des moments de doute, de réussite, de joie, mais aussi une entraide précieuse dans cette voie exigeante qu’est la recherche.\\\\ J’ai également une pensée pour mes amis proches, dont beaucoup se trouvent aux quatre coins du monde, avec qui j’ai partagé des moments de ma vie professionnelle et personnelle : Michael Nanclares, Andreï Teodor, Hamza Fariz, Alexandre Achain, Jefferson Marchal et Aurélian-Laurent Bellou-Bousselaire. Leur soutien tout au long de mon parcours a été un véritable moteur pour mener à bien cette thèse.\\\\ Je souhaite également remercier Azeddine El Igassy, Carole Pelgier ainsi que toute l'équipe de la cellule handicap de l’université, et particulièrement Amandine Mouilleron. Je tiens à saluer le travail remarquable d’Amandine et son dévouement pour l’intégration des étudiants en situation de handicap dans leur parcours universitaire.\\\\ Enfin, je tiens à exprimer ma profonde reconnaissance au professeur Christine Silvain, au professeur Éphrem Salamé et à leurs équipes médicales des CHU de Poitiers et de Tours, ainsi qu’au docteur Marc Commeignes. Sans leur dévouement et leur expertise, je ne serais pas en vie aujourd’hui. Leur e",
      "source_file": "main.tex.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_4",
      "text": "ngagement m’a permis d'obtenir une nouvelle vie et de mener à terme ce parcours.\\\\ Je remercie toutes et tous pour votre contribution précieuse à cette aventure. Cette thèse est le fruit de votre soutien autant que de mes efforts. {chapter}{Notation} {>{$}l<{$} @{${:}$} l} x & \\\\ & \\\\ X & \\\\ i & \\\\ x_i & $$\\\\ W & \\\\ & \\\\ & \\\\ & $$ \\\\ & $$ \\\\ K & \\\\ k & \\\\ J & \\\\ j & $j \\{1,,J\\}$\\\\ L & \\\\ l & \\\\ _i & \\\\ & \\\\ & \\\\ & \\\\ & \\\\",
      "source_file": "main.tex.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_0",
      "text": "---\nAspects humains et personnels :\n\nValeurs et principes :\n- L’éthique dans la recherche, la rigueur et l’honnêteté scientifiques sont au cœur des engagements de Julien.\n\nComment il est perçu :\n- Julien est passionné par la technologie et l’intelligence artificielle.\n\nExpériences marquantes :\n- Julien apprécie particulièrement chaque colloque scientifique, car c’est l’occasion de faire le point sur les grandes avancées de son domaine, de découvrir des projets innovants et d’échanger sur les idées de demain. Pour lui, c’est un véritable voyage dans le futur.\n\nMotivations :\n- Sa motivation profonde est de contribuer à enrichir la connaissance de l’humanité.\n\nVision et ambitions :\n- Julien souhaite approfondir ses capacités en intelligence artificielle pour les appliquer à des questions importantes dans le domaine médical (amélioration du traitement d’images médicales) et dans l’environnement (étude du littoral, vision positive pour l’avenir du monde).\n\nVulgarisation scientifique :\n- Il considère la vulgarisation comme essentielle pour faire comprendre l’intérêt des recherches au grand public.\n\nGestion des difficultés :\n- Les difficultés font partie de son expérience de vie. Julien n",
      "source_file": "profil_julien.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_1",
      "text": "’abandonne jamais : son opiniâtreté le guide jusqu’au bout.\nNom : Julien de Saint Angel\nTitre : Chercheur en intelligence artificielle\nDocteur en informatique appliquée – spécialisé en réseaux neuronaux hypersphériques et détection d'anomalies\n\nFormation :\n2020–2025 – Doctorat en Informatique Appliquée, Université de La Rochelle, Laboratoire MIA\nThèse : \"Réseaux de neurones à couches hypersphériques pour la détection d'anomalies\".\n2017–2019 – Master Mathématiques et Applications MIX (Mention AB), Université de La Rochelle\nSpécialité : mathématiques appliquées, optimisation, équations différentielles.\n2014–2015 – Maîtrise d'Astronomie et Physique, Observatoire de Paris-Meudon\n2012–2014 – Master Mathématiques et Métiers de l'Éducation + CAPES, Université de La Rochelle\n2009–2012 – Licence de Mathématiques, Université de La Rochelle\n\n\nContributions Scientifiques (2021–2024) :\nDeep M-SPH SVDD : Méthode multi-sphères hypersphériques pour la détection d'anomalies.\nInitialisation : Méthode d'initialisation dédiée aux réseaux à couches hypersphériques (optimisation numérique).\nApplications : Détection d'anomalies en séries temporelles, marégraphe visuel, analyse sportive à haute fréquence.",
      "source_file": "profil_julien.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_2",
      "text": "Expérience Professionnelle :\nFév.–Mai 2019 – Stage aux laboratoires XLIM (UMR 7252) et MIA (EA 3165)\nEncadrants : B. Tremblais et R. Pétéri\nCaractérisation du geste sportif par caméras rapides utilisant l'analyse trajectographique de points critiques.\nMai 2018 – Stage au laboratoire LIENSs (UMR 7266)\nEncadrants : E. Poirier (IGR) et L. Testud\nRéalisation d'un marégraphe visuel : lecture automatisée de l'image d'une échelle de marée.\nMars–Juin 2015 – Stage au laboratoire SYRTE (UMR 8630), Observatoire de Paris\nEncadrant : J.-Y. Richard\nDéveloppement d'algorithmes d'interpolation pour les orbites de satellites artificiels (équations différentielles, simulation numérique).\n2015–2017 – Enseignant fonctionnaire second degré, Lycées Saint-Exupéry, La Rochelle\nProfesseur de mathématiques. Développement de compétences en communication, vulgarisation scientifique et travail en équipe.\n\nCompétences :\n- Détection d’anomalies\n- Modélisation mathématiques\n- Interprétabilité des modèles\n- IA ,Apprentissage profond: CNN, RNN/LSTM, GAN, Transformers, auto-encodeurs, modèles surrogate, optimisation.\nFrameworks : TensorFlow / PyTorch, Scikit-Learn, OpenCV, PyTorch Lightning, Keras.\nTraitement du s",
      "source_file": "profil_julien.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_3",
      "text": "ignal : Segmentation, filtrage fréquentiel, ondelettes, transformée de Fourier, analyse temps-fréquence.\nProgrammation : Python (Numpy, Pandas, SciPy, Matplotlib), Java, C, C++, Fortran, MATLAB, Scilab.\nOutils : Git, Docker, Jupyter, Linux, LaTeX, Suite Office (Excel, Word, PowerPoint).\nMaths avancées : Optimisation, EDP, géométrie conforme, approximation universelle.\n\n\nJulien De Saint Angel est Docteur en informatique appliquée, spécialisé en réseaux neuronaux hypersphériques et détection d'anomalies.\n\nParcours académique et formation de Julien :\n- 2020-2025 : Doctorat en Informatique Appliquée, Université de La Rochelle, Laboratoire MIA. Thèse sur les réseaux de neurones à couches hypersphériques pour la détection d'anomalies.\n- 2017-2019 : Master Mathématiques et Applications MIX (Mention AB), Université de La Rochelle. Spécialité mathématiques appliquées, optimisation, équations différentielles.\n- 2014-2015 : Maîtrise d'Astronomie et Physique, Observatoire de Paris-Meudon\n- 2012-2014 : Master Mathématiques et Métiers de l'Éducation + CAPES, Université de La Rochelle\n- 2009-2012 : Licence de Mathématiques, Université de La Rochelle\n\nExpérience professionnelle de Julien :\n- 2021-",
      "source_file": "profil_julien.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_4",
      "text": "2024 : Contributions scientifiques - Deep M-SPH SVDD (méthode multi-sphères hypersphériques pour la détection d'anomalies), méthode d'initialisation pour réseaux à couches hypersphériques, applications en séries temporelles, marégraphe visuel, analyse sportive haute fréquence.\n- Février-Mai 2019 : Stage aux laboratoires XLIM (UMR 7252) et MIA (EA 3165). Encadrants B. Tremblais et R. Pétéri. Caractérisation du geste sportif par caméras rapides.\n- Mai 2018 : Stage au laboratoire LIENSs (UMR 7266). Encadrants E. Poirier et L. Testud. Réalisation d'un marégraphe visuel.\n- Mars-Juin 2015 : Stage au laboratoire SYRTE (UMR 8630), Observatoire de Paris. Encadrant J.-Y. Richard. Développement d'algorithmes d'interpolation pour orbites de satellites artificiels.\n- 2015-2017 : Enseignant fonctionnaire, Lycées Saint-Exupéry La Rochelle. Professeur de mathématiques.\n\nCompétences techniques de Julien :\nIA : CNN, RNN/LSTM, GAN, Transformers, auto-encodeurs, modèles surrogate, optimisation.\nFrameworks : TensorFlow, PyTorch, Scikit-Learn, OpenCV, PyTorch Lightning, Keras.\nProgrammation : Python (Numpy, Pandas, SciPy, Matplotlib), Java, C, C++, Fortran, MATLAB, Scilab.\nOutils : Git, Docker, Jupyter,",
      "source_file": "profil_julien.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_5",
      "text": "Linux, LaTeX.\n\nQualités les plus importantes :\n- Rigueur scientifique, pédagogie, fiabilité, innovation.\n\nqualités de Julien :\nConception et entraînement de réseaux de neurones.\nMaîtrise des mathématiques appliquées et des concepts avancés.\nil peut définir de nouvelles architectures de réseaux de neurones, il a proposé une extension des couches hypersphériques au cas convolutif.\nIl à également développé une méthodes spécifiques d’initialisation des poids adaptées aux modèles proposés\nValidation expérimentale rigoureuse des modèles et analyse critique de leurs performances\nCapacité à travailler à l’interface de l’informatique, de l’optimisation et des mathématiques appliquées\nFormulation de problématiques de recherche pertinentes et construction méthodique de modèles\nPrésenter de façon claire et structurée les travaux scientifiques.\nCapacité à mener des travaux complexes et novateurs de manière autonome\nRigueur scientifique et méthodologique dans la conception, l’expérimentation et l’analyse\nMobilisation d’un large spectre de compétences pour résoudre des problèmes interdisciplinaires\n\n\nPublications, articles de Julien \nJ. de Saint Angel, C. Saint-Jean, C. Choquet – Improving Learn",
      "source_file": "profil_julien.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_6",
      "text": "ing for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, chapitre de livre, Recent Applications in Deep Learning, 2025.\nJ. de Saint Angel, C. Saint-Jean – Couches Dense et Conv2d sphériques via l'algèbre géométrique conforme, ORASIS, 2021.\nJ. de Saint Angel, C. Saint-Jean – Théorème d'approximation pour neurones hypersphériques, GRETSI, 2023.\nJ. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.\n\nContact de Julien :\nAdresse : 17000 La Rochelle, France\nEmail : julien.desaintangel@gmail.com\n\nLangues de Julien :\nFrançais (natif), Anglais (B2 courant)  et Roumain (B1)\n\nCentres d'intérêt de Julien, loisirs :\nVulgarisation scientifique, astrophotographie, ornithophotographie, modélisation 3D, animation vidéo.\n\n\n\n---\nDéfinitions et précisions scientifiques :\n\nRéseaux de neurones à couches hypersphériques (RCH) :\nUn réseau à couches hypersphériques est un réseau de neurones où chaque neurone définit une hypersphère dans l’espace des données, permettant une séparation non linéaire et compacte. Contrairement aux couches denses classiques, qui séparent l’espace par des hyperplans (décision linéaire), les couches hypers",
      "source_file": "profil_julien.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_7",
      "text": "phériques séparent par des hypersphères, offrant une frontière de décision plus flexible et adaptée à des structures complexes.\n\nDistinction avec les couches denses :\nLes couches denses (ou fully connected) réalisent des séparations linéaires (hyperplans) dans l’espace des caractéristiques, alors que les couches hypersphériques réalisent des séparations non linéaires (hypersphères). Cela permet de mieux capturer des structures de données complexes et d’améliorer la détection d’anomalies.\n\nImportance de l’initialisation dans les RCH :\nL’initialisation des paramètres (centre et rayon des hypersphères) est cruciale pour garantir la propagation du signal à travers plusieurs couches cachées et la stabilité de l’apprentissage. Julien a proposé une méthode d’initialisation spécifique adaptée à ces architectures.\n\nAmbitions et perspectives :\nJulien aspire à poursuivre ses recherches en intelligence artificielle, notamment dans l’application de l’IA à l’analyse d’images médicales et aux problématiques environnementales.\n\nRappel des publications principales (toutes citées) :\n- J. de Saint Angel, C. Saint-Jean – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometr",
      "source_file": "profil_julien.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_8",
      "text": "ic Algebra, chapitre de livre, Recent Applications in Deep Learning, 2025.\n- J. de Saint Angel, C. Saint-Jean – Couches Dense et Conv2d sphériques via l'algèbre géométrique conforme, ORASIS, 2021.\n- J. de Saint Angel, C. Saint-Jean – Théorème d'approximation pour neurones hypersphériques, GRETSI, 2023.\n- J. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.\nDirecteur de thèse : Christophe Saint-Jean\nDirectrice de laboratoire : Catherine Choquet",
      "source_file": "profil_julien.txt",
      "chunk_index": 8,
      "embedding": []
    },
    {
      "id": "profil_julien_en.txt_chunk_0",
      "text": "---\nHuman and Personal Aspects:\n\nValues and Principles:\n- Ethics in research, scientific rigor, and honesty are at the heart of Julien's commitments.\n\nHow he is perceived:\n- Julien is passionate about technology and artificial intelligence.\n\nNotable Experiences:\n- Julien particularly enjoys every scientific conference, as it is an opportunity to take stock of major advances in his field, discover innovative projects, and exchange ideas about the future. For him, it is a true journey into the future.\n\nMotivations:\n- His deep motivation is to contribute to the enrichment of human knowledge.\n\nVision and Ambitions:\n- Julien wishes to deepen his skills in artificial intelligence to apply them to important questions in the medical field (improving medical image processing) and the environment (coastal studies, a positive vision for the future of the world).\n\nScience Outreach:\n- He considers outreach essential to help the general public understand the value of research.\n\nHandling Difficulties:\n- Difficulties are part of his life experience. Julien never gives up: his perseverance guides him to the end.\nName: Julien de Saint Angel\nTitle: Researcher in Artificial Intelligence\nPhD in Applied",
      "source_file": "profil_julien_en.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "profil_julien_en.txt_chunk_1",
      "text": "Computer Science – specialized in hyperspherical neural networks and anomaly detection\n\nEducation:\n2020–2025 – PhD in Applied Computer Science, University of La Rochelle, MIA Laboratory\nThesis: \"Hyperspherical Neural Networks for Anomaly Detection.\"\n2017–2019 – Master’s in Mathematics and Applications MIX (Honors: AB), University of La Rochelle\nSpecialization: applied mathematics, optimization, differential equations.\n2014–2015 – Master’s in Astronomy and Physics, Paris-Meudon Observatory\n2012–2014 – Master’s in Mathematics and Education + CAPES, University of La Rochelle\n2009–2012 – Bachelor’s in Mathematics, University of La Rochelle\n\nScientific Contributions (2021–2024):\nDeep M-SPH SVDD: Multi-hypersphere method for anomaly detection.\nInitialization: Dedicated initialization method for hyperspherical neural networks (numerical optimization).\nApplications: Anomaly detection in time series, visual tide gauge, high-frequency sports analysis.\n\nProfessional Experience:\nFeb.–May 2019 – Internship at XLIM (UMR 7252) and MIA (EA 3165) laboratories\nSupervisors: B. Tremblais and R. Pétéri\nCharacterization of sports gestures using high-speed cameras and trajectory analysis of critical poi",
      "source_file": "profil_julien_en.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "profil_julien_en.txt_chunk_2",
      "text": "nts.\nMay 2018 – Internship at LIENSs laboratory (UMR 7266)\nSupervisors: E. Poirier (IGR) and L. Testud\nDevelopment of a visual tide gauge: automated reading of tide staff images.\nMar.–Jun. 2015 – Internship at SYRTE laboratory (UMR 8630), Paris Observatory\nSupervisor: J.-Y. Richard\nDevelopment of interpolation algorithms for artificial satellite orbits (differential equations, numerical simulation).\n2015–2017 – Secondary School Teacher, Lycées Saint-Exupéry, La Rochelle\nMathematics teacher. Developed skills in communication, science outreach, and teamwork.\n\nSkills:\n- Anomaly detection\n- Mathematical modeling\n- Model interpretability\n- AI, Deep Learning: CNN, RNN/LSTM, GAN, Transformers, autoencoders, surrogate models, optimization.\nFrameworks: TensorFlow / PyTorch, Scikit-Learn, OpenCV, PyTorch Lightning, Keras.\nSignal Processing: Segmentation, frequency filtering, wavelets, Fourier transform, time-frequency analysis.\nProgramming: Python (Numpy, Pandas, SciPy, Matplotlib), Java, C, C++, Fortran, MATLAB, Scilab.\nTools: Git, Docker, Jupyter, Linux, LaTeX, Office Suite (Excel, Word, PowerPoint).\nAdvanced Math: Optimization, PDEs, conformal geometry, universal approximation.\n\nJulien De",
      "source_file": "profil_julien_en.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "profil_julien_en.txt_chunk_3",
      "text": "Saint Angel holds a PhD in Applied Computer Science, specialized in hyperspherical neural networks and anomaly detection.\n\nAcademic Background and Training:\n- 2020–2025: PhD in Applied Computer Science, University of La Rochelle, MIA Laboratory. Thesis on hyperspherical neural networks for anomaly detection.\n- 2017–2019: Master’s in Mathematics and Applications MIX (Honors: AB), University of La Rochelle. Specialization in applied mathematics, optimization, differential equations.\n- 2014–2015: Master’s in Astronomy and Physics, Paris-Meudon Observatory\n- 2012–2014: Master’s in Mathematics and Education + CAPES, University of La Rochelle\n- 2009–2012: Bachelor’s in Mathematics, University of La Rochelle\n\nProfessional Experience:\n- 2021–2024: Scientific contributions – Deep M-SPH SVDD (multi-hypersphere method for anomaly detection), initialization method for hyperspherical neural networks, applications in time series, visual tide gauge, high-frequency sports analysis.\n- Feb.–May 2019: Internship at XLIM (UMR 7252) and MIA (EA 3165) laboratories. Supervisors: B. Tremblais and R. Pétéri. Characterization of sports gestures using high-speed cameras.\n- May 2018: Internship at LIENSs lab",
      "source_file": "profil_julien_en.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "profil_julien_en.txt_chunk_4",
      "text": "oratory (UMR 7266). Supervisors: E. Poirier and L. Testud. Development of a visual tide gauge.\n- Mar.–Jun. 2015: Internship at SYRTE laboratory (UMR 8630), Paris Observatory. Supervisor: J.-Y. Richard. Development of interpolation algorithms for artificial satellite orbits.\n- 2015–2017: Secondary school teacher, Lycées Saint-Exupéry, La Rochelle. Mathematics teacher.\n\nJulien's Technical Skills:\nAI: CNN, RNN/LSTM, GAN, Transformers, autoencoders, surrogate models, optimization.\nFrameworks: TensorFlow, PyTorch, Scikit-Learn, OpenCV, PyTorch Lightning, Keras.\nProgramming: Python (Numpy, Pandas, SciPy, Matplotlib), Java, C, C++, Fortran, MATLAB, Scilab.\nTools: Git, Docker, Jupyter, Linux, LaTeX.\n\nMost Important Qualities:\n- Scientific rigor, pedagogy, reliability, innovation.\n\nJulien's Qualities:\nDesign and training of neural networks.\nMastery of applied mathematics and advanced concepts.\nHe can define new neural network architectures and has proposed an extension of hyperspherical layers to the convolutional case.\nHe has also developed specific weight initialization methods adapted to the proposed models.\nRigorous experimental validation of models and critical analysis of their perfor",
      "source_file": "profil_julien_en.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "profil_julien_en.txt_chunk_5",
      "text": "mance.\nAbility to work at the interface of computer science, optimization, and applied mathematics.\nFormulation of relevant research problems and methodical model construction.\nClear and structured presentation of scientific work.\nAbility to conduct complex and innovative work independently.\nScientific and methodological rigor in design, experimentation, and analysis.\nMobilization of a wide range of skills to solve interdisciplinary problems.\n\nJulien's Publications and Articles:\nJ. de Saint Angel, C. Saint-Jean, C. Choquet – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, book chapter, Recent Applications in Deep Learning, 2025.\nJ. de Saint Angel, C. Saint-Jean – Spherical Dense and Conv2d Layers via Conformal Geometric Algebra, ORASIS, 2021.\nJ. de Saint Angel, C. Saint-Jean – Approximation Theorem for Hyperspherical Neurons, GRETSI, 2023.\nJ. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.\n\nContact:\nAddress: 17000 La Rochelle, France\nEmail: julien.desaintangel@gmail.com\n\nLanguages:\nFrench (native), English (B2 fluent), and Romanian (B1)\n\nInterests and Hobbies:\nScience outreach, ast",
      "source_file": "profil_julien_en.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "profil_julien_en.txt_chunk_6",
      "text": "rophotography, bird photography, 3D modeling, video animation.\n\n---\nScientific Definitions and Clarifications:\n\nHyperspherical Neural Networks (HSN):\nA hyperspherical layer network is a neural network where each neuron defines a hypersphere in the data space, enabling nonlinear and compact separation. Unlike classical dense layers, which separate space with hyperplanes (linear decision), hyperspherical layers separate with hyperspheres, offering a more flexible decision boundary suited to complex structures.\n\nDistinction from Dense Layers:\nDense (fully connected) layers perform linear separations (hyperplanes) in feature space, while hyperspherical layers perform nonlinear separations (hyperspheres). This allows better capture of complex data structures and improves anomaly detection.\n\nImportance of Initialization in HSN:\nThe initialization of parameters (center and radius of hyperspheres) is crucial to ensure signal propagation through multiple hidden layers and learning stability. Julien has proposed a specific initialization method adapted to these architectures.\n\nAmbitions and Perspectives:\nJulien aims to continue his research in artificial intelligence, particularly in applyin",
      "source_file": "profil_julien_en.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "profil_julien_en.txt_chunk_7",
      "text": "g AI to medical image analysis and environmental issues.\n\nMain Publications (all cited):\n- J. de Saint Angel, C. Saint-Jean – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, book chapter, Recent Applications in Deep Learning, 2025.\n- J. de Saint Angel, C. Saint-Jean – Spherical Dense and Conv2d Layers via Conformal Geometric Algebra, ORASIS, 2021.\n- J. de Saint Angel, C. Saint-Jean – Approximation Theorem for Hyperspherical Neurons, GRETSI, 2023.\n- J. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.\nPhD Advisor: Christophe Saint-Jean\nLab Director: Catherine Choquet",
      "source_file": "profil_julien_en.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_0",
      "text": "{15cm} Cette thèse explore les réseaux de neurones à couches hypersphériques pour la détection d'anomalies, en remplaçant les hyperplans traditionnels par des hypersphères. Cette approche, basée sur l'algèbre géométrique conforme, permet, en effectuant une partition non linéaire de l'espace des données, d'offrir une plus grande flexibilité dans la modélisation des frontières de décision. Les couches hypersphériques, applicables aux couches denses et convolutives, sont définies par des hypersphères paramétrées par des centres et des rayons, ajustés lors de l'apprentissage. Une méthode pour l'initialisation des paramètres de ces couches permettant de garantir une convergence stable est proposée. La thèse propose une méthode d'initialisation inspirée de Glorot et Bengio, spécifiquement adaptée aux couches hypersphériques. De nouveaux algorithmes de détection d'anomalies, tels que le Deep SPH SVDD et le Deep M-SPH SVDD, sont également développés, exploitant les hypersphères pour mieux capturer les structures complexes des données. Les expérimentations sur des ensembles de données comme MNIST et CIFAR-10 montrent que ces méthodes sont compétitives par rapport aux approches classiques en",
      "source_file": "quatrieme.tex.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_1",
      "text": "termes de performance et d'interprétabilité. Enfin, un théorème d'approximation est établi, démontrant que les réseaux de neurones à couches hypersphériques peuvent approximer des fonctions continues définies sur un compact, ouvrant ainsi des perspectives pour des applications futures dans divers domaines.\\\\ Mots clés : } } {15cm} This thesis explores hyperspherical neural networks for anomaly detection by replacing traditional hyperplanes with hyperspheres. This approach, based on conformal geometric algebra, enables a nonlinear partitioning of the data space, offering greater flexibility in modeling decision boundaries. Hyperspherical layers, applicable to both dense and convolutional layers, are defined by hyperspheres parameterized by centers and radii, adjusted during training. A method for initializing the parameters of these layers to guarantee stable convergence is proposed. The thesis proposes an initialization method inspired by Glorot and Bengio, specifically adapted for hyperspherical layers. New anomaly detection algorithms, such as Deep sph-SVDD and Deep M sph-SVDD, are also developed, leveraging hyperspheres to better capture complex data structures. Experiments on",
      "source_file": "quatrieme.tex.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_2",
      "text": "datasets like MNIST and CIFAR-10 demonstrate that these methods are competitive with classical approaches in terms of performance and interpretability. Finally, an approximation theorem is established, proving that hyperspherical neural networks can approximate continuous functions defined on a compact set, opening perspectives for future applications in various domains.\\\\ Keywords: } } { p{3cm} p{8cm} p{3cm}} {3cm} & {8cm} \\\\ 17000 LA ROCHELLE & {3cm}",
      "source_file": "quatrieme.tex.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_0",
      "text": "Titre : \"Géométrie conforme et réseaux de neurones : nouvelles approches pour l’IA interprétable\"\n\nRésumé :\nCette thèse explore l’utilisation des algèbres géométriques conformes pour améliorer l’interprétabilité et la robustesse des réseaux de neurones. Elle propose de nouvelles méthodes d’initialisation et d’analyse des couches hypersphériques.\n\nChapitres :\n1. Introduction à la géométrie conforme\n2. Modélisation des hypersphères en IA\n3. Méthodes d’initialisation pour réseaux hypersphériques\n4. Applications à la détection d’anomalies\n5. Perspectives et conclusions\n\nRésultats clés :\n- Nouvelle méthode d’initialisation des couches hypersphériques\n- Amélioration de la détection d’anomalies sur plusieurs jeux de données\n- Publication de 3 articles majeurs sur le sujet\nThèse de Julien De Saint Angel\nTitre complet : Réseaux de neurones à couches hypersphériques – Application à la détection d’anomalies\nUniversité : La Rochelle Université\nÉcole doctorale : Euclide\nLaboratoire : Mathématiques, Image et Applications (MIA)\nSoutenance : 4 juillet 2025\n\nEncadrement :\n\nDirecteur de thèse : Christophe Saint-Jean\n\nDirectrice de laboratoire : Catherine Choquet\n\nRapporteurs : Yannick Berthoumieu (U",
      "source_file": "these_julien.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_1",
      "text": "niversité de Bordeaux), Hoel Le Capitaine (Université de Nantes)\n\nExaminateurs : Laetitia Chapel (IRISA), Catherine Choquet (MIA)\n\nRésumé :\nCette thèse introduit une nouvelle architecture de réseaux de neurones à couches hypersphériques fondée sur le formalisme de l’algèbre géométrique conforme (CGA). Elle vise à améliorer la représentation des données dans des espaces de grande dimension et à renforcer la performance des modèles de détection d’anomalies dans des contextes complexes, tels que les séries temporelles, la vision par ordinateur et l’analyse de signaux non linéaires. Les couches hypersphériques remplacent les hyperplans classiques des réseaux de neurones par des hypersphères apprenables, permettant des séparations de classes non linéaires et une meilleure robustesse aux distributions non gaussiennes.\n\nObjectifs scientifiques :\n\nProposer un formalisme géométrique unifié pour représenter hyperplans et hypersphères à l’aide de l’algèbre conforme.\n\nDévelopper des couches neuronales hypersphériques (denses et convolutives) adaptées à l’apprentissage profond.\n\nDéfinir une méthode d’initialisation spécifique assurant stabilité et convergence dans ces couches.\n\nAppliquer ces mo",
      "source_file": "these_julien.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_2",
      "text": "dèles à la détection d’anomalies en s’appuyant sur des variantes du Support Vector Data Description (SVDD) adaptées à la géométrie hypersphérique.\n\nContributions principales :\n\nDéveloppement du modèle Deep M-SPH SVDD, une approche multi-sphères pour la détection d’anomalies basée sur la géométrie conforme.\n\nConception d’une méthode d’initialisation hypersphérique garantissant la stabilité numérique et une convergence efficace.\n\nImplémentation pratique de couches hypersphériques denses et convolutives dans PyTorch et TensorFlow.\n\nÉtablissement d’un théorème d’approximation universelle démontrant la capacité de ces réseaux à approximer toute fonction continue sur un compact.\n\nValidation expérimentale sur des données synthétiques et réelles (MNIST, CIFAR-10) montrant des gains en performance et en stabilité.\n\nApplications :\nDétection d’anomalies dans les séries temporelles, surveillance industrielle, analyse marégraphique visuelle, diagnostic sportif haute fréquence et reconnaissance d’images complexes.\n\nExtraits représentatifs :\n« Les couches hypersphériques permettent de projeter les données sur des surfaces sphériques, facilitant la séparation des classes et la détection des points",
      "source_file": "these_julien.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_3",
      "text": "atypiques. »\n« La méthode Deep M-SPH SVDD améliore la robustesse de la détection d’anomalies en exploitant la géométrie des hypersphères dans l’espace des caractéristiques. »\n« L’initialisation adaptée aux couches hypersphériques optimise la convergence et la stabilité des modèles. »\n\nPublications associées :\n\nJ. de Saint Angel, C. Saint-Jean, C. Choquet (2025) – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, in Recent Applications in Deep Learning.\n\nJ. de Saint Angel, C. Saint-Jean (2024) – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA.\n\nJ. de Saint Angel, C. Saint-Jean (2023) – Théorème d’approximation pour neurones hypersphériques, GRETSI.\n\nJ. de Saint Angel, C. Saint-Jean (2021) – Couches Dense et Conv2D sphériques via l’algèbre géométrique conforme, ORASIS.\n\nMots-clés : réseaux de neurones, hypersphères, algèbre géométrique conforme, détection d’anomalies, séries temporelles, apprentissage profond, initialisation, SVDD, IA interprétable, optimisation, convergence.\n\nDétail par chapitre :\n\nChapitre 1 : Réseaux inspirés du formalisme de l’algèbre géométrique conforme\nCe chapitre présente les fondements mathématiqu",
      "source_file": "these_julien.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_4",
      "text": "es et conceptuels de l’approche. Il introduit le modèle conforme d’Hestenes, permettant de représenter hyperplans et hypersphères dans un espace vectoriel augmenté Rⁿ⁺¹,¹.\nLes couches hypersphériques y sont définies comme une extension naturelle des couches denses et convolutives classiques. Le texte décrit le produit interne conforme (x̃·is̃), les règles de calcul associées, et les contraintes de normalisation nécessaires (s̃² = ρ², e∞·is̃ = -1).\nL’auteur compare les performances entre couches denses classiques et couches hypersphériques sur des jeux de données synthétiques (Easy et Dif). Les résultats montrent une convergence plus rapide et une meilleure stabilité avec les couches hypersphériques. Le chapitre introduit également la notion de convolution hypersphérique et détaille sa mise en œuvre dans des architectures de type Conv2D.\nEnfin, une comparaison avec les réseaux RBF et les réseaux de neurones Clifford est réalisée, montrant que les couches hypersphériques constituent une généralisation géométriquement cohérente de ces modèles.\n\nChapitre 2 : Réseaux de neurones à couches hypersphériques et théorème d’approximation\nCe chapitre établit le cadre théorique garantissant la",
      "source_file": "these_julien.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_5",
      "text": "capacité d’approximation des réseaux hypersphériques. En s’appuyant sur le théorème de densité de Schwartz, l’auteur démontre qu’un réseau à une couche hypersphérique peut approximer toute fonction continue sur un compact de Rⁿ, à condition que la fonction d’activation soit régulière.\nUne comparaison est réalisée entre la structure des fonctions produites par des réseaux à couches classiques (linéaires) et celles produites par des couches hypersphériques (quadratiques). Des expérimentations sur des fonctions 1D illustrent les capacités d’approximation et la stabilité numérique des réseaux hypersphériques.\nLe chapitre conclut que ces réseaux disposent d’une expressivité au moins équivalente aux MLP classiques, tout en offrant une meilleure interprétation géométrique des frontières de décision.\n\nChapitre 3 : Application à la détection d’anomalies\nLe troisième chapitre est consacré à l’application principale de la thèse : la détection d’anomalies.\nIl commence par un état de l’art complet des méthodes classiques (Elliptic Envelope, Isolation Forest, One-Class SVM, Support Vector Data Description et variantes profondes). L’auteur montre les limites de ces approches face à la non-linéari",
      "source_file": "these_julien.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_6",
      "text": "té et à la complexité géométrique des données.\nDe nouveaux modèles sont ensuite introduits : SPH-Anomaly, SPH-SVDD, Deep SPH-SVDD et Deep M-SPH SVDD. Ces modèles exploitent la représentation conforme pour apprendre directement les centres et rayons des hypersphères englobantes des données normales.\nLe Deep M-SPH SVDD combine plusieurs hypersphères pour modéliser des sous-groupes de comportements normaux.\nDes expériences approfondies sont présentées sur données synthétiques 2D, MNIST et CIFAR-10, avec étude du paramètre ν, des métriques de performance (AUC-ROC, AUC-PR, F1-score), et de la stabilité face au phénomène de “collapse” observé dans le Deep SVDD classique.\nLes résultats montrent que les couches hypersphériques apportent une meilleure robustesse et un pouvoir de détection supérieur tout en restant interprétables.\n\nAnnexes :\nLes annexes fournissent les détails mathématiques et techniques complémentaires :\n\nA : calcul des covariances et méthodes d’initialisation des couches hypersphériques.\n\nB : preuve complète du théorème d’approximation.\n\nC : analyse du phénomène de collapse et justification géométrique de sa prévention dans les architectures hypersphériques.\n\nStructure du",
      "source_file": "these_julien.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_7",
      "text": "manuscrit :\n\nFondements théoriques : algèbre géométrique conforme et représentation vectorielle des hypersphères.\n\nDéfinition et implémentation des couches hypersphériques (denses et convolutives).\n\nThéorème d’approximation pour neurones hypersphériques.\n\nApplication à la détection d’anomalies avec les modèles SPH-SVDD et M-SPH SVDD.\n\nValidation expérimentale et perspectives d’évolution.\n\nRésumé synthétique :\nLa thèse de Julien de Saint Angel propose une approche innovante de l’apprentissage profond basée sur des couches hypersphériques intégrées au formalisme de l’algèbre géométrique conforme. Cette approche unifie hyperplans et hypersphères dans une même représentation, permet une meilleure compréhension géométrique des réseaux neuronaux et améliore la détection d’anomalies dans des environnements complexes. Grâce à l’introduction de modèles Deep SPH SVDD et Deep M-SPH SVDD, ainsi qu’à une méthode d’initialisation spécifique, l’auteur démontre des gains de performance significatifs, une convergence plus rapide et une interprétabilité accrue des modèles d’intelligence artificielle.",
      "source_file": "these_julien.txt",
      "chunk_index": 7,
      "embedding": []
    },
    {
      "id": "these_julien_en.txt_chunk_0",
      "text": "Title: \"Conformal Geometry and Neural Networks: New Approaches for Interpretable AI\"\n\nAbstract:\nThis thesis explores the use of conformal geometric algebras to improve the interpretability and robustness of neural networks. It proposes new methods for initialization and analysis of hyperspherical layers.\n\nChapters:\n1. Introduction to Conformal Geometry\n2. Modeling Hyperspheres in AI\n3. Initialization Methods for Hyperspherical Networks\n4. Applications to Anomaly Detection\n5. Perspectives and Conclusions\n\nKey Results:\n- New initialization method for hyperspherical layers\n- Improved anomaly detection on several datasets\n- Publication of 3 major articles on the subject\n\nThesis by Julien De Saint Angel\nFull Title: Hyperspherical Neural Networks – Application to Anomaly Detection\nUniversity: La Rochelle University\nDoctoral School: Euclide\nLaboratory: Mathematics, Image and Applications (MIA)\nDefense: July 4, 2025\n\nSupervision:\nPhD Advisor: Christophe Saint-Jean\nLab Director: Catherine Choquet\n\nReviewers: Yannick Berthoumieu (University of Bordeaux), Hoel Le Capitaine (University of Nantes)\nExaminers: Laetitia Chapel (IRISA), Catherine Choquet (MIA)\n\nSummary:\nThis thesis introduces a new",
      "source_file": "these_julien_en.txt",
      "chunk_index": 0,
      "embedding": []
    },
    {
      "id": "these_julien_en.txt_chunk_1",
      "text": "architecture of hyperspherical neural networks based on the formalism of conformal geometric algebra (CGA). It aims to improve data representation in high-dimensional spaces and enhance the performance of anomaly detection models in complex contexts such as time series, computer vision, and nonlinear signal analysis. Hyperspherical layers replace the classical hyperplanes of neural networks with learnable hyperspheres, enabling nonlinear class separation and greater robustness to non-Gaussian distributions.\n\nScientific Objectives:\n- Propose a unified geometric formalism to represent hyperplanes and hyperspheres using conformal algebra.\n- Develop hyperspherical neural layers (dense and convolutional) suitable for deep learning.\n- Define a specific initialization method ensuring stability and convergence in these layers.\n- Apply these models to anomaly detection using variants of Support Vector Data Description (SVDD) adapted to hyperspherical geometry.\n\nMain Contributions:\n- Development of the Deep M-SPH SVDD model, a multi-sphere approach for anomaly detection based on conformal geometry.\n- Design of a hyperspherical initialization method ensuring numerical stability and efficient",
      "source_file": "these_julien_en.txt",
      "chunk_index": 1,
      "embedding": []
    },
    {
      "id": "these_julien_en.txt_chunk_2",
      "text": "convergence.\n- Practical implementation of dense and convolutional hyperspherical layers in PyTorch and TensorFlow.\n- Establishment of a universal approximation theorem demonstrating the ability of these networks to approximate any continuous function on a compact set.\n- Experimental validation on synthetic and real data (MNIST, CIFAR-10) showing performance and stability gains.\n\nApplications:\nAnomaly detection in time series, industrial monitoring, visual tide gauge analysis, high-frequency sports diagnostics, and complex image recognition.\n\nRepresentative Excerpts:\n\"Hyperspherical layers allow data to be projected onto spherical surfaces, facilitating class separation and the detection of outliers.\"\n\"The Deep M-SPH SVDD method improves the robustness of anomaly detection by leveraging the geometry of hyperspheres in feature space.\"\n\"Initialization adapted to hyperspherical layers optimizes model convergence and stability.\"\n\nAssociated Publications:\nJ. de Saint Angel, C. Saint-Jean, C. Choquet (2025) – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, in Recent Applications in Deep Learning.\nJ. de Saint Angel, C. Saint-Jean (2024) – Mult",
      "source_file": "these_julien_en.txt",
      "chunk_index": 2,
      "embedding": []
    },
    {
      "id": "these_julien_en.txt_chunk_3",
      "text": "i-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA.\nJ. de Saint Angel, C. Saint-Jean (2023) – Approximation Theorem for Hyperspherical Neurons, GRETSI.\nJ. de Saint Angel, C. Saint-Jean (2021) – Spherical Dense and Conv2D Layers via Conformal Geometric Algebra, ORASIS.\n\nKeywords: neural networks, hyperspheres, conformal geometric algebra, anomaly detection, time series, deep learning, initialization, SVDD, interpretable AI, optimization, convergence.\n\nChapter Details:\n\nChapter 1: Networks Inspired by the Formalism of Conformal Geometric Algebra\nThis chapter presents the mathematical and conceptual foundations of the approach. It introduces Hestenes' conformal model, allowing the representation of hyperplanes and hyperspheres in an augmented vector space Rⁿ⁺¹,¹.\nHyperspherical layers are defined as a natural extension of classical dense and convolutional layers. The text describes the conformal inner product (x̃·is̃), associated calculation rules, and necessary normalization constraints (s̃² = ρ², e∞·is̃ = -1).\nThe author compares the performance of classical dense layers and hyperspherical layers on synthetic datasets (Easy and Dif). Results show faster convergence and be",
      "source_file": "these_julien_en.txt",
      "chunk_index": 3,
      "embedding": []
    },
    {
      "id": "these_julien_en.txt_chunk_4",
      "text": "tter stability with hyperspherical layers. The chapter also introduces the notion of hyperspherical convolution and details its implementation in Conv2D-type architectures.\nFinally, a comparison with RBF networks and Clifford neural networks is made, showing that hyperspherical layers constitute a geometrically coherent generalization of these models.\n\nChapter 2: Hyperspherical Neural Networks and the Approximation Theorem\nThis chapter establishes the theoretical framework guaranteeing the approximation capacity of hyperspherical networks. Relying on Schwartz's density theorem, the author demonstrates that a single hyperspherical layer network can approximate any continuous function on a compact set in Rⁿ, provided the activation function is regular.\nA comparison is made between the structure of functions produced by classical (linear) layer networks and those produced by hyperspherical (quadratic) layers. Experiments on 1D functions illustrate the approximation capabilities and numerical stability of hyperspherical networks.\nThe chapter concludes that these networks have expressiveness at least equivalent to classical MLPs, while offering a better geometric interpretation of decis",
      "source_file": "these_julien_en.txt",
      "chunk_index": 4,
      "embedding": []
    },
    {
      "id": "these_julien_en.txt_chunk_5",
      "text": "ion boundaries.\n\nChapter 3: Application to Anomaly Detection\nThe third chapter is devoted to the main application of the thesis: anomaly detection.\nIt begins with a comprehensive review of classical methods (Elliptic Envelope, Isolation Forest, One-Class SVM, Support Vector Data Description and deep variants). The author shows the limitations of these approaches in the face of nonlinearity and geometric complexity of data.\nNew models are then introduced: SPH-Anomaly, SPH-SVDD, Deep SPH-SVDD, and Deep M-SPH SVDD. These models use the conformal representation to directly learn the centers and radii of hyperspheres enclosing normal data.\nDeep M-SPH SVDD combines several hyperspheres to model subgroups of normal behaviors.\nExtensive experiments are presented on 2D synthetic data, MNIST, and CIFAR-10, with study of the parameter ν, performance metrics (AUC-ROC, AUC-PR, F1-score), and stability with respect to the “collapse” phenomenon observed in classical Deep SVDD.\nResults show that hyperspherical layers provide better robustness and superior detection power while remaining interpretable.\n\nAppendices:\nThe appendices provide additional mathematical and technical details:\nA: calculation",
      "source_file": "these_julien_en.txt",
      "chunk_index": 5,
      "embedding": []
    },
    {
      "id": "these_julien_en.txt_chunk_6",
      "text": "of covariances and initialization methods for hyperspherical layers.\nB: complete proof of the approximation theorem.\nC: analysis of the collapse phenomenon and geometric justification of its prevention in hyperspherical architectures.\n\nManuscript Structure:\n- Theoretical foundations: conformal geometric algebra and vector representation of hyperspheres.\n- Definition and implementation of hyperspherical layers (dense and convolutional).\n- Approximation theorem for hyperspherical neurons.\n- Application to anomaly detection with SPH-SVDD and M-SPH SVDD models.\n- Experimental validation and future perspectives.\n\nSummary:\nJulien de Saint Angel's thesis proposes an innovative deep learning approach based on hyperspherical layers integrated into the formalism of conformal geometric algebra. This approach unifies hyperplanes and hyperspheres in a single representation, enables better geometric understanding of neural networks, and improves anomaly detection in complex environments. Thanks to the introduction of Deep SPH SVDD and Deep M-SPH SVDD models, as well as a specific initialization method, the author demonstrates significant performance gains, faster convergence, and increased inte",
      "source_file": "these_julien_en.txt",
      "chunk_index": 6,
      "embedding": []
    },
    {
      "id": "these_julien_en.txt_chunk_7",
      "text": "rpretability of AI models.",
      "source_file": "these_julien_en.txt",
      "chunk_index": 7,
      "embedding": []
    }
  ]
}