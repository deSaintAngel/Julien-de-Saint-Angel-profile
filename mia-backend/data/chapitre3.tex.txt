Après un bref état de l'art sur les méthodes de d\'etection d'anomalies, ce chapitre explore l'application de couches hypersphériques dans les réseaux neuronaux pour ce type d'application, en mettant l'accent sur les techniques de Support Vector Data Description (SVDD) et de Deep SVDD. Une adaptation du Deep SVDD est introduite en incorporant après la dernière couche linéaire une couche hypersphérique définie dans l'algèbre géométrique conforme. De plus, une nouvelle méthode appelée Deep M-SPH SVDD est proposée afin d'étendre l'approche à des multi-sphères, permettant ainsi au modèle de capturer des groupes distincts de points de données normales.\\ De nouvelles fonctions de perte conçues pour éviter l'intersection et l'inclusion des sphères sont également présentées. \\ Des expériences préliminaires sont menées sur un ensemble de données synthétiques, ainsi que des évaluations sur les ensembles de données MNIST et CIFAR-10. Ces comparaisons sont utilisées pour évaluer la performance de la méthode proposée et déterminer s'il est préférable, pour un nombre fixe de paramètres, d'utiliser une seule hypersphère de haute dimension ou plusieurs hypersphères de dimension inférieure. La détection d'anomalies est une \'etape souvent cruciale dans l'exploration de données, y compris pour la phase de pr\'eparation d'un \'eventuel apprentissage. Le but est d'identifier des points de données significativement différents des autres dans un ensemble, souvent appelés valeurs aberrantes ou anomalies. Ces points de données ne se conforment pas au modèle de distribution ``habituel'', ce dernier \'etant d'ailleurs difficile \`a d\'efinir. Dans certains cas, le concept de valeurs aberrantes est distingué de celui des anomalies . En effet, une valeur aberrante englobe à la fois le bruit et l'anomalie. La détection de nouveauté consiste à repérer des motifs ou comportements inhabituels par rapport à la distribution observée des données d'apprentissage. La détection d'anomalies pose des défis notables. En effet, les anomalies sont souvent imprévisibles jusqu'à ce qu'elles se produisent, interdisant d'exploiter le contexte. { A contrario}, la normalité d'une donnée peut varier en fonction du contexte. L'anomalie est ainsi difficile \`a d\'efinir intuitivement. Si on ambitionne une d\'etection automatique, d'autres difficultés s'ajoutent. Par exemple, en raison de leur rareté, faible proportion et diversité, il peut parfois être impossible d'étiqueter les anomalies. Le déséquilibre entre les classes complique également la détection.\\ Il existe plusieurs approches pour la détection d'anomalies. Les différentes méthodes, y compris celles basées sur l'apprentissage profond, sont passées en revue dans . L'article les classe selon trois approches principales : l'apprentissage profond pour l'extraction de caractéristiques, l'apprentissage de représentations de la normalité et l'apprentissage de scores d'anomalies.\\ Dans la catégorie de l'extraction de caractéristiques, on retrouve des méthodes visent à réduire les données de haute dimension en représentations caractéristiques de basse dimension. On suppose que ces représentations extraites préservent les informations discriminantes pour aider à séparer les anomalies des données normales, même si le modèle original n'a pas été entraîné pour détecter les anomalies. Une approche courante consiste à utiliser des réseaux pré-entraînés tels que VGG ou RESNET puis transférer les sorties vers un détecteur d'anomalies tels que SVM ou SVDD .\\ L'apprentissage de représentations de la normalité vise à capturer les schémas réguliers suivis par les données, en utilisant des méthodes comme les autoencodeurs et les réseaux antagonistes génératifs (GAN). Par exemple, les méthodes basées sur la reconstruction apprennent à reconstruire les données normales et détectent les anomalies en mesurant la différence entre l'entrée et la sortie. L'apprentissage de scores d'anomalie apprend directement les scores d'anomalie via des réseaux neuronaux, résumant les instances normales avec un modèle discriminant à une classe.\\ Les m\'ethodes d'apprentissage profond doivent permettre de relever les défis de la détection d'anomalies tels que des faibles taux de rappel, des anomalies complexes, la haute dimensionnalité et le déséquilibre des données. Elles ont en particulier l'avantage de pouvoir intégrer des sources de données diverses pour une solution complète. Nous présentons ici un bref état de l'art des méthodes de détection d'anomalies, en mettant l'accent sur la définition d'anomalie propre à chacune des méthodes. Cette méthode de détection d'anomalie repose sur l'hypothèse que les données normales suivent une distribution normale multivariée, tandis que les anomalies s'écartent de cette distribution. Il s'agit donc d'une méthode paramétrique qui se base sur une estimation robuste de la matrice de covariance des données (Minimum Covariance Determinant) . Rappelons que la matrice de covariance est une mesure de la dispersion des données autour de leur moyenne, et qu'elle est utilisée pour déterminer la forme de la distribution des données. La distance de Mahalanobis est définie par la formule suivante~: $$D_M(, ) = - )^ ^{-1} ( - )}$$ où $$ est le point à comparer, $$ est la moyenne du groupe de points, et $$ est la matrice de covariance du groupe de points. On remarque que cette distance tient compte de la corrélation entre les variables, contrairement à la distance euclidienne ($ = I$).\\ L'algorithme recherche un sous-ensemble de $h$ points qui minimise le déterminant de la matrice de covariance, ce qui revient à identifier la région la plus compacte et donc la plus représentative de la distribution principale des données -- celle où la densité de points normaux est la plus élevée. Pour garantir une estimation robuste et statistiquement valide, la taille $h$ du sous-ensemble doit satisfaire la condition suivante : $$ h ( (1 - )K ,\; {2} ) $$ où $K$ est le nombre d'observations en dimension $m$ et $$ la proportion maximale d'anomalies que l'on souhaite tolérer. Plusieurs sous-ensembles de taille $h$ sont générés aléatoirement, et chacun est raffiné par itérations successives afin d'approcher la configuration minimisant effectivement le déterminant. Pour chaque ensemble: La matrice de covariance et la moyenne sont calculées, et la distance de Mahalanobis est calculée pour chacun des $h$ points. Parmi les $n$ points initiaux, les $h$ points ayant plus petites distances sont choisis pour former un nouveau sous-ensemble. On remarque qu'un algorithme de partitionnement est suffisant pour cette étape; il n'est pas nécessaire d'effectuer un tri complet. Les étapes 1. et 2. sont répétées jusqu'à ce que le déterminant de la matrice de covariance ne diminue plus ou est nul. Le sous-ensemble final est considéré comme robuste. Le sous-ensemble final est celui qui minimise le déterminant de la matrice de covariance.\\ Une version rapide de l'algorithme, appelée FastMCD , accélère encore le processus de recherche par l'utilisation de deux heuristiques: \'Elagage précoce des sous-ensembles les moins prometteurs après deux étapes de l'algorithme précédent. Lorsque le nombre de points est grand (i.e.\ supérieur à 600 en 1999), une stratégie de type ``diviser pour régner'' a été proposée afin de repartir la recherche de sous-ensembles support avant d'en fusionner les meilleurs et de faire un raffinement final. Le score d'anomalie est calculé à partir de la distance de Mahalanobis. Les points dont la distance dépasse un seuil prédéfini sont considérés comme des anomalies potentielles. Cette méthode est sensible aux données aberrantes et aux violations de l'hypothèse de normalité multivariée. Elle est implémentée dans la librairie scikit-learn sous le nom Elliptic Envelope. L'algorithme Isolation Forest (iForest) est conçu pour détecter les anomalies dans un ensemble de données en utilisant une approche basée sur l'isolement. La méthode commence par construire un ensemble d'arbres d'isolation (iTrees), chacun étant un arbre de décision binaire qui partitionne aléatoirement les données.\\ Pour chaque arbre d'isolation, l'algorithme sélectionne aléatoirement un attribut (une caractéristique des données) et une valeur de séparation parmi les valeurs possibles de cet attribut (entre le minimum et le maximum). Ce découpage crée deux sous-groupes de données, que l'on continue à partitionner récursivement jusqu'à ce que chaque point de donnée soit isolé. Le nombre de partitions nécessaires pour isoler un point $x$ est égal à la longueur $h(x)$ du chemin de la racine de l'arbre à ce point. Les anomalies, étant plus rares et séparées des autres points, seront en moyenne isolées plus rapidement, ce qui se traduit par des longueurs de chemin plus courtes. À partir d'un certain nombre d'arbres construits, l'algorithme estime la longueur moyenne \( E(h(x)) \) du chemin pour chaque point $x$ dont il résulte un score d'anomalie suivant: $$ (x) = 2^{-{c(K)}} $$ où $c(K) = 2H(K-1)-{K}$ représente la longueur moyenne du chemin d'une recherche infructueuse dans un arbre binaire équilibré avec $K$ points avec $H(i)$ le i-ème nombre harmonique.\\ Les points dont le score est proche de 1 (ou supérieur à un seuil prédéfini) sont considérés comme des anomalies potentielles (car facilement isolées, i.e. $E(h(x))$ tend vers 0). À l'opposé, les points normaux obtiennent un score bien inférieur à 0.5 (car nécessitant plus de partitions pour être isolés). Si les données sont uniformément réparties dans l'espace des caractéristiques, le score moyen est alors proche de 0.5 pour peu que le nombre d'arbres soit suffisamment grand. L'algorithme est rapide car hautement parallélisable, efficace et robuste aux valeurs aberrantes. L'algorithme Local Outlier Factor (LOF) est une méthode de détection d'anomalies qui identifie des points aberrants en comparant leur densité locale à celle de leurs voisins. L'idée principale est que les anomalies se trouvent dans des régions de faible densité locale par rapport à leur voisinage immédiat. On doit donc se doter en premier lieu d'une distance $D$ entre les points, puis construire une mesure de densité locale à partir des distances entre les points et leurs $k$ plus proches voisins. Pour cela, on note $D_k(p)$ la distance d'un point $p$ à son $k$-ième le plus proche voisin. La distance d'accessibilité (RD) entre deux points $p$ et $q$ est définie comme $$_k(p, q) = \{D_k(q), D(p, q)\}$$ On aura remarqué l'asymétrie de la formule. Ainsi, tous les $k$ plus proches voisins de $q$ seront considérés comme équivalents pour $q$ suivant RD. La densité d'accessibilité locale (LRD) est définie comme l'inverse de la distance moyenne d'accessibilité de $p$ par rapport à ses voisins $q$: $$_{k'}(p) = ({k'} _{q N_k(p)} (p, q))^{-1}$$ où $N_{k'}(p)$ est l'ensemble des $k'$ plus proches voisins de $p$ où $k'$ est un hyper-paramètre désignant le nombre minimal de points pour former un groupement. Cette mesure permet de quantifier la densité locale de $p$ par rapport à ses voisins et sera d'autant plus grande (resp. faible) que $p$ est entouré de points proches (resp. éloignés).\\ Le score LOF d'un point $p$ est obtenu en normalisant la densité locale moyenne autour de $p$ par rapport à celle de $p$: $$_{k'}(p) = ( (p)} _{k'}(q)}{k'}) / _{k'}(p)$$ Si le score satisfait $_{k'}(p) > 1$, alors la densité du point $p$ est faible et le label correspondant à une anomalie lui est attribué. Une illustration est donnée dans la figure où le rayon de chaque cercle est proportionnel au score d'anomalie de son centre. On constate que les points les plus éloignés des autres obtiennent un score élevé. [htbp] La méthode One-class SVM (OC-SVM) est une méthode de détection d'anomalies qui repose sur l'apprentissage non supervisé. Elle est basée sur les machines à vecteurs de support (SVM) qui sont classiquement utilisées pour la classification binaire supervisée. En modifiant la fonction à optimiser, on peut adapter les SVM à un problème de détection d'anomalies. Commençons par décrire les machines à vecteurs de support (SVM) . Les machines à vecteurs de support (SVM) sont des modèles d'apprentissage supervisé qui peuvent être utilisés pour la classification ou la régression. Pour la classification binaire, l'objectif des SVM est de trouver un hyperplan qui sépare les données en deux classes. On distingue deux types de SVM~: Les SVM linéaires, qui cherchent à séparer les données par un hyperplan linéaire. Les SVM à noyaux qui permettent de séparer non linéairement les données à l'aide d'une fonction noyau. Le cas du noyau linéaire nous ramène au premier cas. Lorsque les données sont linéairement séparables, il existe une infinité d'hyperplans qui peuvent séparer les données. Parmi ces hyperplans, l'unique plan qui maximise la marge de séparation entre les deux classes est appelé hyperplan optimal. La marge est définie comme la distance entre l'hyperplan et les points les plus proches de chaque classe. Certains points, appelés vecteurs de support, se trouvent sur la marge et définissent l'hyperplan optimal. Nous allons détailler le problème d'optimisation des SVM dans le cas linéairement séparable.\\ Soit $$ un hyperplan de $^m$ défini par l'équation $ + w_0 = 0$. La fonction de décision est la suivante: y( ) = sgn ( . + w_0) Pour un point générique $ ^m$, on peut le décomposer en deux parties~: sa projection $_{}$ sur l'hyperplan $$ et un vecteur orthogonal à cet hyperplan colinéaire à $$ (voir figure ). Afin de quantifier la marge de séparation, on introduit la distance signée $r$ d'un point $$ à l'hyperplan~: $$r() = . + w_0}{||||}$$ [H] Après normalisation de $$ et $w_0$, les points se situant sur la marge coté positif (resp. négatif) se trouvent à une distance signée de 1 (resp. -1). Maximiser la marge tout en classant correctement l'ensemble des points $\{(x_k,y_k) \{1, -1\})\}_k$ s'écrit comme un problème de maximisation sous contraintes, à savoir $$,w_0)}{} {||||} y_k (. + w_0) 1 k \{1,,K\} $$ On remarquera que les contraintes intègrent naturellement le cas des exemples positifs et négatifs. Ce problème peut se reformuler comme le problème de minimisation sous contraintes suivant: ,w_0)}{} {2} ||||^2 \\ & y_k (. + w_0) 1 k \{1,,K\} La solution de ce problème est unique en raison de la convexité stricte du premier terme et de la convexité des contraintes . Le lecteur intéressé par la résolution effective peut se reporter à ce même ouvrage. Lorsque les données ne sont pas linéairement séparables dans l'espace initial, on cherche un autre espace de représentation, généralement de plus grande dimension, dans lequel on espère faciliter la séparabilité des données par un hyperplan. Pour effectuer cette transformation, nous utilisons une fonction de plongement \(\), illustrée dans la figure .\\ En effet, le premier sous-graphique montre un ensemble de données non linéairement séparable dans un espace 2d. Les points rouges et bleus représentent deux classes distinctes. La distribution des données est telle qu'il est impossible de tracer une ligne droite qui sépare les deux classes de manière correcte. Cette disposition montre clairement qu'une séparation linéaire est impossible dans cet espace.\\ Le second sous-graphique montre les mêmes données après qu'elles aient été projetées dans un espace de dimension 3 via un plongement explicite. Cela signifie que chaque point \( (x_1, x_2) \) est transformé en un vecteur dans l'espace de caractéristiques par la fonction suivante de \( ^2 ^3 \) : $$ (x_1, x_2) = (x_1^2, x_2^2, 2 x_1 x_2) $$ Cette projection transforme les coordonnées \((x_1, x_2)\) de chaque point en une représentation dans un espace à trois dimensions. Comme on peut le voir, le plan en jaune permet une séparation linéaire des données dans cet espace transformé. Le graphique illustre donc le principe des méthodes à noyaux pour traiter des problèmes non linéairement séparables. [htbp] Dans le cadre de ce manuscript, nous nous limitons au cas où $$ est connu explicitement puisque réalisée par un réseau de neurones. Dans ce cadre, l'espace $F$ des caractéristiques qui est l'image de $^n$ par $$ est un sous-ensemble de $^m$. Le problème d'optimisation pass\'e dans l'espace caractéristique $F$ devient : ,w_0)}{{}} & {2} ||||^2 \\ & y_k( . (_k) + w_0) 1 , k \{1,,K\} où le vecteur $w$ est maintenant un vecteur de $^m$.\\ Le lagrangien associ\'e \`a est donn\'e par : (,w_0,) = {2} ||||^2 - _{k=1}^{K} _k (y_k -1) où les $_k 0$ sont les multiplicateurs de Lagrange. En observant le terme de droite dans cette minimisation, on constate que pour tout point $_k$ qui n'étant pas dans la marge ($y_k -1 > 0$), la solution doit vérifier $_k = 0$ pour être un minimum. La recherche des points critiques annulant la diff\'erentielle du lagrangien ($(,w_0,)}{ } = 0$ et $(,w_0,)}{ w_0} = 0$) amène les conditions suivantes :\\ = _{k=1}^{K} _k y_k (_k)\\ 0 = _{k=1}^{K} _k y_k \\ D'après notre remarque précédente, on en déduit que seul les points sur la marge, vérifiant $y_k -1 = 0$, contribuent à la définition de $$. Ils sont appelés vecteurs de support. En remplaçant $$ par $_{k=1}^{K} _k y_k (_k)$ dans le lagrangien, on élimine les variables $$ et $w_0$ pour obtenir le problème d'optimisation (dit dual) suivant:\\ () = _{k=1}^{K} _k -{2}_{k, k'=1}^{K} _k_{k'} y_k y_{k'} (_k). (_{k'}) \\ \\ _{k=1}^{K} _k y_k =0 _k 0 k \{1,,K\} La prédiction initiale (équation ) dans l'espace dual s'écrit : y() = sgn (_{k=1}^{K} _k y_k ().(_k)+w_0) = sgn (_{_k SV} _k y_k ().(_k)+w_0) \\ où $SV$ désigne l'ensemble des vecteurs de support. variables")} En pratique, l'existence d'un hyperplan séparateur n'est pas garantie malgré un plongement dans un espace de dimension supérieure, dans le cas d'étiquetage contradictoire, . Afin d'autoriser à violer la contrainte $y_k(. + w_0) 1$, on va introduire des nouvelles variables $ 0$ permettant cela. Le problème d'optimisation ~: ,w_0)}{} {2} ||||^2 \\ \\ y_k( . _k + w_0) 1 k \{1,,K\} \,. devient ,w_0, _k)}{} {2} ||||^2 + C_{k=1}^{K} _k\\ \\ y_k( . _k + w_0) 1-_k, _k 0, k \{1,,K\} \,. où $C > 0$ est un hyper-paramètre de la méthode. On notera que: L'exemple est bien classé si et seulement si $_k 0; 1]$ est un paramètre de la $ SVM$ permettant obtenir des garanties statistiques sur la marge. En fixant le paramètre de $$ à 1 dans la $ SVM$, on établit une équivalence entre SVM classique et ce cas particulier. Il suit que : \(\) est une borne supérieure sur la fraction des points d'apprentissage qui peuvent être mal classés (i.e., ceux pour lesquels \(_k > 1\)). \(\) est une borne inférieure sur la fraction des points qui sont des vecteurs supports (i.e., ceux pour lesquels \(_k > 0\)). Cette paramétrisation par $$ est ainsi plus naturelle.\\ En observant les contraintes dans l'équation , on remarque que soit $_k=0$, soit $_k 1 - y_k( . _k + w_0)$. On peut intégrer directement ces contraintes dans le critère à optimiser: ,w_0)}{} {2} ||||^2 + C_{k=1}^{K} (0, 1 - y_k( . _k + w_0))\\ ce qui correspond à l'écriture d'une fonction de perte { hinge}. \\ Dans la section précédente, nous avons décrit un algorithme de classification binaire basé sur la maximisation de la marge. On se place maintenant dans le cas où le jeu de données est constitué principalement de données normales. L'objectif de décrire ces données de façon à pouvoir identifier d'autres qui s'en écarteraient. L'algorithme One Class SVM (OC-SVM) repose sur la recherche de l'hyperplan $(w, 0)$ qui sépare les données de l'origine (voir figure ). [htbp] Un nouveau terme $$, assimilable à une distance de l'hyperplan à l'origine apparaît dans la minimisation : , , _k)}{} {2} ||||^2 + C_{k=1}^{K} _k - \\ \\ ._k -_k, _k 0, k \{1,,K\} \\ Le paramètre $C = { K}$, avec $ ]0, 1]$, contrôle toujours le taux de points du mauvais coté de l'hyperplan via les variables de tolérance $_k$. La fonction de décision (normal / anormal) met en lumière le lien entre $$ et un potentiel terme $w_0$ (cf. équation ) y() = sgn (_{k=1}^{K} _k ().(_k) - ) On serait tenté donc d'écrire $ = -w_0$ et garder l'optimisation de l'algorithme précédent. Toutefois, on remarque que l'on maximise à la fois la marge (via la minimisation de $||w||^2/2$) et $$ (distance signée à l'origine) (cf. figure ). Comme précédemment, on peut transformer les contraintes par la fonction convexe de perte { hinge} pour en faire un problème d'optimisation non contrainte: , )}{} {2} ||||^2 - + C_{k=1}^{K} (0, - . _k)\\ Cette formulation peut être rapprochée de la formulation classique du problème de classification binaire avec SVM (équation ), où un terme de biais est également présent. Dans le cadre des OC-SVM, ce terme de biais \(w_0\) permet d'ajuster l'hyperplan de telle sorte qu'il ne passe pas nécessairement par l'origine mais optimise la marge par rapport à l'ensemble des données normales, considérées comme appartenant à une seule classe. Les méthodes de type SVM sont connues pour être qualitativement performantes mais avoir du mal à passer à l'échelle. Afin de remplacer l'optimisation du problème primal ou dual, il est nécessaire d'en adapter la formulation. Dans , le problème d'optimisation est réécrit pour une approche par les réseaux de neurones en réinjectant les contraintes à l'aide des variables { slack}. Le probl\`eme revient \`a un problème d'optimisation quasi-quadratique. En effet, en tenant compte de l'inégalité ^t _k + w_0 _k$} (la marge est à $1$ de la frontière de décision) et de la condition $_k 0 $, les contraintes peuvent être retrouv\'ees en introduisant le maximum entre 0 et $\{1 - ^t _k + w_0\}$. La solution du problème minimise donc aussi la fonction de coût suivante : $$(, w_0; \{_k\}_k) = {2} ||||^2 + C_{k=1}^{K} \{0,1- ^t _k+w_0 \} + w_0$$ Le réseau de neurones consiste en une simple couche linéaire avec un bias. Cela revient à utiliser la fonction de perte de { hinge} (terme de droite) avec un terme de régularisation. Il est naturel de se demander \`a quel point la solution obtenue par la résolution du problème d'optimisation convexe (via l'implémentation dans ) diff\`ere de celle obtenue par une descente de gradient pour la fonction de perte ci-dessus. Une illustration est pr\'esent\'ee dans la figure pour un jeu de données synthétiques en dimension 2. Après avoir extraites les équations cart\'esiennes ($y = a x + b$) des droites correspondantes aux frontières de décision pour les solutions trouvées, on observe que la différence entre les valeurs des $a$ (resp. des $b$) est de l'ordre de $1 10^{-3}$ (resp. $1 10^{-2}$). On peut donc conclure à une quasi-equivalence des deux méthodes sur ce jeu de données. [htbp] {0.49} {0.49} \`A ce stade, et même si nous n'avons pas poursuivi l'idée, on peut imaginer adapter l'idée de la méthode OC-SVM au cas d'une sphère qui sépare le mieux le point euclidien $$ ($e_0$ dans le modèle conforme) des données en maximisant son rayon: {} {2} ^2 - C_{k=1}^{K} _k \\ \\ _i _k _k, _k 0 k \{1,,K\}, \\ avec $C={ K} >0$ et le produit int\'erieur not\'e ici $_i$. On remarque immédiatement que ne pas fixer le centre aboutit à un problème mal posé; le centre pourrait sans doute être fixé ailleurs qu'en $$. L'optimisation par Adam ne s'effectue que sur le rayon alors que le centre de la sphère sera fixé et minimise la fonction de perte $-{2} ^2 + { K} _k \{0, _i _k\}$. La figure montre les résultats obtenus pour différentes valeurs de $$ sur les données précédentes. De manière cohérente, on constate que le taux de couverture des points augmente en même temps de $$. [ht] [b]{0.32} [b]{0.32} [b]{0.32} Les méthodes précédemment décrites cherchaient à séparer par un hyperplan (ou une hypersphère) les données de l'origine. Nous nous intéressons maintenant au cas où l'on cherche à englober les données dans l'hypersphère la plus petite possible avec une marge de tolérance. Cette approche nous est apparue comme la plus à même d'être développée dans le cadre de l'algèbre conforme. Nous allons commencer par décrire la méthode Support Vector Data Description (SVDD) puis allons voir comment elle peut être adaptée à l'apprentissage profond à l'instar de ce que nous avons vu pour la "Deep One Class SVM". La méthode Support Vector Data Description (SVDD) vise à trouver les paramètres de centre et de rayon ($c$, $$) de la plus petite hypersphère englobante pour un ensemble de points. La formulation du problème introduit les variables de relaxation qui permettent à certains points de violer ces contraintes et de se retrouver en dehors de l'hypersphère. Comme dans le cas des SVM, le travail est fait apr\`es plongement par $$ des donn\'ees dans un espace de caractéristiques. Pour ${ccccc} & : & ^n & & F $ une fonction de plongement associée au noyau $$, une sphère de centre $ F$, de rayon $ $, et $C = 1/ K$ la variable de contrôle avec $ ]0,1]$, le problème d'optimisation associ\'e \`a une SVDD s'écrit ainsi~:\\ ,, )} ^2 + C_{k}^{} _k\\ \\ ~||(_k) - ||^2_{F} ^2 + _k, _k 0 k \{1,,K\} . où $||.||_{F}$ désigne la norme 2 dans $F$.\\ Le Lagrangien associ\'e \`a est d\'efini par~: (,, ,, ) &=& ^2 + C _{k}^{} _k - _{k}^{} _k -_{k}^{} _k _k \\ & =& ^2(1-_{k}^{}_k) + _{k}^{} (C-_k-_k) _k + _{k}^{} _k ||(_k)-||^2_{F} où $_k$ et $_k$ sont les multiplicateurs de Lagrange tels que, pour tout $ k \{1, , K\}$, $_k 0$ et $_k 0$. Les points critiques v\'erifient $}{ } = 0$, $}{ } = 0$ et $}{ _k} = 0$, c'est-\`a-dire 2 _{k}^{} _k ( - (_k)) = 0 = _{k} _k (_k) \\ 2 (1-_{k}^{} _k ) = 0 _{k}^{} _k = 1 \\ C - _k - _k = 0 _k = C - _k k \{1,,K\} \'Etant données les positivités de $_k$ et $_k$, on peut éliminer la dépendance en $_k$ en imposant l'encadrement $0 _k C$. À partir des deux premières égalités du système que l'on injecte dans le lagrangien ci-dessus, on obtient la formulation dite duale du problème que l'on va maximiser et qui ne dépend plus que de $_k$: ( ) = _{k}^{} _k (_k).(_k) - _{k,k'}^{}_k _{k'} (_k).(_{k'}) \\ sc. 0 _k C _{k}^{} _k =1 \,. Ceci est à nouveau un problème d'optimisation quadratique sous-contraintes. D'après les conditions de complémentarité de Karush-Kuhn-Tucker (Remarque 7.3, p. 198 ), la solution optimale vérifie pour tout $k \{1,,K\}$: _k^{*} = 0 \\ ^{*}_k ^{*}_k = 0 \,. Les vecteurs de support (SV) qui sont l'ensemble des points qui satisfont $_k^{*}> 0$ définissent à eux seuls le centre de la sphère. Géométriquement, il s'agit des points qui sont exactement à la surface de la sphère optimale ($0<_k^{*}< C, ^{*}_k=0$) ou à l'extérieur grâce aux variables de tolérance ($_k^{*} = C, ^{*}_k>0$). Seuls les vecteurs support interviennent dans la définition du centre: $$^* = _{_k SV} ^*_k (_k)$$ Toutefois ce centre n'a pas besoin d'être explicitement calculé. Pour $ ^n$ et $()$ son plongement dans $F$, la distance entre $()$ et le centre s'exprime uniquement par des produits scalaires avec les vecteurs de support: D^2_{}(,):=||()-||^2_{F}= ().()-2_{_k SV}^{} _k^{*} ().(_k) + _{_k, _{k'} SV} _k^{*}_{k'}^{*} (_k).(_{k'}) Afin d'établir un critère d'anomalie, il reste à calculer le rayon de la sphère à partir de l'équation pour un vecteur de support $^<$ strict ($_k^{*}< C$) sur la sphère optimale ${^*}^2 = D^2_{}(^<,)$. Le score d'anomalie associé à la SVDD est: () = D^2_{}(,^*) - {^*}^2 Ainsi, le score est positif pour les points en dehors de l'hypersphère, et seront donc considérés comme une anomalie.\\ Avec un effort minimal, ce modèle peut prendre en compte des anomalies connues en imposant que celles-ci soient à l'extérieur de la sphère avec une certaine tolérance. Pour résoudre le problème quadratique à partir de sa formulation duale, il est nécessaire, dans un premier temps, de réécrire le problème sous forme matricielle, afin de procéder ensuite à l'optimisation à l'aide de la librairie .\\ En notant $ = (_1, , _K)^T$, $0_K = (0, 0, ..., 0)^T$, $1_K = (1, 1, ..., 1)^T$ et $$ la matrice de Gram dont le terme général est $_{ij}= (_i).(_j)$, }{} ^t - ^t ()\\ \\ galit: } 1_K^t =1 \\ galit: } -Id_K \\ Id_K \\ 0_K \\ C~1_K En général, les $_k$ étant linéairement indépendants, la matrice de Gram est par construction définie positive. La fonction objective à minimiser est dans ce cas strictement convexe. Les contraintes forment de plus un ensemble convexe fermé. On en conclut à l'existence d'une solution unique. Une façon d'adapter la SVDD au contexte des réseaux de neurones et de reproduire la méthodologie qui a permis de passer de la "One Class SVM" (voir p. ) à la "Deep One Class SVM" (voir p. ): les contraintes sur les variables { slack} sont intégrées dans la fonction de perte charnière ({ hinge loss}) . Le réseau de neurones dont les paramètres sont notés $W$ définit explicitement le plongement $_W: ^n F ^m$, espace dans lequel est calculé la SVDD (cf. figure ). La fonction objective est donc _{, W} ^2 + { K} _k (0, ||_W(_k) - ||^2_F - ^2) + {2} ||W||^2 Le plongement $_W$ sera pénalisé pour tout point $_W(_k)$ à l'extérieur de la sphère. Le réseau de neurones est régularisé par une pénalité $L_2$ afin de rendre le plongement dans $F$ plus robuste. On remarque que le centre ne fait pas partie de l'optimisation afin d'éviter une solution triviale. En effet, si on laisse $$ libre, la sphère va s'effondrer sur le cas dégénéré $=_0$, $=0$ où $_0$ est la sortie du réseau pour $W=0$ (tous les poids du réseau sont nuls). Le centre $$ doit donc être fixé et différent de $_0$.\\ Les auteurs démontrent que deux conditions supplémentaires sont nécessaires pour éviter cet effondrement(appelé également "collapse"). Il n'existe pas de terme de biais apprenable dans les couches cachées; le réseau serait capable de trouver un $W^*$ tel que $_W() = $ pour tout $$ et $^*=0$. Les fonctions d'activations ne doivent pas être bornées. Cette condition découle de la précédente car si une fonction d'activation sature un neurone en dehors de 0 quelque soit son entrée, ce neurone pourra jouer le rôle d'un terme de biais dans la couche suivante. Il est donc possible d'utiliser l'activation mais pas l'activation sigmoïde. Le score d'anomalie pour cette méthode est: $$() = ||_{W^*}() - ||^2_F - {^*}^2$$ Afin de faciliter l'interprétation et la comparaison entre modèles, on peut adimensionner ce score en le divisant par ${^*}^2$. [htbp] {0.9} {0.33} {60} in {1,...,15} { {rand*360} { rand*1.5} (:) circle (1pt); } in {1,...,} { {rand*360} {rand*1.5} (:) circle (1pt); } {0.33} [shorten >=1pt,->,draw=black!50, node distance=2cm] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0.5,--0.5) {$x_{}$}; / in {1,...,3} (J-) at (1.5,-) {$h_{}$}; / in {1,...,3} (K-1) edge (J-); / in {1,...,3} (K-2) edge (J-); [decorate,decoration={brace,amplitude=10pt},yshift=-0.5cm] (-2,0) -- (2,0) node [black,midway,yshift=0.6cm] {$_W()$}; {0.33} (0,0) circle (0.5*); {60} in {1,...,15} { {rand*360} { + rand*0.5} (:) circle (1pt); } in {1,...,} { {rand*360} {rand*0.5} (:) circle (1pt); } )$ et SVDD} Si l'on est dans le cas où la (quasi-)totalité des données sont normales, les mêmes auteurs ont proposé une version simplifiée du précédent algorithme appelée "One Class Deep SVDD" pour laquelle la notion de sphère disparait en prenant $ = 0$. Par extension, on peut dire que tous les points sont considérés comme à l'extérieur de la sphère et on cherche de minimiser la variance des données normales autour d'un centre $ F$: _W {K} _k ||_W(_k) - ||^2_F + {2} ||W||^2 En cherchant la transformation $_W$ qui contracte les données normales vers $$, on espère que les anomalies en seront éloignées. Il est évident que le réseau doit être suffisamment régularisé pour maintenir une "certaine" quantité de variance et éviter l'effondrement sur $$. Comme précédemment, il convient de fixer $$ différent de $_0$ obtenu pour $W=0$. Nos tests ont montré qu'il est possible d'utiliser des biais apprenables et des activations sur toutes les couches hormis la dernière. \\ Le score d'anomalie pour cette méthode est simplement la distance à $$: $$() = ||_{W^*}() - ||^2_F$$ Cependant, en pratique, il est nécessaire de fixer un seuil à ce score. Pour cela, nous avons utilisé la méthode de qui détermine un seuil "optimal" en maximisant le score AUC-ROC (décrit dans la section ). Ce post-traitement appliqué au score d'anomalie pour chacune des méthodes décrites ne garantit pas que la frontière de décision de l'hypersphère (généralement le seuil est à 0) coïncide avec le seuil optimal trouvé.\\ Une amélioration a été proposée dans afin de préserver suffisamment d'information et de structure. L'idée est de pré-entraîner un auto-encodeur pour lequel $_{W}$ serait l'encodeur. Le vecteur $$ serait alors défini comme la sortie moyenne de $_{W}$. La poursuite de l'entraînement consiste à minimise la perte auquel on a rajouté l'erreur de reconstruction de l'auto-encodeur en norme $L_2$. Les fonctions à base radiale sont des fonctions dont la valeur dépend de la distance par rapport au centre $ ^n$ de la fonction. Une fonction à base radiale $()$ est de la forme~: (, \{, \}) = (||-||) où $ ^n$ est le vecteur d'entrée, $$ est le centre de la fonction, $$ un paramètre de contrôle sur la largeur de la fonction et $$ une fonction donn\'ee. On considérera $||||$ comme la norme euclidienne. Une des fonctions à base radiale les plus couramment utilisées est la fonction Gaussienne, définie par~: (, \{, \}) = e^{--||^2}{2^2}} Les fonctions à base radiale sont souvent utilisées, tr\`es efficacement, dans le cadre de méthodes d'approximation ou de classification. Elles servent de fonctions de base pour représenter des fonctions plus complexes (cf. approximation universelle par les fonctions à base radiale dans le chapitre précédent). Elles sont aussi structurellement assez proches des fonctions de sortie hypersph\'eriques. On d\'etaille donc la m\'ethode Deep Radial Basis Function Data Descriptor (D-RBFDD) pour que le lecteur la différencie bien des outils que nous introduirons par la suite. La sortie d'une couche RBF s'écrit~: (, \{w_j, _j, _j\}) = _{j=1}^J w_j ( _j|| -_j|| ) . où $J$ est le nombre de fonctions radiales. Dans la littérature, les points centraux $_j$ des fonctions radiales sont initialisés par clustering et les facteurs d'échelle fixés à 1.\\ Le principe de la méthode RBFDD est d'utiliser une couche RBF puis une activation $g$ pour régresser la valeur de normalité de $1$. Afin d'éviter une activation qui sature pour la valeur $1$, les auteurs utilisent la fonction recommandée dans , à savoir $1.7159~(2x/3)$. Dès lors qu'un module $_W$ d'extraction de caractéristiques est mis en amont de la couche RBF, les auteurs appellent ce modèle Deep RBFDD. La fonction de coût à minimiser est: L(\{_k\}, W, \{w_j, _j, _j\}) = {2K} _k ^2+{2}||W||^2_2 Comme précédemment, un seuil automatique est calculé en sortie de l'activation $g$ par la procédure automatique liée au critère AUC-ROC (cf. ). Les anomalies seront typiquement les données pour lesquelles la sortie est inférieure à ce seuil. Les couches hypersphériques sont définies dans le cadre de l'algèbre géométrique conforme, permettant de représenter des points et des hypersphères dans un espace étendu. Les méthodes D-RBFDD ou Deep SVDD et ses variantes décrites précédemment, sont adaptées pour utiliser des couches hypersphériques et effectuer une optimisation. Dans cette section sont d\'efinis tous les algorithmes que nous proposons, en s'appuyant sur leur construction \`a partir de probl\`emes d'optimisation sous contraintes et en d\'etaillant le fonctionnement des fonctions de co\^ut dirigeant l'apprentissage. Cela donne des premiers \'el\'ements de comparaison avec les m\'ethodes Deep SVDD. L'analyse des propri\'et\'es des algorithmes propos\'es et les exp\'erimentations num\'eriques seront d\'evelopp\'ees aux paragraphes et . L'espace des caract\'eristiques $F$ dans lequel sont projet\'ees les donn\'ees est suppos\'e de dimension $m$. Une hypersph\`ere dans $^m$ de centre $=(c_1,,c_m)$ et de rayon $$ correspond \`a $ $, dont les $m+2$ coordonn\'ees sont $(1,, ( ^2 - ^2)/2)$, avec $ ^2=_{i=1}^m c_i^2$, dans l'alg\`ebre conforme $(m+1,1)$. Dans toute la suite, pour \'eviter la confusion avec le produit scalaire, le produit interne de l'alg\`ebre conforme est not\'e $_i$. On rappelle en particulier que pour tout point $ =(x_0,,x_)$ de $(m+1,1)$, on a~:\\ $$ _i = - {2} ( - ^2 -^2 ) .$$ On note en particulier que seules les coordonn\'ees de $$ correspondant aux coordonn\'ees dans l'espace des caract\'eristiques sont utilis\'ees. Cette approche s'inspire du principe que la méthode RBFDD décrite précédemment. Le réseau de neurones est constitué d'une seule couche composée de plusieurs hypersphères. La sortie du réseau correspond à une somme pondérée de sigmoïdes (fonctions $g$) appliquées au produits conformes entre les hypersph\`eres et les points testés. (, \{ w_j, _j \}) = _{j=1}^{J} w_j g(_j _i ) La figure illustre deux points principaux: Elle approxime une fonction porte à bords lisses sur l'intervalle $[-, ]$. Quelque soit la valeur de $$, son maximum qui est atteint pour $=$ appartient à l'intervalle $]0.5, 1[$. [H] _i $ pour $=0$} Le réseau SPH Anomaly cherche par une régression au sens des moindes carrés à ajuster les sphères pour obtenir une sortie à 1:\\ (\{_k\}, \{w_j, _j\}) = {2K} _k (1- (_k, \{w_j, _j\}) )^2 \\ \'Etant donnée une hypersphère de rayon $$ "suffisant" grand ($>3$), la sortie $g(_j _i )$ pour les points $$ situés à une distance inférieure à $$ de $$ est proche de 1. On appellera Deep SPH Anomaly, un réseau de neurones tel que SPH Anomaly suit un extracteur de caractéristiques. Dans nos observations, nous avons constaté qu'il est préférable que ce dernier soit pré-entrainé (via un auto-encodeur). {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0,-+1/2) {$x_{}$}; (K-3) at (0,-3*+1/2) {$1$}; (K-4) at (0,-4*+1/2) {${2}$}; / in {1,...,3} (J-) at (0,-) {$_{}_i $}; / in {1,...,3} (K-1) edge (J-); / in {1,...,3} (K-2) edge (J-); / in {1,...,3} (K-3) edge[dashed,->,red] (J-); / in {1,...,3} (K-4) edge[dashed,->,red] (J-); (I) at (0.5cm,-2*) {$ w_{j=1}^3 g(_j _i _k)$}; in {1,...,3} (J-) edge (I); L'idée de la méthode SPH SVDD est de reformuler le problème la SVDD (cf. équation ) en utilisant une hypersphère dans l'espace géométrique conforme. En d'autres termes, utiliser une couche hypersphérique pour résoudre le problème d'optimisation en utilisant l'algèbre géométrique conforme (cf. figure ). Comme ce sont les paramètres d'une hypersphère d'encadrement qui sont recherchés, le réseau contient une couche hypersphérique à un seul neurone (une sphère) et les entrées $ ^m$ sont d'abord plongés dans l'espace conforme pour donner $ (m+1, 1)$.\\ [h!] {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!120]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0,-+1/2) {$x_{}$}; (K-3) at (0,-3+1/2) {$1$}; (K-4) at (0,-4+1/2) {${2}$}; / in {0} (J-) at (0,-1.75) {$y_i = _i $}; / in {0} (K-1) edge (J-); / in {0} (K-2) edge (J-); / in {0} (K-3) edge[dashed,->,red] (J-); / in {0} (K-4) edge[dashed,->,red] (J-); Par conséquent, la sortie du réseau est le produit conforme entre un point $ (m+1, 1)$ et une hypersphère $ (m+1, 1)$ de centre $$ et de rayon $$. Rappelons que ce produit est\\ _i = -{2} \\ On rappelle que le problème d'optimisation pour la SVDD se formule comme\\ {} & ^2 + { K}_{k=1}^{K} _k\\ \\ & ||_k - ||^2-^2 _k, _k 0, k \{1,,K\} \\ peut se reformuler à l'aide du produit conforme ($||_k - ||^2-^2 = - 2~ _i _k$) comme\\ ,)}{} & ^2 + { K}_{k=1}^{K} _k\\ \\ & -2_i_k _k, _k 0, k \{1,,K\} . \\ En utilisant une fonction de coût charnière, on obtient une formulation sur une ligne de SPH SVDD comme\\ }{}\, ^2 + { K} _{k=1}^{K} \{0, -2 _i _k \} \\ où $K$ est le nombre total de points dans l'ensemble des données. On peut préciser que $^2$ peut également s'écrire $^2$ (produit géométrique) ou encore $ _i $ car $ = 0$.\\ La $$-propriété de la SVDD qui permet de contrôler la proportion des points à l'intérieur de la sphère est perdue ici. L'hyper-paramètre $$ reste toutefois un moyen de contrôler la tolérance du modèle à l'erreur. En pratique, différents codes de la littérature montrent que la $$-propriété peut-être rétablie après optimisation en modifiant le rayon pour couvrir $100~\ Si les données ne peuvent pas être séparées par une hypersph\`ere, une ou plusieurs couches cachées (extraction de caractéristiques) sont ajoutées au réseau en amont (cf. figure ). Les paramètres $W$ de ce module du réseau permettent caractériser la fonction de plongement $_W: ^n ^m$ dans l'espace des caractéristiques $F$. La sortie $_W()$ de $_W$ est ensuite plongée dans l'algèbre géométrique conforme $(m+1,1)$. La suite du réseau correspond à la partie SPH SVDD décrite plus haut, l'ensemble est appelé Deep SPH SVDD.\\ [h!] {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0,--2.5) {$x_{}$}; / in {1,...,5} (J-) at (0,--1) {$h_{}$}; / in {1,...,5} (K-1) edge (J-); / in {1,...,5} (K-2) edge (J-); / in {1,...,3} (L-) at (2.5,-2) {$_{W}(h)$}; / in {1,...,3} (J-1) edge (L-); / in {1,...,3} (J-2) edge (L-); / in {1,...,3} (J-3) edge (L-); / in {1,...,3} (J-4) edge (L-); / in {1,...,3} (J-5) edge (L-); (L-4) at (2.5,-7.75) {$1$}; (L-5) at (2.5,-9) {${2}$}; (I) at (2,-3.75) {$y_i = _i $}; in {1,...,3} (L-) edge (I); / in {4,...,5} (L-) edge[dashed,->,red] (I); La recherche des paramètres de l'hypersphère englobante est effectuée dans l'espace de caractéristiques en dimension $m$. La fonction de coût est relativement similaire à celle de la SPH SVDD puisque seul un terme de régularisation est rajouté: }{}\, ^2 + { K} _{k=1}^{K} \{0, -2 _i (_k) \} + {2} ||W||^2 Contrairement à la méthode Deep SVDD, tous les paramètres de l'hypersphère, y compris le centre, sont appris dans Deep SPH SVDD. Nous verrons par la suite la raison qui fait que cela est possible sans effondrement. On d\'efinit le score d'anomalie comme () = -2^* _i }(z). Ce score est si et seulement si $}(z)$ est dans la sphère $^*$. Cela indique que le réseau de la méthode Deep SPH SVDD produit directement $ ^* _i }(z)$ qui est un score de normalité. Comme nous l'avons déjà évoqué, un seuil automatique du score est calculé par la procédure automatique liée au critère AUC-ROC (cf. équation ). Cela revient à ajuster le rayon de la sphère trouvée.\\ Les étapes d'implémentation sont résumées à travers le pseudo-code suivant:\\ [ colback=gray!5, colframe=blue!75!black, fonttitle=, title={Deep SPH SVDD} ] [H] _1, , _K\}$\\ Hyperparamètres $ ]0, 1[$, $> 0$, $$, nombre maximal d'itérations $N_{}$, $$} $ et rayon $^*$ de l'hypersphère, fonction de détection d'anomalie} \\ \\ Initialiser aléatoirement les poids $W$ de $$ (méthode He)\; { Initialiser $$ avec $ = $ et $$ leur écart-type\; } $ avec $ (0,1)$ et $ = 1$\; } \\ 1$ $N_{}$}{ Mélanger aléatoirement $X$ \; { _k B$}{ Extraction de caractéristiques: $_k ^n _W (_k) ^m$\; Passage en conforme: $_W (_k) ^m (_k) (m+1,1)$\; Calculer le score d'anomalie $-2 _i (_k)$; } $ = ^2 + { |B|} _{x_k B} (0, -2 _i (_k))) + {2} ||W||_2^2$\; Mettre à jour $W$, $$ à partir de $$ avec la méthode ADAM\; } $}{ Arrêter l'étape 2\; } } $ = 0$\; \\ {un jeu de validation $V$ est disponible}{ _k V$}{ Calculer le score d'anomalie $-2^* _i }(_k)$; } Obtenir un seuil automatique $$ via l'équation ; } $$() := -2^* _i }() > $$ \\ Certains des \'el\'ements ci-dessous seront établis dans les paragraphes et . Nous pr\'ef\'erons les rassembler aussi ci-dessous pour le confort du lecteur. [H] {|p{0.9}|} {|c|}{} \\ .\\ L'hyper-paramètre $$ reste un coefficient de régularisation. Un taux de couverture peut être obtenu en aval par l'ajustement du rayon de la sphère sur un jeu de données de validation. \\ [H] {|p{0.9}|} {|c|}{} \\ .\\ Le nombre de paramètres entraînables est $m+1$ dans les deux cas. Pour cela, Deep SPH SVDD fixe le coefficient de $e_0$ à 1 pendant l'entraînement. \\ .\\ Par définition, les anomalies sont les points situés à l'extérieur de la sphère (score(z) $> 0$). En pratique, ce seuil peut être ajusté automatiquement sur un jeu de validation afin de maximiser la performance ROC. Si l'on considère que le centre de l'hypersphère est fixé par l'entraînement, cela correspond à en ajuster le rayon.\\ [H] {|p{0.435}|p{0.435}|} {|c|}{} & {c|}{} \\ Le centre $$ de l'hypersphère afin éviter la dégénerescence de celle-ci. Par exemple, on peut choisir $$ comme le barycentre des points ${(_k)}$ pour le premier batch. & Le centre $$ est libre, pas de dégénérescence observée. \\ Le rayon $$ est libre hormis pour la OC-Deep SVDD qui le fixe à $0$ (requiert une hypothèse de normalité). & Le rayon $$ est libre. \\ La sortie du réseau fournit la projection des données dans l'espace des caractéristiques \( F \). & La sortie du réseau donne directement le score de normalité. \\ Les termes de biais et les activations bornées sont interdites sous peine de dégénérescence de l'hypersphère & L'encodeur $$ peut contenir des termes de biais et utiliser des fonctions d'activation bornées.\\ Dans cette section, nous étendons la méthode Deep SPH SVDD au cas de plusieurs sphères. L'idée première est d'envelopper efficacement des groupes spécifiques de points normaux dans l'espace des caractéristiques par un groupe de sphères. Par ce choix, nous désirons améliorer la finesse de d\'etection et l'interprétabilité du modèle notamment dans le cas de données multimodales.\\ Les paramètres à optimiser sont toujours les poids $W$ de la fonction de projection $$ qui envoie les données vers l'espace des caract\'eristiques, ainsi que les centres $$ et les rayons $_j$ des hypersphères. Afin d'adapter la formulation au cadre multisph\'erique, deux questionnements doivent être tranchés: Comment définir une anomalie dans le cas de plusieurs hypersphères ? Comment calculer le volume des données normales couvertes par le modèle ? Pour répondre à la première question, rappelons qu'un point $$ est à l'intérieur (couvert) par la sphère $_j$ si et seulement si $-2_j _i < 0$. La règle de couverture la plus naturelle est de décréter qu'un point est couvert si et seulement si il existe au moins une sphère qui le couvre. Cela revient alors à attribuer à chaque point le score d'anomalie minimal parmi toutes les sphères: () = _j \{ -2_j _i \} et espérer qu'il soit négatif.\\ Ce choix est justifié par le fait que si un point est bien inclus dans au moins une sphère, alors son score d'anomalie sera faible. À l'inverse, un point qui est éloigné de toutes les sphères recevra un score élevé, reflétant son caractère atypique. Ainsi, cette définition permet d'assurer une cohérence entre la formulation du score d'anomalie et la règle de couverture adoptée pour la classification des données normales.\\ Dans toutes les méthodes présentées jusqu'à présent, la minimisation du volume d'une sphère qui englobe les données normales est obtenue en minimisant de manière auxiliaire le carré de son rayon. Dans le cas de plusieurs hypersphères, la minimisation du volume couvert nécessite de prendre en compte les relations spatiales entre elles. Par exemple, si deux sphères se chevauchent fortement le volume de leur union est supérieur à celui d'une unique sphère optimisée couvrant les mêmes points, ce qui peut être inefficace en termes de représentation des données normales. De plus, même s'il existe une formule analytique pour le volume de l'intersection entre deux sphères, gérer plus de deux sphères semble inextricable.\\ En conséquence, une stratégie naturelle consiste à imposer une contrainte d'exclusion mutuelle entre les hypersphères, garantissant ainsi qu'elles ne s'intersectent pas. Cette contrainte permet de s'assurer que chaque sphère capture une région distincte de l'espace, évitant ainsi les redondances et améliorant la séparation des groupes de données. De plus, en éliminant les intersections, on simplifie considérablement la modélisation du volume couvert, qui devient simplement la somme des volumes individuels des sphères (ou de manière auxiliaire la somme des carrés des rayons). Ce choix garantit ainsi une optimisation cohérente et contrôlable, en adéquation avec l'objectif de minimisation du volume global des données normales. On notera que cette question n'\'etait pas prise en compte dans l'approche avec la m\'ethode SPH Anomaly. Pour garantir la distinction et la non-intersection entre les sphères, on va introduire deux termes supplémentaires dans la fonction de co\^ut. Le théorème suivant de Hestenes et al. (voir théorème 2.6.1 in ) inspire la construction (adaptation des notations).\\ [colframe=blue, colback=white!10, title=Critère pour l'intersection de deux sphères] Deux sphères $_1$, $_2$ se coupent, sont tangents ou parallèles, ou ne se coupent pas, si et seulement si $(_1 _2)^2$ est inférieur, égal ou supérieur à $0$, respectivement. Ainsi le d\'eveloppement d'une expression de la forme $(_{1} _2)^2$, où $$ désigne le produit externe, permet d'obtenir une relation entre les centres et les rayons des sphères, dont le signe indique si deux sphères s'intersectent.\\ Par exemple en dimension 2, soit $_1$ et $_2$ sont deux hypersphères caract\'eris\'ees par les coordonn\'ees\\ {l} _1 = e_0 + x_1 e_1 + y_1 e_2 + (- {2} + {2} ) e_, \\ \\ _2 = e_0 + x_2 e_1 + y_2 e_2 + (- {2} + {2} ) e_, dont les centres sont donc $_1 = (x_1, y_1)$ et $_2 = (x_2, y_2)$ et les rayons $_1$ et $_2$. On calcule le scalaire: (_1 _2)^2 &= {4} (-_1^2 - 2_1_2 - _2^2 + x_1^2 - 2x_1x_2 + x_2^2 + y_1^2 - 2y_1y_2 + y_2^2) \\ \\ & (-_1^2 + 2_1_2 - _2^2 + x_1^2 - 2x_1x_2 + x_2^2 + y_1^2 - 2y_1y_2 + y_2^2) \\ \\ &= {4} \\ \\ & \\ \\ &= {4} \\ Il s'avère que ce d\'eveloppement se généralise en dimension quelconque~: \\ (_1 _2)^2 &= {4} (-_1^2 - 2_1_2 - _2^2 + _{i=1}^m (_{1i} - _{2i})^2) \\ \\ & (-_1^2 + 2_1_2 - _2^2 + _{i=1}^m (_{1i} - _{2i})^2) \\ \\ &= {4} \\ La figure illustre le signe de $(_1 _2)^2$ suivant la relation spatiale entre les deux sphères.\\ [htbp] {0.2} [thick, scale=0.6] (0,0) circle (1.5); (1.8,0) circle (1.5); _1 _2)^2 < 0$} {0.37} [thick, scale=0.6] (0,0) circle (1.5); (2.5,0) circle (1); (3.75,-1.5) -- (3.75,1.5); (5.5,0) circle (1.5); (6,0) circle (1); _1 _2)^2 = 0$} {0.37} [thick, scale=0.6] (0,0) circle (1.5); (2.5,0) circle (0.75); (3.65,-1.5) -- (3.65,1.5); (5.5,0) circle (1.5); (6,0) circle (0.75); _1 _2)^2 > 0$} _1 _2)^2$} On remarque plusieurs éléments: L'expression est bien entendu symétrique~: $ (_1 _2)^2 =(_2 _1)^2$ Une sphère est tangente à elle-même: $( )^2 = 0$ Si la sphère $_1$ est totalement incluse dans $_2$, $(_1 _2)^2 > 0$ Réduire l'intersection des sphères revient à pénaliser le terme $- (_1 _2)^2$ s'il est positif. Dans le cas de $J$ hypersphères, on est amené à considérer ce terme de pénalité pour chaque couple d'hypersphères et ainsi définir le coût:\\ oss_{}(_1, , _J) = _{j < j'} \{0, - (_j _{j'})^2 + \} \\ où $$ est une petite constante positive introduite pour \'eviter que deux hypersphères soient tangentes.\\ Cependant, comme indiqué dans la troisième remarque et le dernier cas de la figure , le signe de $(_1 _2)^2$ n'est pas suffisant pour garantir l'exclusion mutuelle. Pour cela, un terme de non-inclusion est rajouté pour favoriser la situation où\\ _1 _i _2 < 0 _2 _i _1 < 0 \\ Cela revient à faire en sorte que le centre de chaque sphère soit perçu comme une anomalie par l'autre. On imagine bien que ce critère contribue également à la non-intersection de deux sphères. Comme précédemment, on définit un terme global pour les $J$ sphères:\\ oss_{}(_1, , _J) = _{j, j' j} \{0, _j _i _{j'} + \} où $$ est une petite constante positive introduite pour éviter que les centres soient trop proches des bords des sphères. Conformément à l'objectif poursuivi, nous avons retiré le terme $j = j'$ dans la double sommation de l'équation . Toutefois, de manière amusante, on constate que\\ $\{0, _j _i _{j} + \} = _j^2/2 + $, ce qui conduit de minimiser $_j^2$ déjà présent dans la fonction de coût finale. Pour une raison de lisibilité et de lien entre les algorithmes, nous avons fait le choix de supprimer le terme $j=j'$ de la double sommation.\\ Finalement, en regroupant l'ensemble des consid\'erations pr\'ec\'edentes, la formulation du problème de la Deep M-SPH SVDD s'écrit comme\\ ,, , W}{} \{ & _{j} ^2 + { K} _{k} \{ 0, {}\{ -2_j _i _W(_k) \} \} \\ \\ & + oss_{}(_1, , _J) + oss_{}(_1, , _J) + {2} ||W||^2 \} \\ Pour mieux illustrer les étapes d'implémentation de la méthode Deep SPH SVDD, la représentation algorithmique ci-dessous, sous forme de pseudo-code, synthétise les étapes essentielles de sa mise en œuvre.\\ [ colback=gray!5, colframe=blue!75!black, fonttitle=, title={Deep M-SPH SVDD} ] [H] _1, , _K\}$, nb. hypersphères $J$\\ Hyperparamètres: $ ]0, 1[$, $> 0$, $$, nombre max. d'itérations $N_{}$, $$} $ et rayons $_j^*$ des hypersphères, fonction de détection d'anomalie} \\ \\ Initialiser aléatoirement les poids $W$ de $$ (méthode He)\; { Clustering des $\{_W(x) | x X\}$ en $J$ groupes\; Initialiser les $_j$ à partir des centres $_j$ et des écart-types $_j$ des clusters\; } _j$ avec $_j (0,1)$; $_j$ = 1\; } \\ 1$ $N_{}$}{ Mélanger aléatoirement $X$ \; { _k B$}{ Extraction de caractéristiques: $_k ^n _W (_k) ^m$\; Passage en conforme: $_W (_k) ^m (_k) (m+1,1)$\; _j$}{Calculer les scores d'anomalie $-2_j _i (_k)$;} } $_{} = 0$; $_{} = 0$ \; ^2, j < j'$} { $_{} = _{} + \{0, - (_j _{j'})^2 + \}$\; $_{} = _{} + \{0, _j _i _{j'} + \} + \{0, _{j'} _i _{j} + \}$\; } $ = _j _j^2 + { |B|} _{x_k B} _j \{ -2_j _i _{W}(_k)\} + _{} + _{} + {2} ||W||_2^2$\; Mettre à jour $W, _1, , _J$ à partir de $$ avec la méthode ADAM\; } $}{ Arrêter l'étape 2\; } } $ = 0$\; \\ {un jeu de validation $V$ est disponible}{ _k V$} { Calculer le score d'anomalie $_j \{ -2^*_j _i _{W^*}(_k)\}$ } Obtenir un seuil automatique $$ via l'équation ; } $$() := _j \{ -2^*_j _i _{W^*}(z) \} > $$ } La méthode Deep M-SPH SVDD généralise Deep SPH SVDD, car lorsque le nombre de sphères est fixé à $J=1$, les deux algorithmes deviennent équivalents. L'algorithme le plus proche de Deep M-SPH SVDD est la variante Deep Multi-Sphere SVDD (DMSVDD) développée par Ghafoori et Leckie dans . Avant de comparer point par point les deux algorithmes, donnons la fonction de coût suivi par DMSVDD: _{W, } {J} _j ^2_j + { K} _k (0, ||_W(_k) - c(_W(_k))||^2 - (_W(_k))^2) + {2} ||W||^2_2 où $c(_W(_k))$ (resp. $(_W(_k))$) désigne le centre (resp. le rayon) du centre le plus proche de $(_W(_k)$ dans $F$. Le score d'anomalie ainsi construit est clairement identique au nôtre dans l'équation (). Pour faciliter la comparaison, diff\'erents \'el\'ements sont list\'es dans l'encadr\'e ci-dessous. [H] {|p{0.48}|p{0.48}|} & } \\ $k$-means dans l’espace des caractéristiques pour initialiser les centres et les rayons des sphères. & $k$-means dans l’espace des caractéristiques pour fixer les centres.\\ Rétropropagation conjointe des paramètres de l’encodeur et des sphères via Adam. & Optimisation alternée : mise à jour séparée des rayons et des paramètres de l’encodeur. \\ Centres ajustés dynamiquement pendant l’entraînement via rétropropagation. & Fixés après l’initialisation, restent constants tout au long de l’optimisation. \\ Rayons ajustés dynamiquement pendant l’entraînement via rétropropagation. & Déterminés empiriquement comme les $(1-)$-quantiles des distances aux centres. \\ La $$-propriété n'est pas respectée lors de l'optimisation. & La $$-propriété forcée par le choix empirique des rayons. \\ Introduit un coût d’intersection et de non-inclusion pour éviter le recouvrement excessif des sphères. & Les sphères peuvent s'intersecter; la somme des carrés des rayons ne mesure donc pas le volume de données couvert.\\ Le nombre de sphères $J$ est fixé. Les sphères peuvent avoir un rayon nul lors de l'optimisation & Ajusté dynamiquement en supprimant les sphères avec un effectif trop faible. \\ Le score d'anomalie est celui de la distance à la plus proche sphère. & Le score d'anomalie est celui de la distance à la plus proche sphère. \\ Au final, on voit que Deep M-SPH SVDD adopte une approche plus flexible grâce à une optimisation conjointe qui permet une adaptation fine des sphères aux données et qui modélise explicitement les relations géométriques entre les sphères. L'approche de DMSVDD est plus rigide avec des centres fixés et des rayons "optimisés" pour la $$-propriété. Cela favorise sans doute une répartition stable des sphères, mais la répartition des données dans les sphères dépend largement de la régularité de $$. Dans les modèles classiques de Deep SVDD (ex. One-Class Deep SVDD), l'encodeur est associé à un centre $c$ et un rayon $$ fixés. Dans les modèles Deep SPH SVDD proposés, \`a une ou plusieurs sph\`eres, les paramètres caract\'eristiques des sphères, rayon et centre, sont libres \`a l'initialisation puis appris. C'est un int\'er\^et de ces nouvelles m\'ethodes. C'est \'egalement surprenant au regard du contexte classique. En effet, il est connu (voir ) que si les centre et rayon des sph\`eres sont laiss\'es libres dans les algorithmes classiques de Deep SVD, alors ceux-ci convergent vers la solution triviale $(W_0, , 0)$, o\`u $W_0$ d\'esigne le r\'eseau dont tous les poids sont nuls. En particulier, le rayon de la sph\`ere limite est nul, elle est d\'eg\'en\'er\'ee, r\'eduite \`a un point not\'e $c_0$ : c'est le phénomène de collapse. Les algorithmes Deep SPH SVDD propos\'es ont pu appara\^ tre jusqu'ici comme des r\'e-\'ecritures d'algorithmes classiques profitant du formalisme de l'alg\`ebre conforme. Il s'av\`ere qu'ils ont un comportement tr\`es diff\'erent vis \`a vis du risque de collapse. On a constat\'e apr\`es de nombreuses exp\'erimentations que, bien que les centres et les rayons soient laiss\'es libres, les rayons ne tendent pas toujours vers z\'ero. Le pr\'esent paragraphe est donc consacr\'e \`a ce comportement { a priori} \'etonnant. On introduit quelques notations pour uniformiser le raisonnement entre les diff\'erentes m\'ethodes pr\'esent\'ees dans ce manuscrit combinant r\'eseau de neurones et SVDD. Soit d'abord $$ _W \, : \, ^n F ^m.$$ la fonction associ\'ee au r\'eseau $W$ qui effectue le plongement des donn\'ees de $^{n}$ à un espace de caractéristiques $F$. Soit $W_0$ le r\'eseau dont tous les poids sont nuls (avec les notations du manuscrit, $^l = 0$ pour tout $^l W_0$). La fonction $_{W_0}$ associ\'ee est donc constante. Soit $ ^m$ cette constante, $\{ \} = _{W_0}(^n)$. On suppose qu'on dispose de $K$ donn\'ees not\'ees $_k$ pour $1 k K$. Toutes les m\'ethodes m\^elant apprentissage et SVDD pr\'esent\'ees ont une structure commune dans la fonction de perte associ\'ee. On peut les \'ecrire sous la forme d'une somme de fonctions de la forme J (W,, ) = ^2 + { K} _{ k}^{} ||_W(_k) - ||^2 _{F} + (_W) + (,) avec && 0, \ 0,\ 0, \ 0, \\ && \, : \, ^{m+1} _+ (,0) = 0 \ ^m. Dans la suite, on va supposer pour all\'eger les notations qu'on ne cherche qu'une sph\`ere englobant les donn\'ees, si bien que la fonction de perte est exactement donn\'ee par . Si les param\`etres \`a apprendre sont les param\`etres du r\'eseau, le rayon et le centre de la sph\`ere, le but de l'algorithme est donc de d\'eterminer (W^*,^*,^*) = _{(W,,)} J (W,, ) , que le centre et le rayon soient appris directement ou appris { via} l'hypersph\`ere conforme $^* = (1, c_1^*,,c_m^*, ( ^* ^2 - (^*)^2)/2)$. On \'enonce le r\'esultat suivant. ^*,^*)$ du probl\`eme est } $$ W^*= W_0, ^* = _0, ^* = 0.$$ On v\'erifie facilement que $J(W_0,_0,0)=0$. Comme la fonction $J$ est positive, $(W_0,_0,0)$ r\'ealise n\'ecessairement le minimum de $J$ est est une solution du probl\`eme . S'il existe une autre solution $(W^*,^*,^*)$ au probl\`eme , elle v\'erifie n\'ecessairement aussi $J(W^*,^*,^*)=0$. Chaque terme dans la somme d\'efinissant $J$ \'etant une fonction \`a valeur positive, cela implique que $$ (^*)^2 =0 ,\ (_{W^*}) =0,\ (^*,^*) =0 , \ ||_{W^*}(_k) - ^*||^2 _{F} =0 \ 1 k K.$$ La premi\`ere relation implique $^*=0$, la seconde $W^*=W_0$, si bien que la quatri\`eme implique alors $^*=_{W^*}(_k) = _{W_0}(_k) =_0$. $$ Un tel r\'esultat a d\'ej\`a \'et\'e d\'emontr\'e par les auteurs de pour le cadre Deep SVDD. Ils en concluent que l'algorithme en question risque de converger vers cette solution d\'eg\'en\'er\'ee. Pour l'\'eviter ils pr\'econisent de fixer rayon et centre et de ne plus chercher qu'\`a apprendre les poids du r\'eseau $W$ : pr\'ecis\'ement, pour $ 0$ donn\'e, pour $ _0$ donn\'e, r\'esoudre $$W^* = _{W} J (W,, ).$$ C'est un appauvrissement du mod\`ele dont il est cependant bien connu qu'il est n\'ecessaire~: l'algorithme Deep SVDD a tendance \`a collapser lorsque le centre et le rayon ne sont pas fix\'es. Pour autant, comme on va l'illustrer dans le paragraphe suivant, les algorithmes Deep SPH SVDD, bien que soumis aussi au r\'esultat de la Proposition ci-dessus, ne conduisent pas au collapse. Le fait que les Deep SPH SVDD ne soient pas sensibles au collapse va entra\^iner une cascade de bonnes propri\'et\'es. L'article mentionne aussi le fait que le r\'eseau associ\'e \`a la m\'ethode Deep SVDD ne doit pas contenir de biais. D'abord, en toute rigueur, ce n'est vrai que si la fonction de perte ne contient pas le terme de régularisation $(_W)$ (voir la d\'emonstration de la Prop. 2 dans ). Ensuite, et surtout, c'est une cons\'equence du fait que le centre est fix\'e pour la Deep SVDD. Les Deep SPH SVDD qui ne collapsent pas quand le centre est laiss\'e libre peuvent donc s'appuyer sur un r\'eseau dont l'architecture est enrichie par des biais. D\`es lors (voir Prop. 3 dans ), les Deep SPH SVDD ne sont pas limit\'es \`a l'utilisation de fonctions d'activation non born\'ees. Le but de ce paragraphe est de comprendre pourquoi comment l'approche Deep SPH SVDD se comporte par rapport au phénomène de collapse ({ i.e.} ne produit pas une sph\`ere d\'eg\'en\'er\'ee en un point) et pourquoi il n'est pas necessaire comme dans le cas des methodes Deep SVDD et ses variantes de laisser libres le rayon et le centre de la sph\`ere calcul\'ee. Afin de simplifier les calculs, nous allons faire le raisonnement dans un cas extr\^emement simple. Supposons donc que nous n'avons qu'une seule donn\'ee, not\'ee $$. On suppose aussi qu'il n'y a qu'une couche et pas de fonction d'activation dans les r\'eseaux impliqu\'es. On peut facilement v\'erifier que tout ce qui est dit ci-dessous s'adapte \`a des configurations plus r\'ealistes, cela rend simplement les notations plus lourdes. Pour comparer m\'ethodes Deep SVDD et Deep SPH SVDD, on introduit deux fonctions de perte caract\'erisant respectivement ces deux m\'ethodes tout en se limitant \`a ce qui les diff\'erencie vraiment. Les probl\`emes associ\'es sont~:\\ && (W^*,^*,^*) = J_{cl}(W,,), J_{cl}(W,,) = ^2 + _W() - ^2, \\ && (W^*,^*) = J_{sph}(W,) , \\ \\ && J_{sph}(W,) = {} _W() - ^2 + {} ( _{i=1}^m c_i^2 - 2 s_{m+1} ) , \ = (1, c_1,,c_m, s_{m+1}). \\ En effet, les coordonn\'ees d'une sph\`ere $$ dans l'espace conforme sont $$ = (1, c_1,, c_m,(\|\|^2 - ^2)/2)$$ o\`u $(c_1,,c_m)$ sont les coordonn\'ees du centre dans $^m$ et $$ est le rayon de la sph\`ere. Dans l'algorithme Deep SPH SVDD les paramètres de l'hypersph\`ere sont donc intégrés dans un vecteur $$ qui encapsule à la fois le centre et une expression du rayon. On va voir que le couplage induit par cette structuration des inconnues a une influence sur la convergence de l'algorithme et explique l'absence de collapse observ\'e avec Deep SPH SVDD. Pour reproduire l'apprentissage, on d\'ecrit la r\'esolution des probl\`emes et par descente de gradient. On construit donc une suite $(y_n)_{n }$ selon l'algorithme suivant\\ \{ {l} y_0 ,\\ y_{n+1} = y_n - J(y_n) \ n 0, . \\ o\`u $ >0$ est le taux d'apprentissage et :\\ -- pour le probl\`eme , $J=J_{cl}$, $y ^{ W + m +1}$ contient les inconnues et peut s'\'ecrire $y=(W,,)$ ; $ J = (_W J_{cl}, _{} J_{cl}, _{} J_{cl})$ ; \\ -- pour le probl\`eme , $J=J_{sph}$, $y ^{ W + m +1}$ contient les inconnues et peut s'\'ecrire $y=(W,,s_{m+1} )$ ; $ J = (_W J_{sph}, _{} J_{sph}, _{s_{m+1}} J_{sph})$.\\ Le r\'eseau n'ayant qu'une couche cach\'ee et pas de fonction d'activation, on consid\`ere que $_W(x)=Wx$ et on calcule facilement les gradients~:\\ && _W J_{cl}(W,,) = 2 (_W()-) , \\ && _{} J_{cl}(W,,) = - 2 (_W()-) ,\\ && _{} J_{cl} (W,,) = 2 \, ; \\ && _W J_{sph}(W,) = {} (_W()-) , \\ && _{} J_{sph}(W,) = - {} (_W()-) +2 {} = - {} ( _W() - ) , \\ && _{s_{m+1}} J_{sph} ( W,) = -2 {} . \\ Dans une descente de gradient telle que , la vitesse de la convergence est contr\^ol\'ee par la norme du gradient $ J$. Ici, la solution du problème est connue : $W^*=W_0$, $^*=(W_0)()=_0$, $^*=0$ (voir paragraphe ), c'est-\`a-dire $^*=(1,_0, _0^2/2)$. Or, gr\^ace aux calculs des gradients ci-dessus, on remarque que $$ _{(W,,) (W_0,_0,0 )} J_{cl}(W,,) =0$$ $$ _{(W,) (W_0,1,_0, _0^2/2 )} _W J_{sph}(W,) =0$$ mais _{(W,) (W_0,1,_0, _0^2/2 )} _{} J_{sph}(W,) = {} _0 0 _0 0_{^m}, \\ _{(W,) (W_0,1,_0, _0^2/2 )} _{s_{m+1}} J_{sph} ( W,) = {} 0 car le travail de d\'etection d'anomalie n'a de sens que si $ <1$.\\ Ces limites montrent : Pendant que $y_n$ construit par et converge vers la solution $ (W_0,_0,0 )$, le gradient $ J(y_n)$ qui dirige la descente tend en norme vers z\'ero : les pas sont donc de plus en plus petits, \'evitant ainsi des oscillations autour de la solution. \`A l'inverse, si $y_n$ est construit par et , seule la partie du gradient li\'ee \`a la convergence des param\`etres $W$ du r\'eseau converge en norme vers z\'ero ; Deep SVDD et Deep SPH SVDD traitent le m\^eme probl\`eme de minimisation mais Deep SPH SVDD est bas\'e sur une formulation non strictement convexe du probl\`eme~; En pratique, dans le cas de Deep SPH SVDD, les bonnes propri\'et\'es de convergence de la partie r\'eseau, de $W$, permettent au terme $ _W()-$ de d\'ecro\^ tre et de se stabiliser rapidement, tandis que l'instabilit\'e par rapport \`a $s_{m+1}$ fait osciller le couple centre-rayon (surtout le rayon qui n'est pas stabilis\'e par $W$), \'evitant ainsi d'atteindre la solution optimale tout en convergeant vers cette dernière. Le caract\`ere oscillant de $s_{m+1}$ ne peut pas \^etre compens\'e par une diminution du taux d'apprentissage (on ne peut pas choisir un taux inf\'erieur \`a la constante de Lipschitz des gradients par exemple, puisque cette constante est nulle pour une fonction constante). C'est le fait d'avoir transform\'e la formulation du probl\`eme sous la forme d'une minimisation non convexe qui permet d'apprendre le centre et le rayon. Ces consid\'erations th\'eoriques sont illustr\'ees dans le paragraphe ci-dessous. Les exp\'erimentations suivantes sont r\'ealis\'ees avec le jeu de donn\'ees { `easy'} d\'ecrit au paragraphe On peut v\'erifier que sur ce jeu de donn\'ees, un algorithme Deep SVDD o\`u le centre et le rayon sont appris conduit au collapse, l'hypersph\`ere calcul\'ee \'etant de rayon nul et centr\'ee en $_0= _{W_0}()$ o\`u $W_0$ est le r\'eseau dont tous les poids sont nuls. Le collapse est illustr\'e en annexe \`a la fin du chapitre, voir page . Comme annonc\'e, une telle d\'eg\'en\'erescence n'est pas observ\'ee avec Deep SPH SVDD. Des illustrations sont donn\'ees ci-dessous pour appuyer les r\'esultats th\'eoriques du paragraphe . Un exemple est donn\'e dans la figure . On a utilis\'e une fonction d'activation lin\'eaire. On a donc $_0=_{W_0}()=0_{^m}$ (voir les notations au paragraphe ). Pour v\'erifier que le r\'eseau ne tend pas vers celui associ\'e \`a l'hypersph\`ere d\'eg\'en\'er\'ee il suffit donc de v\'erifier que la norme des $_W(_k)$, $_k$ d\'esignant les donn\'ees, ne tend pas vers z\'ero. On voit aussi que les rayons ne tendent pas vers z\'ero non plus. [H] {0.445} {0.445} _k)||$} L'algorithme Deep SPH SVDD est construit pour avoir de bonnes propri\'et\'es de convergence pour le r\'eseau qui apprend en particulier l'espace des caract\'eristiques mais une convergence instable pour le rayon (voir paragraphe ). L'\'evolution des diff\'erents termes de la fonction de perte au fil des it\'erations est d\'etaill\'ee dans la figure et . [H] [H] [H] On note dans la figure que le r\'eseau envoie rapidement les donn\'ees sur le bord de l'hypersph\`ere et corrige de ce point de vue les oscillations du rayon. [H] ||$ et le rayon $$} Ainsi le r\'eseau structure-t-il les donn\'ees en les envoyant sur le bord de l'hypersph\`ere plut\^ot que dans l'hypersph\`ere (du moins pour un choix pertinent de $$ pour la d\'etection, voir aussi la figure au paragraphe suivant). Pour illustrer que le r\'eseau envoie bien les donn\'ees sur un espace pertinent de caract\'eristiques, on peut mener l'exp\'erimentation suivante : on force le r\'eseau \`a r\'epartir de fa con uniforme les donn\'ees sur le bord de ou dans l'hypersph\`ere. Pour se faire, on définit deux distributions uniformes: Sur la sphère unité, $(^{m-1})$ où $^{m-1} = \{\, ^m : \|\| = 1 \}$. Dans la boule unité, $(^m)$ où $^m = \{\, ^m : \|\| 1 \}$. Pour chacune des distributions que l'on notera génériquement $$, on ajoutera à $J$ la pénalité suivante: J_{KL} = C~~D_{} ( \| (_W() - ) / ) o\`u $D_{}$ désigne la divergence de Kullback-Leibler. Le coefficient $C$ sert \`a donner plus ou moins d'importance \`a cette partie du loss. Divers r\'esultats sont donn\'es dans les figures , et . En premier lieu, on constate que le score AUC-ROC reste constant à partir de l'epoch 10200 (cf. figure ). En forçant la distribution des $_W()$ à suivre les deux distributions sus-cités, les scores AUC-ROC sont dégradés voire très dégradés et oscillent en permanence.\\ Au regard de la distribution des scores d'anomalies (cf. figure ), on constate que le seuil optimal de décision est déplacé du bord de la sphère unité (où le seuil devrait être à $0$), et la distribution apparaît plus étalée. \\ Dans le premier cas (avec ajout du terme \( J_{KL} \) pour forcer les points à se répartir uniformément dans la \( ^m \)), on observe que l'hypersphère laisse beaucoup de points normaux à l'extérieur, car la frontière est déplacée vers l'intérieur. \\ Dans le cas où l'on ajoute le terme \( J_{KL} \) pour forcer les points à se répartir sur la \( ^{m-1} \), l'hypersphère rejette également de nombreux points normaux à l'extérieur tout en incluant parfois des anomalies à l'intérieur. {0.3125} {0.3125} {0.3125} 6$.} {0.3125} {0.3125} {0.3125} {0.3125} {0.3125} {0.3125} L'absence de collapse observ\'e est li\'e (voir paragraphe ) \`a une forme d'instabilit\'e dans le calcul du rayon. Il est important de confirmer en pratique que cette instabilit\'e perdure m\^eme si le taux d'apprentissage est diminu\'e (le r\'esultat th\'eorique l'affirme). Le comportement est v\'erifi\'e dans la figure , o\`u l'on voit l'absence de tendance vers un collapse lorsque l'on diminue drastiquement le taux d'apprentissage. [H] 5$.} Les derni\`eres illustrations concernent le remarque juste avant le paragraphe : contrairement \`a la Deep SVDD, la Deep SPH SVDD n'est pas limit\'ee \`a des r\'eseaux sans biais ou/et \`a des fonctions d'activation born\'ee placée après la dernière couche cachée.\\ Suivant la fonction d'activation, la figure illustre les résultats, toujours sur le même jeu de données, avec les fonctions d'activation , et . La fonction est définie comme suit : \( (x) }{e^x + e^{-x}} \), qui ramène les valeurs dans l'intervalle [-1, 1]. De même, la fonction est définie par \( (x) {1 + e^{-x}} \), qui ramène les valeurs dans l'intervalle [0, 1].\\ On constate tout d'abord que les scores AUC-ROC sont largement amélior\'es par rapport celui du modèle sans biais ($ 0.653$) pour les cas et , ce dernier produisant de l'instabilité (cf. figure ). Les distributions des scores d'anomalies sont comme attendues proches de la frontière de décision pour les données normales (cf. figure ). Dans le cas de l'activation , on constate un resserrement de la zone de décision autour de 0. Pour les frontières de décision induites, les résultats sont conformes à ce que l'on pouvait attendre (cf. figure ).\\ [htbp] {0.3125} } {0.3125} } {0.3125} } [htbp] {0.3125} } {0.3125} } {0.3125} } [htbp] {0.3125} } {0.3125} } {0.3125} } Pour finir, l'apport du biais a été testé. Cet enrichissement a donn\'e de meilleurs r\'esultats comme on peut le voir sur les figures . Avec biais, le rayon converge en oscillant autour de zéro (cf. figure ). En observant la moyenne sur les $_k$ des $||_W(_k)||$ que la solution optimale n'est pas atteinte car les poids de la couche cachée sont non nuls (ie. $W^* W_0$). \\ [H] {0.425} {0.425} [H] {0.425} {0.425} [H] {0.425} {0.425} Pour obtenir une vue d'ensemble visuelle simple, un réseau de neurones avec une seule couche hypersphérique (pour SPH SVDD) et un modèle avec une couche cachée suivie d'une couche hypersphérique (pour Deep SPH SVDD) ont été testés sur des ensembles de points dans $^2$. Ensuite, la méthode Deep M-SPH SVDD est testée sur des ensembles de données de dimensions supérieures, à savoir MNIST et CIFAR-10. Les méthodes d'initialisation sont variées pour examiner leur impact sur les performances des modèles. La fonction de coût dans l'équation dépend du paramètre $ ~]0,1[$, qui contrôle le nombre de points en dehors de l'hypersphère. Pour vérifier la sensibilité de ce paramètre, la méthode SPH SVDD a été testée sur des ensembles de données synthétiques en dimension 2 ({ pt anomaly blob circle}) en faisant varier $$ (cf. figure ). À mesure que $$ augmente, l'algorithme rejette plus de points lors de l'apprentissage en les plaçant à l'extérieur de la sphère.\\ Les jeux de données utilisés sont les suivants : : 300 points normaux (en noir) et 30 points d'anomalie (en blanc) disposés en cercle autour de l'origine. : 300 points normaux (en noir) et 30 points d'anomalie (en blanc) disposés un peu plus loin de l'origine. Le premier jeu de données est plus simple que le second, car les points d'anomalie sont plus éloignés des points normaux.\\ La figure montre la pré-image de sphère englobante (en rouge) au seuil 0 pour les deux jeux de données (sous-ensemble d'entraînement) trouvée par SPH SVDD avec $ = 0.001$ et $ = 0.5$. Les résultats montrent que pour un $$ petit, tous les points sont à l'intérieur de la sphère, tandis que pour un $$ plus grand, une proportion similaire de points est à l'extérieur de la sphère.\\ [htpb] {1} [H] {|c|c|c|} & $ = 0.001$ & $ = 0.5$ \\ {} & & \\ {} & & \\ L'évaluation des performances d'un modèle de classification peut être effectuée sur la base de trois critères : l'aire sous la courbe ROC (AUC-ROC), l'aire sous la courbe Précision-Rappel (AUC-PR) et le score F1. Typiquement, le critère le plus couramment utilisé est l'aire sous la courbe ROC.\\ Le tableau de confusion est un outil essentiel pour évaluer les performances d'un modèle de classification binaire. Le tableau de confusion présente les prédictions du modèle par rapport aux véritables classes cibles. Il permet de calculer diverses métriques comme la précision, le rappel, et le taux de faux positifs, qui sont utilisées pour tracer la courbe ROC et la courbe de Rappel-Précision. La structure d'un tableau de confusion se présente ainsi~: [H] {|c|c|c|} & & \\ & Vrai Positif (VP) & Faux Négatif (FN) \\ & Faux Positif (FP) & Vrai Négatif (VN) \\ Si l'on adapte le discours à la détection d'anomalies, les termes du tableau de confusion sont les suivants~:\\ : Nombre d'anomalies correctement identifiées par le modèle. : Nombre de fois où une anomalie n'a pas été détectée par le modèle. : Nombre de fois où le modèle a prédit une anomalie, mais la classe réelle est normale. : Nombre de fois où le modèle a prédit une classe normale et que la classe réelle est normale. On en déduit les métriques suivantes~: [Taux de Vrais Positifs (TVP)] ou ou encore : Il concerne la capacité du modèle à identifier les anomalies (à maximiser). $$ = }{ + }$$ : Il traduit la capacité du modèle à identifier les points normaux comme étant des anomalies (à minimiser). $$ = }{ + }$$ : Elle est définie comme le rapport des vrais positifs par rapport à l'ensemble des prédictions positives (vrais positifs + faux positifs). Elle permet de mesurer la qualité des prédictions positives (à maximiser). $$ = }{ + }$$ : Elle est définie comme le rapport des vrais négatifs par rapport à l'ensemble des prédictions négatives (vrais négatifs + faux négatifs). Elle permet de mesurer la qualité des prédictions négatives (à maximiser). $$ = }{ + }$$ : Il s'agit de la moyenne harmonique entre la précision et le rappel. Il permet de trouver un compromis entre ces deux métriques en un seul score en favorisant les modèles qui ont des valeurs équilibrées de précision et de rappel (à maximiser). $$ = 2 }{ + }$$ En classification binaire (et donc en détection d'anomalies), il est nécessaire de trouver un compromis entre tous les critères sus-mentionnés: le rappel, la précision, la spécificité... (Ex.: score F1). D'autant que ces métriques dépendent du seuil de décision. Dans notre cas, le seuil de décision par défaut est 0, ce qui correspond à positionner la frontière de décision sur la sphère englobante.\\ Pour différentes valeurs de seuil, les métriques évoluent. On peut donc tracer des courbes pour visualiser ces évolutions. On peut ainsi tracer les courbes ROC et Précision-Rappel.\\ La courbe ROC, correspond dont à la courbe tracée par les points qui ont pour abscisse les TFP et pour ordonnée les TVP. L'AUC-ROC est l'aire sous la courbe de ROC et permet de quantifier la capacité du modèle à différencier les classes positives des classes négatives. Sa valeur est comprise entre 0.5 et 1, où 1 correspond à un modèle parfait. La recherche d'un seuil de décision optimal peut être effectuée en cherchant le point le plus proche du coin supérieur gauche de la courbe ROC parmi les seuils testés $\{_i\}$: ^* = _{_i} ( (_i)^2 + (1 - (_i))^2 ) Nous avons appliqué cette méthode en post-traitement pour obtenir le seuil optimal dans l'ensemble de nos algorithmes dans le cas où un jeu de données de validation est disponible. En conséquence, cela améliore les performances de nos modèles mais ne fait plus correspondre la frontière de décision avec la sphère englobante. Cela impacte également la $$-propriété. L'AUC-PR (Aire sous la courbe rappel-précision) mesure également la performance du modèle en termes de précision et de rappel à différents seuils de décision. La courbe est tracée en représentant la précision en fonction du rappel. D'après la littérature , ce critère semble plus approprié dans le cas d'un problème à classes déséquilibrées. Cependant, elle peut mener à une interprétation trop optimiste des modèles.\\ [H] La figure , montre un exemple de tracé des courbes ROC et PR (Rappel-Précision). Une régression logistique est utilisée et les prédictions sont seuillées avec différents seuils (en rouge). ^2$} Dans cette section, les méthodes présentées sont testées et comparées sur des jeux de données synthétiques, incluant la SVDD reprogrammée à partir des éléments décrits dans la section , les méthodes utilisant les réseaux de neurones à couche hypersphérique, ainsi que les méthodes classiques de la bibliothèque . Chaque jeu de données se compose de 300 points. Les points bleus représentent les données considérées comme normales, tandis que les points rouges indiquent les anomalies. Les jeux de données sont divisés en deux parties : l'une pour l'entraînement et l'autre pour le test, constitués donc de 150 points chacun. Les anomalies sont présentes uniquement dans l'ensemble de test, avec un taux de 15\ [H] [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} Les types de données considérés sont des configurations variées d'anomalies, telles que des anomalies ponctuelles en forme de cercle, des blobs, et des données avec différentes structures comme les blobs, les données variées, anisotropes, lunaires, ou sans structure spécifique. Dans les modèles testés, les centres sont initialisés à l'aide d'une distribution normale standard et les rayons sont initialisés à 1. L'optimisation est effectuée en utilisant la méthode Adam, avec un taux d'apprentissage initial de 0,001 qui diminue en utilisant la méthode de réduction de plateau avec un facteur de 0,1 et une patience de 50.\\ La Deep SVDD et la Deep SPH SVDD étant des méthodes d'apprentissage profond, chacune est composée d'un modèle de réseau de neurones : Deep SVDD : L'architecture du modèle comprend un encodeur séquentiel qui prend en entrée un vecteur et le traite à travers trois couches linéaires. La première couche réduit la dimension à 64, la seconde maintient cette dimension à 64, et la troisième couche réduit la dimension à celle des centres de l'hypersphère englobante. Chaque couche linéaire est suivie d'une activation LeakyReLU. Deep SPH SVDD : L'architecture du modèle comprend deux parties principales : un encodeur et une couche hypersphérique. L'encodeur est constitué d'une séquence de couches, débutant par une couche linéaire qui réduit la dimension de l'entrée à 64, suivie d'une activation , puis d'une seconde couche linéaire qui réduit à nouveau la dimension à celle correspondant aux centres des hypersphères, suivie d'une autre activation . ( la méthode SPH SVDD est seulement composé de la couche hypersphérique). } Le tableau ci-dessous montre les résultats des scores AUC-ROC pour les modèles Enveloppe elliptique, OC-SVM, isolation forest, LOF et SVDD pour les différents jeux de données (cf. Section ). [H] {|l|c|c|c|c|c|} &[c]{@{}c@{}}Enveloppe \\ elliptique& [c]{@{}c@{}}OC-SVM \\ (RBF)& [c]{@{}c@{}}Isolation \\ Forest& LOF & SVDD\\ pt anomly circle & 0.86 & 0.91 & 0.86 & 0.80 & \\ pt anomaly blob circle & 0.90 & 0.89 & 0.90 & & 0.92 \\ point anomaly & 0.95 & 0.82 & 0.90 & & 0.83 \\ make blob & 0.93 & 0.92 & 0.93 & & 0.95 \\ make varied & 0.88 & 0.91 & 0.89 & 0.93 & \\ make aniso & 0.75 & & 0.86 & 0.85 & 0.85 \\ make moo, & 0.76 & 0.81 & 0.86 & 0.76 & \\ make no structure & 0.49 & 0.42 & 0.43 & 0.45 & \\ Globalement, la méthode SVDD a obtenu les meilleurs résultats dans plusieurs scénarios expérimentaux. En particulier, dans la détection des anomalies ponctuelles en forme de cercle avec un score de 0.99, ainsi que pour les données de type lune, avec un scores de 0.98. Ces résultats suggèrent que SVDD est efficace pour détecter des anomalies dans des structures de données plus complexes et variées. Cette section présente les performances de plusieurs méthodes de détection d'anomalies : SVDD, Deep SVDD, SPH SVDD et Deep SPH SVDD. Les résultats sont évalués à l'aide de différentes métriques, notamment l'AUC-ROC, l'AUC-PR et le score F1 dans le but de comparer l'efficacité de chaque méthode sur divers jeux de données synthétiques. [H] {|c|c|c|c|c|} & & & & \\ & 0.99 & 0.90 & 0.99 & \\ & & 0.90 & 0.70 & \\ &0.83 & 0.77 & 0.89 & \\ & 0.95 & 0.99 & 0.95 & \\ & 0.95 & 0.96 & 0.76 & \\ & 0.85 & 0.86 & 0.66 & \\ & 0.93 & 0.93 & 0.58 & \\ & 0.67 & 0.58 & 0.47 & \\ [H] {|c|c|c|c|c|} & & & & \\ &0.99& 0.98 & 0.99 & 0.99 \\ &0.99 & 0.99 & 0.97 & 0.98 \\ &0.98 & 0.97 & 0.98 & 0.98 \\ &0.99 & 0.99 & 0.99 & 0.99 \\ &0.99 & 0.99 & 0.97 & 0.98 \\ &0.98 & 0.98 & 0.96 & 0.97 \\ &0.99 & 0.99 & 0.96 & 0.97 \\ &0.96 & 0.96 & 0.95 & 0.95 \\ [H] {|c|c|c|c|c|} & & & & \\ &0.99 & 0.88 & 0.99 & 0.97 \\ &0.91 & 0.89 & 0.97 & 0.96 \\ &0.79 & 0.70 & 0.98 & 0.99 \\ &0.99 & 0.99 & 0.99 & 0.99 \\ &0.99 & 0.99 & 0.97 & 0.98 \\ &0.98 & 0.98 & 0.96 & 0.97 \\ &0.99 & 0.99 & 0.96 & 0.97 \\ &0.96 & 0.96 & 0.92 & 0.95 \\ Le critère AUC-ROC est celui pour lequel les différences entre les méthodes sont les plus significatives. C'est donc sur ce critère que l'analyse va pouvoir être la plus pertinente pour différencier l'efficacité des méthodes sur les différents jeux de données. Dans l'ensemble, les méthodes SVDD et Deep SVDD sont similaires. On rappelle que nous sommes dans un cas où la SVDD est applicable car il y a peu de points dans chaque jeu de données. La méthode SPH SVDD ne contenant pas de couche cachée, cela peut expliquer la différences des résultats obtenus puisque la fonction de décision ne dépend que d'un seul neurone contenue dans la couche hypersphérique. Les résultats montrent que la méthode Deep SPH SVDD offre les meilleures performances globales dans la plupart des scénarios notamment avec les jeux de données "make aniso", "make moon" et même "no structure" où les données sont plus complexes par leur structure. Dans cette section, l'approche utilisant les réseaux de neurones basés sur les fonctions à bases radiales (RBF) est comparée à une approche similaire utilisant des couches hypersphériques. Les performances sont évaluées selon trois mêmes critères que précédemment. [H] {|c|c|c|c|c|c|c|} & {|c|}{ AUC-ROC} & {|c|}{ AUC-PR} & {|c|}{score F1}\\ & & & & & & \\ && & 1.00 & 1.00 & 1.00 & 1.00\\ && 0.80 & 0.99 & 0.98 & 0.97 & 0.98 \\ &0.56& & 0.95 & 1.00 & 0.92 & 1.00 \\ &0.9& & 0.99 & 0.99 & 0.99 & 0.99\\ && 0.83 & 0.98 & 0.98 & 0.98 & 0.98 \\ && 0.63 & 0.97 & 0.96 & 0.96 & 0.96\\ && 0.67 & 0.80 & 0.96 & 0.97 & 0.93 \\ && 0.44 & 0.48 & 0.92 & 0.93 & 0.73 \\ Comme précédemment, les scores F1 et AUC-PR sont très proches pour les deux méthodes, avec des différences plus prononcées pour le score AUC-ROC. Hormis pour le jeu de données "point anomaly", où la méthode SPH Anomaly est beaucoup plus performante, elle ne se montre pas très convaincante par rapport à l'approche classique. Ce qui laisse suggérer que la methode proposée n'est pas efficace dans certaines configurations de données. Le tableau reprend les résultats obtenus entre les deux méthodes proposées Deep SPH SVDD et SPH Anomaly . Les scores AUC-ROC, AUC-PR et F1 sont présentés pour chaque jeu de données avec un focus particulier pour le score AUC-ROC.\\ [htbp] {|c|c|c|} & {|c|}{ AUC-ROC} \\ & & \\ & & 1.00 \\ & & 0.8 \\ & & 1.00 \\ & & 0.93 \\ & & 0.83 \\ & & 0.63 \\ & & 0.67 \\ & & 0.44 \\ Les résultats montrent que la méthode Deep SPH SVDD est plus performante que la méthode SPH Anomaly pour tous les jeux de données. Ces résultats suggèrent que la méthode Deep SPH SVDD est plus efficace pour détecter des anomalies dans des structures de données plus complexes et variées.\\ Afin d'illustrer cela, nous avons tracé les frontières de décisions des sphères pour chaque méthode sur quelques jeux de données (cf. figure ). La méthode SPH Anomaly est évaluée avec deux types d'initialisation des paramètres : d’une part, une initialisation où le centre suit une loi normale $(0,1)$ avec un rayon fixé à 1, et d’autre part, une approche d'initialisation analogue à celle de la méthode -means++ (décrite dans la section ).\\ On constate immédiatement que les couches cachées permettent de mieux capturer la structure des données avec une seule sphère alors SPH Anomaly correspond "simplement" à un assemblage de sphères en dimension 2.\\ Ici on peut pointer le principal défaut de la méthode SPH Anomaly: la méthode contraint le rayon des sphères à être suffisamment grand pour que la sigmoïde soit proche de 1. La figure montre que le rayon doit être au minimum de 3. Dans le cas du jeu de données "make aniso", on observe ce forcage du rayon sur la partie inférieure gauche. On déduit qu'il est nécessaire de faire précéder la dernière couche de la méthode SPH Anomaly d'une ou plusieurs couches cachées (vers une Deep SPH Anomaly ...).\\ On peut également observer que, pour des ensembles de données comme ou , la valeur de sortie obtenue dépasse 1 à certains endroits où il n'y a pas de données normales. En effet, comme l’illustre la figure , selon la position des centres et les rayons des sphères, celles-ci peuvent se chevaucher. Cela a pour conséquence d’additionner le signal, ce qui augmente la valeur de sortie. La méthode nécessite donc une réflexion supplémentaire afin de contraindre les sphères à ne pas s’intersecter. On observe que le déplacement des centres lors de l’initialisation améliore les résultats.\\ [htpb] {1} {|c|c|c|c|} &Deep SPH SVDD & SPH Anomaly & SPH Anomaly ($k$-means++)\\ { } & & & \\ { } & & & \\ \\ {1} {|c|c|c|c|} &Deep SPH SVDD & SPH Anomaly & SPH Anomaly ($k$-means++)\\ { } & & & \\ { } & & & \\ Dans cette section, les expériences menées visent à observer le comportement des réseaux de neurones dans le cas de l'utilisation de plusieurs hypersphères en utilisant le modèle Deep M-SPH SVDD proposé. On testera d'abord sur un ensemble de données de points dans $^2$ pour une évaluation visuelle, puis sur les ensembles de données MNIST et CIFAR10. Cette expérimentation dans $^2$ a également pour ambition d'améliorer la compréhension du phénomène de collapse décrit dans . Le jeu de données synthétiques (nommé ) est composé de 350 points normaux générés à partir de trois groupes distincts et de 98 anomalies générées à partir d'une distribution uniforme tronquée pour être majoritairement à l'extérieur des points normaux; il s'agit d'une version modifiée des jeux de données de . Cette configuration est choisie pour être suffisamment simple afin de vérifier le comportement de la méthode: on s'attend à ce que les hypersphères englobent chaque groupe de points normaux tout en excluant les anomalies. L'ensemble de données est divisé en un ensemble d'entraînement de 175 points et un ensemble de test de 175+98 points, qui inclut les anomalies. Le modèle utilisé est constitué d'un encodeur qui joue le rôle de la fonction $_W$. Dans le modèle classique de Deep SVDD, l'encodeur comprend deux couches cachées linéaires sans biais suivies d'une BatchNorm et d'une activation LeakyReLU, tandis le modèle proposé Deep M-SPH SVDD, utilise un à la place du LeakyReLU, contient des biais pour les couches linéaires et ajoute une couche hypersphérique en sortie.\\ Contrairement au modèle classique, où la sortie est de dimension $d$ (correspondant à la dimension du centre $$ de l'hypersphère), la sortie du modèle Deep M-SPH SVDD est un scalaire représentant directement le score d'anomalie $-2_j _i _k$ (cf. équation ).\\ Étant donné que les fonctions d'activation des couches cachées de l'encodeur sont des , l'initialisation des poids des couches linéaires se fait via la méthode par défault He uniforme. Les centres des hypersphères sont initialisés selon une distribution normale $(0,1)$, tandis que le rayon est fixé à 1.\\ Comme mentionné précédemment, le seuil de l'AUC-ROC est ajusté en fonction des scores d'anomalie sur un ensemble de validation et ne coïncide donc pas avec les frontières des hypersphères ajustées sur l'ensemble d'entraînement. Les autres hyperparamètres de la méthode ($$,$$) sont également définis à partir de l'ensemble de validation.\\ L'optimisation est effectuée en utilisant la méthode Adam, avec un taux d'apprentissage initial de 0.001 qui diminue en utilisant la méthode de réduction de plateau avec un facteur de 0.1 et une patience de 100. [htpb] _1$ et $_2$ de centre: $ 3.85$ et de rayon: $ = 4$ modifiées par des sigmoïdes, ainsi que la somme des deux termes} Cette section analyse l'application du modèle Deep M-SPH SVDD sur le jeu de données "easy", afin de visualiser son comportement. Trois configurations sont testées : avec 1, 3 et 10 hypersphères. Selon la méthode de détermination des paramètres de l'hypersphère, incluant le centre et le rayon, ces derniers peuvent être soit appris, soit fixés. Dans ce dernier cas, les gradients sont maintenus à donc fixés à 0.\\ modèle 1 : [{$J$: 1, $m$: 64, $$: 0.1, $$ : 0.0003321558199348189}] \\ modèle 2 : [{$J$: 3, $m$: 64, $$: 0.001, $$: 1.711799308608884e-6}] \\ modèle 3 : [{$J$: 10, $m$: 64, $$: 0.01, $$: 0.004318232518633555}] {6.5cm} [H] {0.45} {0.45} {0.99} {7cm} [H] {0.475} {non} appris.} {0.475} {0.475} {non} appris.} {0.475} {0.475} {non} appris.} {0.475} {8.9cm} [H] {0.495} {0.495} {non} appris.} {0.495} {non} appris.} {0.495} {0.495} {non} appris.} {0.495} \\ [H] {|c|c|c|} & rayon et centres {non} appris. & rayon et centres appris. \\ Modèle 1 & $0.85$ & $0.86$ \\ Modèle 2 & $0.79$ & $0.91$ \\ Modèle 3 & $0.76$ & $0.88$ \\ La figure montrent les pré-images des hypersphères (c. à d.~ $ _i_W(x)=0$) (ligne pleine), tandis que la ligne pointillée représente la frontière de décision optimale _i_W(x)$, une valeur négative correspond à une valeur positive du score d'anomalie}. Étant donné que $$ est fixé à 0.001, l'hypersphère englobe toutes les données.\\ Pour le modèle 1 (avec les paramètres de $s$ non fixés, donc gradients non fixés), la frontière de décision est à l'intérieur de l'hypersphère puisque le seuil optimal est positif et égal à 9.9. L'AUC-ROC est a 0.86.\\ Pour le modèle 2 (toujours avec les paramètres de $s$ non fixés, donc rayons et centres appris), il peut être observé que les pré-images des trois sphères couvrent la majorité des points d'entraînement et ne s'intersectent pas. Cette dernière propriété est favorisée par la perte d'intersection. Il est à noter que les sphères finales correspondent approximativement aux trois groupes de points normaux. Le score AUC-ROC est amélioré à 0,91 avec un seuil de 0,004. De plus l'observation des distribution de sortie ( cf figure ), montre que dans cette configuration, les distributions sont moin écrasées en zéro. \\ Lorsque plusieurs hypersphères sont rajoutées (avec 10 hypersphères et gradient toujours libre), voir le modèle 3 dans la figure , on observe que les rayons de nombreuses sphères convergent vers 0, ce qui signifie que ces hypersphères dégénèrent en points. Bien que l'observation des frontières de décisions correspondant à ce modèle présente la frontière de trois hypersphères, quatre rayons sont non nulles. En effet, le score d'anomalie donnée par cette quatrième sphère étant négatif, il n y a pas de frontière en zéro. Un inspection minutieuse de $_j$.$_W(x_k)$ a montré que cette hypersphère ne couvre aucun point. \\ Il est constaté que la liberté laissée aux paramètres améliore les résultats, en particulier le score AUC-ROC (cf. tableau ). De plus, les distributions (cf. ) montrent que les points normaux obtiennent des scores positifs, contrairement au cas où les gradients sont fixés. Cela permet d'établir une frontière de décision proche du seuil, comme illustré dans la figure . Les ensembles de données MNIST et CIFAR10 sont largement utilisés pour évaluer les méthodes de détection d'anomalies. Selon la méthodologie couramment adoptée, les expériences comparent une classe à toutes les autres ("OneVsAll"). L'ensemble d'entraînement contient 5000 images de la classe normale, tandis que l'ensemble de test contient 1000 images de chaque classe, soit 10000 images de test avec 90 \ Les figures et illustrent les architectures des modèles Deep SVDD appliquées aux ensembles de données MNIST et CIFAR-10 qui ont été utilisés dans . Ces réseaux de neurones convolutifs sont conçus pour détecter les anomalies en apprenant une représentation compacte des données normales vers un espace de caractéristiques. Chaque architecture comprend plusieurs couches de convolution, de normalisation, et d'activation LeakyReLU présenté dans les figures. S'enchaînent ensuite une couche pour mettre à plat le tenseur et une couche linéaire finale. Les paramètres de centre et de rayon de l'hypersphere $[C, ]$ sont simplement déclarés comme des tenseurs () et non en tant que paramètres d'une couche linéaire. Ils ne seront pas mis à jour lors de l'optimisation. \\ 1{1.5} [h] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(1, 28, 28)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(8, 28, 28)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(8, 14, 14)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(4, 14, 14)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(4, 7, 7)$}; (flatten) [rectangle, draw, right of=pool2, rotate=90] { Flatten (196)}; (linear) [rectangle, draw=blue, right of=flatten, rotate=90] { Linear $(32)$}; (sph) [rectangle, draw=blue, below of=linear, rotate=90, node distance=2cm] { $[C, ]$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (sph) -- (linear); [h] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(3, 32, 32)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(32, 32, 32)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(32, 16, 16)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(64, 16, 16)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(64, 8, 8)$}; (conv3) [rectangle, draw, right of=pool2, rotate=90] { Conv $(128, 4, 4)$}; (bn3) [rectangle, draw, right of=conv3, rotate=90] { BatchNorm}; (lrelu3) [rectangle, draw, right of=bn3, rotate=90] { LeakyReLU}; (flatten) [rectangle, draw, right of=lrelu3, rotate=90] { Flatten (2048)}; (linear) [rectangle, draw=blue, right of=flatten, rotate=90] { Linear $(128)$}; (sph) [rectangle, draw=blue, below of=linear, rotate=90, node distance=2cm] { $[C, ]$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (conv3); [->] (conv3) -- (bn3); [->] (bn3) -- (lrelu3); [->] (lrelu3) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (sph) -- (linear); Les modèles suivent une architecture similaire à celle utilisée pour la méthode Deep SVDD mais ajoutent une couche sphérique à la fin. Contrairement au modèle Deep SVDD, ici les paramètres d'hypersphère sont intégrés dans la couche sphérique, ce qui transforme la sortie linéaire en une représentation unidimensionnelle par hypersphère. Cela signifie que pour $m$ hypersphères utilisées, le tenseur de sortie est de taille $m$ et pour le modèle Deep SPH SVDD, il est de taille 1.\\ Concernant les paramètres d'apprentissage, L'hyperparamètre $$ est fixé à 1e-4 pour MNIST et à 0,01 pour CIFAR-10. Le taux d'apprentissage est initialisé à 1e-4 et réduit d'un facteur de 0,1 lorsque le plateau est atteint (patience = 5). Le nombre d'epochs est limité à 2500. Le terme de régularisation est la somme des normes $L_2$ des poids du réseau avec $ = 1-6$. Chaque noyau de convolution a une taille de (5,5). 1{1.5} [H] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(1, 28, 28)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(8, 28, 28)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(8, 14, 14)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(4, 14, 14)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(4, 7, 7)$}; (flatten) [rectangle, draw, right of=pool2, rotate=90] { Flatten (196)}; (linear) [rectangle, draw=red, right of=flatten, rotate=90] { Linear $(32)$}; (sph) [rectangle, draw=red, right of=linear, rotate=90] { Spherical $[C, ]$ $ (m)$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (linear) -- (sph); [H] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(3, 32, 32)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(32, 32, 32)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(32, 16, 16)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(64, 16, 16)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(64, 8, 8)$}; (conv3) [rectangle, draw, right of=pool2, rotate=90] { Conv $(128, 4, 4)$}; (bn3) [rectangle, draw, right of=conv3, rotate=90] { BatchNorm}; (lrelu3) [rectangle, draw, right of=bn3, rotate=90] { LeakyReLU}; (flatten) [rectangle, draw, right of=lrelu3, rotate=90] { Flatten (2048)}; (linear) [rectangle, draw=red, right of=flatten, rotate=90] { Linear $(128)$}; (sph) [rectangle, draw=red, right of=linear, rotate=90] { Spherical $[C, ]$ $(m)$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (conv3); [->] (conv3) -- (bn3); [->] (bn3) -- (lrelu3); [->] (lrelu3) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (linear) -- (sph); où $m$ est le nombre d'hypersphères utilisées dans la couche finale. Pour l'initialisation de la première partie du réseau (encodeur), nous avons testé deux stratégies: Un autoencodeur pré-entraîné sur les données normales (AE) (mentionné par AE dans les tableaux ). Une initialisation des poids selon la méthode de Glorot (mentionné comme $$ dans les mêmes tableaux). Dans les modèles testés, si le rayon est initialisé à zéro, les paramètres d'hypersphère sont fixes. Cependant, dans le second cas, l'initialisation du rayon est déterminée par la méthode d'initialisation des centres.\\ Les centres sont initialisés soit en suivant une distribution normale standard (désignée par $(0,1)$ dans les figures et ), avec un rayon initialisé à 1, soit en utilisant la méthode $k$-means++ , suivie d'une étape d'assignation au plus proche voisin, puis d'un calcul de la distance moyenne aux centres pour déterminer les rayons (notés dist. $_J$ dans les tableaux de résultats, où $m$ est le nombre d'hypersphères utilisées). Dans les deux cas, les paramètres des hypersphères sont entraînés par le modèle.\\ Les tableaux ci-dessous présentent les résultats pour les différentes méthodes utilisées (Deep M-SPH SVDD, Deep M-SPH SVDD ainsi que Deep SVDD et les références de l'article ) correspondant à l'aire sous la courbe de ROC en pourcentage (AUC-ROC). Les résultats sont analysés en fonction des paramètres d'initialisation du centre $$, de la fixation ou non des paramètres de rayon et de centre (dans , , et , si les rayons et les centres sont appris, il sont mentionnés dans la ligne ), de la valeur initiale du rayon ($0$, $1$ ou $_m$), et de l'initialisation des poids des couches précédentes avec ceux de l'autoencodeur pour la couche hypersphérique (avec $m$ le nombre d'hypersphères considéré). Cela implique que pour la Deep SPH SVDD, m=1.\\ Les résultats observés dans les figures et révèlent des tendances contrastées en fonction des méthodes et des ensembles de données utilisés. L'effet du pré-entraînement avec un autoencodeur (AE) semble bénéfique pour les modèles appliqués à MNIST mais moins favorable pour ceux appliqués à CIFAR10.\\ Concernant l'impact des paramètres, les modèles basés sur Deep OC-SVDD montrent généralement de meilleures performances par rapport à Deep soft-bound SVDD sur les deux ensembles de données. Il est donc préférable d'entraîner le modèle avec un rayon fixé à zéro plutôt que d'utiliser une hypersphère de rayon 1 pour les modèles classiques.\\ Pour les modèles proposés comme Deep M SPH-SVDD, les performances sont meilleures lorsque les paramètres ne sont pas fixés, alignant ainsi ces modèles plus près de la méthode Deep soft-bound SVDD. Cependant, une exception notable se produit lorsque les centres sont initialisés en utilisant une distribution normale réduite centrée ($c ( 0, 1)$) pour les données CIFAR10.\\ En termes d'impact de l'initialisation, pour MNIST, initialiser les centres avec la méthode $k$-means++ donne de meilleures performances par rapport à l'initialisation avec une distribution normale réduite centrée. Ce qui est l'inverse pour les modèles traitant CIFAR10, hormis le cas où les paramètres de centre et de rayon sont fixés. Dans ce cas, les résultats sont supérieurs lorsque les centres sont initialisés avec $k$-means++ et les rayons sont initialisés en fonction des paramètres $_m$.\\ En résumé, ces observations soulignent l'importance critique de l'initialisation des paramètres du modèle adaptée à la complexité et à la nature des données pour la détection d'anomalies. L'ensemble de données CIFAR10 présente une tâche de détection d'anomalies plus difficile comparée à MNIST, ce qui se reflète dans des scores AUC-ROC généralement plus bas. De plus , les résultats montrent que la méthode Deep M-SPH SVDD fonctionne mieux lorsque les autres méthodes échouent, mais est moins performante lorsque les autres méthodes réussissent. La classe "oiseau" est connue pour être particulièrement difficile, comme en témoigne la large dispersion de son encodage en deux dimensions (voir ). Pour cette classe, la méthode Deep M-SPH SVDD avec tous les paramètres fixés obtient constamment le score AUC-ROC le plus élevé avec une marge significative. [H] {!}{ {|c|c|c|c|c||c|c|c|c||c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{{@{}c@{}}Soft Bound \\ Deep SVDD} & {c|}{{@{}c@{}}Deep \\ OC SVDD} \\ Paramètres appris & $(W)$ & $(W,,)$ & $(W)$ & $(W,,)$ & $(W)$ &$(W,,)$ & $(W)$ & $(W,,)$ & $(W)$ & $(W,)$\\ Initialisation $_W$ &{c|}{$$} & {c||}{AE} & {c|}{$$} & {c||}{AE} & {c|}{AE} \\ centre init &{c||}{$c ( 0, 1)$ } &{c||}{$k$-means++ } & - & -\\ rayon init & 0 & 1 & 0 & 1& 0&dist. $_1$& 0 & dist. $_1$& 0 & 1\\ 0 & 0.9389 & 0.9742 & 0.9609 & 0.9839 & 0.9384 & & 0.9682 & 0.9811 & 0.9800 & 0.9780 \\ 1 & 0.9894 & 0.9941 & 0.9696 & 0.9946 & 0.9937 & & 0.9937 & 0.9951 & 0.9970 & 0.9960 \\ 2 & 0.7580 & 0.7817 & 0.8002 & 0.8608 & 0.8332 & 0.8777 & 0.9035 & & 0.9170 & 0.8950 \\ 3 & 0.8420 & 0.8741 & 0.8729 & 0.9297 & 0.8735 & 0.8978 & 0.8803 & 0.8689 & & 0.9030 \\ 4 & 0.8945 & 0.9353 & 0.9123 & 0.9441 & 0.9125 & 0.9369 & 0.9081 & 0.9371 & & 0.9380 \\ 5 & 0.8128 & 0.8457 & 0.8583 & 0.8563 & 0.8229 & 0.8578 & 0.8192 & & 0.8850 & 0.8580 \\ 6 & 0.9309 & 0.9652 & 0.9400 & 0.9872 & 0.9752 & 0.9743 & 0.9819 & & 0.9830 & 0.9800 \\ 7 & 0.8777 & 0.9286 & 0.8602 & 0.9285 & 0.9474 & 0.9378 & 0.9178 & 0.9280 & & 0.9270 \\ 8 & 0.7846 & 0.8925 & 0.9218 & 0.9263 & 0.9138 & 0.9045 & 0.9210 & & 0.9390 & 0.9290 \\ 9 & 0.8818 & 0.9513 & 0.8919 & 0.9528 & 0.9605 & 0.9598 & 0.9638 & & 0.9650 & 0.9490 \\ } : MNIST, 1-sphère, dimension 32.} [H] {!}{ {|c|c|c|c|c||c|c|c|c||c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{{@{}c@{}}Soft Bound \\ Deep SVDD} & {c|}{{@{}c@{}}Deep \\ OC SVDD} \\ paramètres appris & $(W)$ & $(W,,)$ & $(W,,)$ & $(W)$ & $(W)$ &$(W,,)$ &$(W,,)$ & $(W)$ & $(W)$ & $(W,)$\\ Initialisation $_W$ & {c|}{$$} & {c||}{AE} & {c|}{$$} & {c||}{AE} & {c|}{AE} \\ centre init &{c||}{$c ( 0, 1)$ } &{c||}{$k$-means++} & - & -\\ rayon init & 0 & 1 & 0 & 1 & 0 & dist. $_1$ & 0 & dist. $_1$ & 0 & 1 \\ 0 & 0.6087 & 0.6360 & 0.6789 & 0.6662 & & 0.6477 & 0.6583 & 0.6458 & 0.617 & 0.617 \\ 1 & 0.5838 & 0.5711 & 0.4743 & 0.4762 & 0.5106 & 0.4991 & 0.5338 & 0.4993 & & 0.648 \\ 2 & 0.6530 & 0.6641 & 0.6467 & 0.6429 & & 0.6503 & 0.6427 & 0.6474 & 0.508 & 0.495 \\ 3 & 0.5652 & 0.5766 & 0.5274 & 0.5304 & & 0.5608 & 0.5344 & 0.5483 & 0.591 & 0.56 \\ 4 & 0.7246 & 0.7306 & 0.7276 & 0.7224 & & 0.7041 & 0.7404 & 0.7206 & 0.609 & 0.591 \\ 5 & 0.6093 & 0.6283 & 0.5257 & 0.5409 & & 0.5704 & 0.5168 & 0.5191 & 0.657 & 0.621 \\ 6 & 0.7184 & 0.7160 & 0.7610 & 0.7626 & 0.6843 & 0.7276 & 0.7594 & & 0.677 & 0.678 \\ 7 & 0.5736 & 0.5638 & 0.5204 & 0.5229 & 0.5603 & & 0.5380 & 0.5261 & 0.673 & 0.652 \\ 8 & 0.6794 & 0.7241 & 0.6409 & 0.6590 & 0.7574 & 0.7315 & 0.6429 & 0.6353 & & 0.756 \\ 9 & 0.5554 & 0.5700 & 0.4987 & 0.5362 & 0.5430 & 0.5902 & 0.5757 & 0.5411 & & 0.71 \\ } : CIFAR-10, 1-sphère, dimension 128. } La prochaine étape consiste à analyser s'il est bénéfique d'utiliser davantage de neurones avec des dimensions plus petites. Pour maintenir un nombre approximatif de paramètres, la dimension des hypersphères est divisée par le nombre d'hypersphères utilisées dans la couche. Pour MNIST, cela implique de passer d'une seule sphère de dimension 64 à quatre sphères de dimension 16, et pour CIFAR10, cela signifie passer d'une sphère de dimension 128 à quatre sphères de dimension 32. Cela augmente le nombre de paramètres de 3. Les observations faites précédemment ne varient pas avec les changements dans le nombre de sphères à travers les modèles. [H] {!}{ {|c|c|c|c|c||c|c|c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{Deep M-SPH SVDD } \\ m, dim & {c|}{1, 64} & {c||}{4, 16} & {c|}{1, 64} & {c|}{4, 16} \\ paramètres appris & {c||}{$(W)$}& {c|}{$(W,,)$}\\ Initialisation $_W$ & {c|}{AE }\\ centre init & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++\\ rayon init & 0 & dist. $_1$ & 0 & dist. $_m$ & 1 & dist. $_1$ & 1 & dist. $_m$ \\ 0 & 0.9764 & & 0.8715 & 0.9742 & & 0.9824 & 0.9751 & 0.9730 \\ 1 & 0.9864 & 0.9932 & 0.9541 & 0.9940 & 0.9961 & 0.9956 & & 0.9949 \\ 2 & 0.8654 & & 0.8019 & 0.9008 & 0.8965 & 0.9117 & 0.8624 & 0.8857 \\ 3 & 0.8930 & 0.8909 & 0.7385 & 0.8791 & 0.9036 & & 0.8932 & 0.8855 \\ 4 & & 0.8964 & 0.9018 & 0.9085 & 0.9288 & 0.9381 & 0.9205 & 0.9221 \\ 5 & & 0.8518 & 0.8209 & 0.8356 & 0.8739 & 0.8921 & 0.8185 & 0.8413 \\ 6 & 0.9824 & 0.9824 & 0.9028 & 0.9755 & & 0.9894 & 0.9726 & 0.9779 \\ 7 & 0.9182 & 0.9098 & 0.8561 & 0.9175 & & 0.9335 & 0.9023 & 0.9102 \\ 8 & 0.9422 & 0.9219 & 0.8810 & 0.9122 & 0.9225 & & 0.9279 & 0.9005 \\ 9 & 0.9430 & 0.9611 & 0.8321 & 0.9546 & & 0.9676 & 0.9517 & 0.9536 \\ } : MNIST.} [H] {!}{ {|c|c|c|c|c||c|c|c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{Deep M-SPH SVDD } \\ m, dim & {c|}{1, 128} & {c||}{4, 32} & {c|}{1, 128} & {c|}{4, 32} \\ paramètres appris & {c||}{$(W)$}& {c|}{$(W,,)$}\\ Initialisation $_W$ & {c|}{$$ }\\ centre init & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++\\ rayon init & 0 & dist. $_1$ & 0 & dist. $_m$ & 1 & dist. $_1$ & 1 & dist. $_m$ \\ 0 & 0.6087 & & 0.5709 & 0.6367 & 0.6360 & 0.6341 & 0.6124 & 0.5956 \\ 1 & 0.5838 & 0.5127 & 0.5027 & 0.5094 & & 0.5538 & 0.4729 & 0.5004 \\ 2 & 0.6530 & & 0.6288 & 0.6640 & 0.6641 & 0.6590 & 0.6527 & 0.6477 \\ 3 & 0.5652 & & 0.5510 & 0.5603 & 0.5766 & 0.5711 & 0.5469 & 0.5349 \\ 4 & 0.7246 & 0.7498 & 0.6931 & & 0.7306 & 0.7071 & 0.6987 & 0.7041 \\ 5 & 0.6093 & & 0.5392 & 0.5547 & 0.6283 & 0.5832 & 0.5731 & 0.5400 \\ 6 & & 0.6843 & 0.7012 & 0.7078 & 0.7160 & 0.7149 & 0.7005 & 0.7181 \\ 7 & 0.5736 & 0.5603 & 0.5378 & 0.5333 & & 0.5638 & 0.5322 & 0.5398 \\ 8 & 0.6794 & & 0.6474 & 0.6970 & 0.7241 & 0.7247 & 0.6769 & 0.6543 \\ 9 & 0.5554 & 0.5430 & 0.4944 & 0.5619 & 0.5700 & & 0.5195 & 0.5284 \\ } : CIFAR-10.} Comme le montrent les courbes de fonction de perte, elles diminuent avec les modèles utilisant davantage de neurones de dimensions plus petites. Cependant, cela ne garantit pas de meilleures performances. En effet, pour tous les modèles, les performances diminuent selon ce critère. Plus précisément, le calcul des scores moyens d'évaluation des méthodes par classe d'anomalies révèle une diminution de l'AUC-ROC de 2\ Ces expériences se concluent par une comparaison de la méthode Deep M-SPH SVDD avec celle de sur un sous-ensemble de CIFAR10 ({Automobile, Truck} vs "animaux" := {Bird, Horse, ...}). Les auteurs de semblent avoir utilisé la même architecture que Ruff et al. et ont obtenu un AUC-ROC de 0,663. La méthode proposée atteint un score de 0,71 en utilisant Deep M-SPH SVDD avec 1 hypersphère de dimension 128, paramètres non fixés. La différence notable entre les deux méthodes réside dans le fait que, pour Deep M-SPH SVDD, les paramètres de l'encodeur sont appris par le réseau. Cependant, une diminution des performances est observée avec un score de 0,65 lors de l'utilisation de 4 hypersphères en dimension 32. Ce phénomène est similaire à celui observé dans les ensembles de données de points dans \(^2\), ce qui peut expliquer cette baisse de performance. En effet, les courbes montrant les rayons (cf. figure et ) des hypersphères en fonction du nombre d'epochs selon le modèle utilisé indiquent qu'une seule hypersphère ne dégénère pas en un point lorsque ce modèle utilise plusieurs hypersphères. Cela implique que sur des données de haute dimension, le réseau utilise uniquement une sphère dont la dimension est par conséquent réduite.\\ L'observation des courbes de fonctions de pertes (cf. figure et ), montre que les modèles composé de plusieurs hypersphères convergent d'avantage vers 0. Mais cette convergence ne garantit pas une meilleure performance du modèle, si l'on compare les scores AUC-ROC respectifs des modèles. En revanche, on peut observer que pour les modèles dont le rayon est plus petit, c'est-à-dire ceux contenant plusieurs hypersphères, la { loss} est également plus petite. [H] [b]{0.475} [b]{0.475} (0,1)$ )} [H] [b]{0.475} [b]{0.475} La figure présente les histogrammes des distributions de sortie de Deep M-SPH SVDD pour différentes configurations de paramètres. Ici encore, nous avons cherché des configurations avec un nombre de paramètres comparable. Comme les histogrammes présentés dans la section , les distributions de sortie ne correspondent évidement pas à une gaussienne, mais on tendance à former un pic autour de 0. Cela indique que l'encodeur envoie les données d'entrée vers le bord de l'hypersphère.\\ Pour les données MNIST, les histogrammes entre les données d'entraînement et les données de test, se chevauchent, mais sur un petit intervalle. Ce n'est pas le cas avec les histogrammes des données CIFAR10, ce qui montre la difficulté à définir un seuil optimal pour des données plus complexes. (0,1)$} =0 =1 =2 =3 =4 =5 =6 =7 =8 =9 [H] [b]{0.45} =1$, , } [b]{0.45} =1$, ,} [b]{0.45} =1$, , } [b]{0.45} =1$, ,}