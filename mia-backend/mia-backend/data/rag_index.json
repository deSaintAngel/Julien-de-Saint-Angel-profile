{
  "chunks": [
    {
      "id": "annexea.tex.txt_chunk_1",
      "text": "Cette annexe détaille un certain nombre de calculs (notamment de covariance) qui nous permis de justifier notre proposition d'initialisation pour les couches hypersphériques. (2 x_i s_i - x_i^2 - s_{ji}^2) = (2 x_i s_{ji} - x_i^2) + (s_{ji}^2) + 2 (2 x_i s_{ji} - x_i^2, s_{ji}^2) Or, (2x_i s_{ij} - x_i^2, -s_{ij}^2) &= [(2x_i s_{ij} - x_i^2)(-s_{ij}^2)] - [2x_i s_{ij} - x_i^2] [-s_{ij}^2] \\\\ \\\\ &= -[2x_i s_{ij}^3] + [x_i^2 s_{ij}^2] + [2x_i s_{ij}] [s_{ij}^2] - [x_i^2] [s_{ij}^2] \\\\ \\\\ &= -2[x_i",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_2",
      "text": "] [s_{ij}^3] + [x_i^2] [s_{ij}^2] + 2[s_{ij}] [x_i] [s_{ij}^2] - [x_i^2] [s_{ij}^2] \\\\ \\\\ &= -2[x_i] [s_{ij}^3] + 2[s_{ij}] [x_i] [s_{ij}^2] \\\\ \\\\ &= 2_s _x (_s^2 + _s^2) - 2 _x (_s^3 + 3 _s _s^2) \\\\ \\\\ &= -4 _s _x _s^2 (2 x_i s_{ji} - x_i^2) = (2 x_i s_{ji}) + (-x_i^2) + 2 (2 x_i s_{ji}, x_i^2) Or, (2 x_i s_{ji}, -x_i^2) &= [(2 x_i s_{ji})(-x_i^2)] - [2 x_i s_{ji}] [-x_i^2] \\\\ \\\\ &= -2[s_{ji}] [x_i^3] + 2 [x_i][s_{ji}] [x_i^2] \\\\ \\\\ &= 2_s _x (_x^2 + _x^2) - 2 _s (_x^3 + 3 _s _x^2) \\\\ \\\\ &= -4 ",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_3",
      "text": "_s _x _x^2 Donc, (2 x_i s_{ji} - x_i^2 - s_{ji}^2) = (2 x_i s_{ji}) + (-x_i^2) + (s_{ji}^2) - 8 _s _x (_x^2 + _s^2 ) Les calculs précédents ont pris en compte uniquement des couches hypersphériques de type dense. Cependant, la plupart des réseaux sont constitués de couches convolutionnelles. Chaque élément du tenseur de sortie d'un neurone correspond au produit scalaire entre la version vectorisée de la sphère et une partie du tenseur d'entrée de taille identique au noyau de convolution. Il est ",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_4",
      "text": "donc possible de considérer des vecteurs constitués des parties vectorisées par blocs de l'entrée. En conséquence, pour une sphère donnée, contrairement au cas des couches de type dense où la sortie est un scalaire, on obtient un vecteur dont chaque élément provient du produit scalaire entre la sphère et les sous-vecteurs correspondants. \\\\ Pour les calculs suivants, on considère une couche convolutive où le tenseur d'entrée est de taille $(c,n)$ (un vecteur à $c$ canaux), la sortie est un tense",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_5",
      "text": "ur de taille $(m,d')$ où $m$ représente le nombre de sphères, et $d'$ le nombre de composantes issues du produit de convolution (on a $d'= n-d-1$, dans le cas d'une convolution sans padding, et $d'= n$ avec padding). Les éléments de la sortie sont donc les $y_{i'j}$ définis par $y_{i'j} = } _i }{}$.\\\\ Pour le produit entre un vecteur de composantes $}^{}$ et une sphère $}$ qui donne l'élément $y_{i'j}$ de la convolution, avec $d$ la taille du filtre tel que $i\\{1,,d\\}$ et $d'$ le nombre de compo",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_6",
      "text": "santes tel que $i'\\{1,,d'\\}, $ on obtient pour une sphère $j$ donnée l'écriture de la $i'$-ième composante ~:\\\\ $${ y_{i'j} = {2}}$$ L'élément $x_{i'ci}$ représente le $i$-ème élément pour le canal $c$ de la composante $}^{}$. On considère pour l'écriture suivante les composantes $p$ et $q$ pour la sphère $}$. Pour alléger la notation l'indice $j$ n'est pas mentionné. Le terme en $^2$ est intégré dans la double somme.\\\\ $$ y_p = _i = _{c=1}^{C}_{i=1}^{d} , p \\{1,,d'\\}$$\\\\ $$ y_q = _i = _{c=1}^{C",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_7",
      "text": "}_{i=1}^{d} , q \\{1,,d'\\}$$ On calcule alors : \\\\ { ${ll} & (y_p,y_q) = cov( , ) p<q = q-p ] \\\\ \\\\ & =( _{c=1}^{C} _{i=1}^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd}, _{c'=1}^{C} _{i\"=1}^{d}{{x_{qi\"c'}}} {{s_{i\"}}}-{2}{{x^{2}_{qi\"c'}}}-{2}{{s^{2}_{i\"c'}}}+}{2Cd})\\\\ \\\\ & =_{c=1}^{C}(_{i=1}^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{c'=1}^{C}_{i\"=1}^{d}{{x_{qi\"c'}}} {{s_{i\"c'}}}-{2}{{x^{2}_{qi\"c'}}}-{2}{{s^{2}_{i\"c'}}}+}{2Cd})\\\\ \\\\ \\\\ & =_{c=1",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_8",
      "text": "}^{C} ^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{i\"=1}^{d}{{x_{qi\"c}}} {{s_{i\"c}}}-{2}{{x^{2}_{qi\"c}}}-{2}{{s^{2}_{i\"c}}}+}{2Cd})}_{ c=c'}\\\\ \\\\ & ++_{c=1}^{C} ^{d}{{x_{pic}}} {{s_{ic}}}-{2}{{x^{2}_{pic}}}-{2}{{s^{2}_{ic}}}+}{2Cd},_{i\"=1}^{d}{{x_{qi\"c'}}} {{s_{i\"c'}}}-{2}{{x^{2}_{qi\"c'}}}-{2}{{s^{2}_{i\"c'}}}+}{2Cd})}_{ c c'} $}\\\\ On peut restreindre le calcul à la condition $c=c'$. En effet, on cherche à calculer la somme des covariances de termes independants deux à",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_9",
      "text": " deux. Cela implique que si on développe les termes de la somme pour $c c\"$, alors ils sont tous nuls. Donc pour $c=c'$ fixé on a~: \\\\ $$ (y_p,y_q) = _{c=1}^{C} (_{i=1}^{d}{{x_{pi}}} {{s_{i}}}-{2}{{x^{2}_{pi}}}-{2}{{s^{2}_{i}}}+}{2Cd},_{i=1}^{d}{{x_{qi}}} {{s_{i}}}-{2}{{x^{2}_{qi}}}-{2}{{s^{2}_{i}}}+}{2Cd})}_{cov(_i ,_i )_c}$$\\\\ Ainsi le calcul du terme cov$(y_p,y_q)_c$ donne~:\\\\ ${ll} (y_p,y_q)_c & = (_i ,_i )_c\\\\ \\\\ & = \\\\ \\\\ \\\\ & = \\\\ $\\\\ ${ll} (y_p,y_q)_c & = ^{ d}(x_{pi} s_i-{2}x^{2}_{pi}) ",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_10",
      "text": ",_{i=1}^{d}(x_{qi} s_i-{2}x^{2}_{ql})]}_{(1)}\\\\ \\\\ & + }_{(2)}\\\\ \\\\ \\\\ & }_{(3)}+ }_{(4)} $\\\\ ${rl} (4) : & s_i ^2 \\\\ & _s, s_{i} (_s,^{2}_{s}), ~:\\\\ \\\\ &{ccl} var(s^{2}_{i}) = -^{2} = 2 _s^2 (_s^2 + 2 _s^2 ) $\\\\ ${rl} & \\\\ \\\\ & {lll} & = & \\\\ \\\\ & = & (_{i=1}^{ d}-{2}s^{2}_{i})+ ({2cd})}_{0}+(_{i=1}^{ d}-{2}s^{2}_{i},{2cd})}_{0}\\\\ & = & {4}(s^{2}_{i}) s_i \\\\ \\\\ & = & {2} _s^2 (_s^2 + 2 _s^2 ) $\\\\ Les termes (2) et (3) étant symétriques, leur calcul est identique.\\\\ ${lll} & = & _{i=1}^{d}_{i=1}",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_11",
      "text": "^{d} \\\\ $ ${ll} = & _{i=1}^{d}_{i=1}^{d} $\\\\ {$ et les $s_i$, pour tout $i \\{1, , d\\}$, sont indépendants alors on a l'égalité [x^{2}_{pi}s^{2}_{i}]=[x^{2}_{pi}][s^{2}_{i}]$} qui permet d’établir que la covariance du terme donné est nulle.\\\\ }} ${ll} (*) & _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,-{2}s^{2}_{i})=_{i=1}^{d} $\\\\ $$ - = - $$\\\\ Ainsi, $$ =-d_s _x _x^2$$\\\\ En reprenant le terme (1), on a~:\\\\ $${ll} & \\\\ \\\\ = & _{i=1}^{d}_{i=1}^{d} \\\\ $$\\\\ {ll} = & _{i=1}^{d}_{i=1}^{d}\\\\ \\\\ Le terme est donc dé",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_12",
      "text": "composé en 4 termes que l'on calcule~: \\\\ $${ll} (B),(C) & _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,-{2}x^{2}_{qi})= _{i=1}^{ d}_{i=1}^{ d}(-{2}x^{2}_{pi},x_{qi} s_i)= 0.\\\\ \\\\ \\\\ & i \\{1,, d\\} \\ :\\\\ & (-{2} x_{pi} s_{i} x^{2}_{qi})-(x_{pi} s_{i})(-{2}x^{2}_{qi})\\\\ \\\\ &= (s_{i})(-{2}x_{pi}x^{2}_{qi})-(x_{pi})(s_{i}) (-{2}x^{2}_{qi})\\\\ \\\\ &= -{2}(s_{i})(x_{pi} ) (x^{2}_{qi})+{2}(x_{pi})(s_{i}) (x^{2}_{qi})\\\\ \\\\ & = 0\\\\ $$\\\\ Pour tout $i \\{1, , d\\}$, les $s_{i}$ sont indépendants des $x_{p_i}$ et des $x_{q_",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 12
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_13",
      "text": "i}$, ce qui implique que l'espérance des produits entre ces termes est égale au produit des espérances. Cela justifie que les termes $(B)$ et $(C)$ sont nuls.\\\\ Donc $$ (x_{pi} s_i,-{2}x^{2}_{qi}) =0 (x_{qi} s_i,-{2}x^{2}_{pi}) =0 $$\\\\ $(A)$ $$ les vecteurs $X_{p}$ et $X_{q}$, pour $p q$, correspondent à une partie de la forme vectorisée du vecteur d'entrée $X$, dont la taille est identique à la forme vectorisée du noyau de convolution. Ainsi les éléments des deux vecteurs $X_p$ et $X_q$ ont res",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 13
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_14",
      "text": "pectivement pour indices $p+k$ $q+k$ avec $k\\{0,,d-1\\}$. En posant $k' = k + 1$, il en résulte que~: $$X_p=\\{x_{pi}\\}_{i\\{1,,d\\}}=\\{x_{p+k'-1}\\}_{k'\\{1,,d\\}}$$ et $$X_q=\\{x_{qi}\\}_{i\\{1,,d\\}}=\\{x_{q+k'-1}\\}_{k'\\{1,,d\\}}$$\\\\ La double somme du terme A peut s'écrire de la façon suivante~:\\\\ $$_{k'=1}^{d}_{k'=1}^{d}(x_{p+k'-1} s_{k'},x_{{q}+k'-1} s_{k'})=_{k'=1}^{d}_{k'=1}^{d}(x_{p+k'-1} s_{k'},x_{{p+} +k'-1} s_{k'}) $$\\\\ ou bien encore~:\\\\ $$_{k'=1}^{d}_{k''=1}^{d}(x_{{p}+k'-1} s_{k'},x_{{p+} +k''",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 14
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_15",
      "text": "-1} s_{k''}) $$\\\\ En réarangeant les indices en fonction de $k'$ et $k''$, on obtient~:\\\\ $$_{k'={p}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1}) $$. \\\\ Pour continuer le calcul, il faut émetre l'hypothèse que $ d$ et est non nul. En effet dans le cas contraire, pour tout $k'\\{p,,p+d-1\\}$ et tout $k''\\{p+,,p++d-1\\}$, les $x_{k'}$, $x_{k''}$, $s_{k'-p+1}$ et $s_{k''-p-+1}$ sont tous distincts, et on peut donc justifier le passage des espérances des produits au produit ",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 15
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_16",
      "text": "des espérances de chaque terme et observer que la covariance des termes calculés est nulle. En effet, les $x_{k'}$ et $x_{k''}$ sont indépendants entre eux, et indépendants des $s_{k'-p+1}$ et $s_{k''-p+1}$. En développant les termes de la covariance, il apparaît que l'expression dépend uniquement des espérances des produits de ces termes.\\\\ $${lll} (x_{k'} s_{k'-p+1}, x_{k''} s_{k''-p-+1}) & = & \\\\ \\\\ & & - \\\\ \\\\ \\\\ & = & [x_{k'}] [x_{k''}] [s_{k'-p+1} s_{k''-p-+1}]\\\\ \\\\ & & - [x_{k'}] [s_{k'-p",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 16
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_17",
      "text": "+1}] [x_{k''}] [s_{k''-p-+1}]\\\\ \\\\ \\\\ & = & [x_{k'}] [x_{k''}] [s_{k'-p+1}] [s_{k''-p-+1}]\\\\ \\\\ & & - [x_{k'}] [s_{k'-p+1}] [x_{k''}] [s_{k''-p-+1}]\\\\ $$\\\\ Tous les produits d'espérance de ces termes se simplifient et donc sont nuls. Ainsi, chaque terme de la somme est nul, ce qui implique que la somme est aussi nulle.\\\\ Par linéarité de la covariance, on peut séparer la double somme selon les termes suivants~: $$ {p}}^{{p+-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})}_{=0}+",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 17
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_18",
      "text": " _{k'={p+}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1}) $$ $ et tout $k''\\{p+,,p+d-1\\}$, les $x_{k'}$ et $x_{k''}$ sont tous distincts. Un raisonnement analogue à l'étape précedente, permet de conclure que chaque terme de la somme est nul, donc la somme est nulle.} $$=_{k'=p+}^{p+d-1}_{k''={p+}}^{{p+d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})+ ^{p+d-1}_{k''={p+d}}^{{p++d-1}}(x_{k'} s_{k'-p+1},x_{k''} s_{k''-p-+1})}_{=0} $$ {$ et tout $k''\\{p+,,p++d-1\\}$, les $x_{k'}",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 18
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_19",
      "text": "$ et $x_{k''}$ sont tous distincts. Ainsi chaque terme de la somme est nul, donc la somme totale est nulle. }} Dans la double somme, les termes où $k' k''$ vont donner des $x_{k'}$ et $x_{k''}$ distincts, ce qui permet également de conclure que les termes de covariance calculés sont nuls. Il ne reste donc que les termes où $k'=k''$. Ce qui permet de simplifier la double somme sou la forme suivante~:\\\\ $$_{k'=p+}^{p+d-1}(x_{k'} s_{k'-p+1},x_{k'} s_{k'-p-+1})$$\\\\ Pour chaque terme de cette dernièr",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 19
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_20",
      "text": "e somme le calcul de la covariance donne~:\\\\ $${lll} (x_{k'} s_{k'-p+1},x_{k'} s_{k'-p-+1})& = & [x_{k'}^2] [s_{k'-p+1}] [s_{k'-p-+1}]\\\\ \\\\ & & - [s_{k'-p+1}] \\\\ \\\\ & = & ({_x^2 + _x^2}) _s^2 - _x^2 _s^2 $$ $$ _{i=1}^{d}_{i=1}^{d}(x_{pi} s_i,x_{qi} s_i)=(d-) _s^2 _x^2$$\\\\ Pour le calcul du terme $D$, le raisonnement sur les indices de sommation est similaire à ce qui a été effectué pour le calcul du terme $A$. \\\\ Comme énoncé précédemment, les éléments des deux vecteurs $X_p$ et $X_q$ ont respec",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 20
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_21",
      "text": "tivement pour indices $p+k$ et $q+k$ avec $k\\{0,,d-1\\}$. On fait l'hypthèse que $ d$, et on a $=q-p$. On pose $k' = k + 1$, il en résulte que~: $$X_p=\\{x_{pi}\\}_{i\\{1,,d\\}}=\\{x_{p+k'-1}\\}_{k'\\{1,,d\\}}$$ et $$X_q=\\{x_{qi}\\}_{i\\{1,,d\\}}=\\{x_{q+k'-1}\\}_{k'\\{1,,d\\}}$$. ${ll} (D) & = _{i=1}^{d}_{i=1}^{d}(-{2}x^{2}_{pi},-{2}x^{2}_{qi})\\\\ \\\\ & = _{k'=1}^{d}_{k''=1}^{d}(-{2}x^{2}_{p+k'-1},-{2}x^{2}_{q+k''-1}) \\\\ \\\\ & = _{k'=1}^{d}_{k''=1}^{d}(-{2}x^{2}_{p+k'-1},-{2}x^{2}_{p++k''-1}) \\\\ \\\\ & = _{k'={p}}^",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 21
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_22",
      "text": "{{p+d-1}}_{k''={p+}}^{{p++d-1}} (-{2}x^{2}_{k'},-{2}x^{2}_{k''})\\\\ \\\\ & = {p}}^{{p+-1}}_{k''={p+}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})}_{=0}+ _{k'={p+}}^{{p+d-1}}_{k''={p+}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})\\\\ \\\\ & = _{k'=p+}^{p+d-1}_{k''={p+}}^{{p+d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})+ ^{p+d-1}_{k''={p+d}}^{{p++d-1}}(-{2}x^{2}_{k'},-{2}x^{2}_{k''})}_{=0}\\\\ $ $$ {ll} (D) & = _{k'=p+}^{p+d-1} (-{2}x^{2}_{k'}, -{2}x^{2}_{k'}) x_{pi} (0,^{2}_{x})\\\\ \\\\ & = _{i=1+}^{d} (x_{k'}^2)}{4} ",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 22
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_23",
      "text": "= \\{ {l} {2} ^{2}_{x} (^{2}_{x} + 2 _x^2 ) (=q-p) \\{1, , d\\}, \\\\ 0 . $$\\\\ {_{k'}$ et $x^{2}_{k''}$ distincts. Grâce à l'hypothèse d'indépendance on a [x^{2}_{k'}x^{2}_{k''}]=[x^{2}_{k'}][x^{2}_{k''}]$}. Cela permet de conclure que les termes de covariance calculés sont nuls. Il ne reste donc que les termes où $k'=k''$.}} Reprenons , enfin, le calcul . Cette covariance correspond à la somme des termes suivants~: $$\\{ {cl} (1) : & (A = {(d-) _s^2 _x^2 }) + (B = {0} ) + (C = {0}) \\\\ \\\\ & +(D = {{2}",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 23
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_24",
      "text": " ^{2}_{x} (^{2}_{x} + 2 _x^2 )} \\{1, , d\\} \\\\ \\\\ (2) : & {-d _x _s _x^2} \\\\ \\\\ (3) : & {-d _x _s _x^2}\\\\ \\\\ (4) : & {{2} _s^2 (^{2}_{s} + 2_s^2)} . $$\\\\ soit donc~: $${ll} (y_p,y_q) &= \\{{l} _{c=1}^{C}{2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 ) \\{ 1,,d\\}. \\\\ \\\\ _{c=1}^{C}{2}_s^2 (_s^2 + 2 _s^2 -2 _x _s ) .\\\\ \\\\ &= \\{{l} C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 ) ) \\{1,,d\\}. \\\\ \\\\ C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )) .\\\\ $$\\\\ Pour le cas où",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 24
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_25",
      "text": " $^t$ est un tenseur de dimension $(m,n,c)$, c'est-à-dire une image à $c$ canaux, nommons $$ et $$ les indices de colonnes des éléments du tenseur $Y$ de sortie. On reprend $=q-p$ et on pose $ = -$. On peut rappeler que $d'$ représente le nombre de composantes dans la direction $i'$ telle que $i'\\{1,,d'\\}$. Si l'on considère un filtre de convolution de taille $d d$ alors, $d''$ représente le nombre de composantes dans la direction $i''$ telle que $i''\\{1,,d''\\}$. Le tenseur $Y$ de sortie est don",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 25
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_26",
      "text": "c constitué des éléments $\\{y_{i'i''j}\\}$. On suppose $ $. On considère donc pour une sphère $j$ fixée les composantes $y_{p}$ et $y_{q}$ telles que $ \\{1,,d''\\}$ et $ \\{1,,d''\\}$. Le nombre total de d'éléments pour la partie vectorisée d'une composante donnée sera alors $d'd''$. On a donc~:\\\\ $${ll} (y_{p},y_{q}) & {l} = \\{{l} C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )+{2}^{2}_{x} (_x^2 + 2 _x^2 + 2_s^2 )) \\\\ , \\{ 0,,d\\}. \\\\ \\\\ C({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s )) . \\\\ $$\\\\ On a finalement obtenu la m",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 26
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_27",
      "text": "atrice de covariance, dont la largeur de la bande dépendant du facteur $({2}_s^2 (_s^2 + 2 _s^2 -2 _x _s ))$ correspond à la dimension du noyau de convolution~: { $ (y_j) & _1+ _2 & _1 & & _1 \\\\ \\\\ _1 + _2 & (y_j) & _1 + _2 & & \\\\ \\\\ _1 & _1 + _2 & (y_j) & & _1 \\\\ \\\\ & & & & _1 + _2 \\\\ \\\\ _1 & & _1 & _1 + _2 & (y_j) $ } avec,\\\\ { \\{ {l} (y_j) = {2 ^2} (_s^2 + _x^2) ( 2(_x^2 - _s^2)^2 + 2_s_x + _s^2 + _x^2 ) \\\\ \\\\ _1 = C({2}_s^2 (_s^2 + 2 _s^2 - 2 _x _s ) ) \\\\ \\\\ _2 = C({2} _x^2 (_x^2 + 2 _x^2 + ",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 27
      },
      "embedding": []
    },
    {
      "id": "annexea.tex.txt_chunk_28",
      "text": "2_s^2 ) ) . } ${ll} \\\\ ({ ^{l}_{}} ) & = ( {}(S^{l+1}_{}- ^{l+1}_{}){ ^{l+1}_{}} ) \\\\ \\\\ \\\\ & = ( _{j'=1}^{m'}{}(s^{l+1}_{jj'}- x^{l+1}_{j}){ z^{l+1}_{j'}} )\\\\ \\\\ \\\\ & = m'( {}(s^{l+1}_{jj'}- x^{l+1}_{j}){ z^{l+1}_{j'}} )\\\\ \\\\ \\\\ & = {^2}\\\\ \\\\ \\\\ & = {^2}\\\\ $\n\n",
      "metadata": {
        "source_file": "annexea.tex.txt",
        "chunk_index": 28
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_1",
      "text": "Cette annexe reprend les différents éléments de la démonstration du théorème d'approximation classique pour une couche dense. Cette première étape correspond à la Proposition 3.3 de l'article de Pinkus . On suppose que si $f^0(^n)$ alors $f (,,)}$ pour des sous-ensembles $$ et $$ de $$ bien choisis, o\\`u $$ (,,) = \\{( x - )\\, :\\, ,\\, \\}.$$ \\\\ On choisit \\( A S^{n-1} \\), où \\( S^{n-1} = \\{ ^n : \\|\\|_2 = 1\\} \\), tel qu'il n'existe pas de polyn\\^ome homog\\`ene (non trivial) qui s'annule sur $A$. Al",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_2",
      "text": "ors, d'apr\\`es un r\\'esultat de Vostrecov et Kreine (voir Th. 3.2 dans ), l'espace $(A)$ engendré par les fonctions ``ridge\" continues $$ (A) = g() : A^n, g ^0(): $$ est dense dans $^0(^n)$ pour la norme uniforme sur les compacts. On a le résultat suivant: Soit~: $$(, A,) = \\{( -)~:w A, )\\}.$$ Sous les hypothèses précédentes, $(, A,)$ est dense dans $^{0}(^n)$ au sens de la convergence uniforme sur tout compact. Soit \\( f \\) une fonction à plusieurs variables continue sur un compact \\( ^n \\)~: $",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_3",
      "text": "${cccl} f~: & ^n & & \\\\ &(x_1,,x_n) & & f(x_1,,x_n)\\\\ $$ Cette fonction $f$ est la fonction à approcher. Soit $ >0$ donné quelconque. Comme $(A)$ est dense dans $^0()$ alors on a~:\\\\ $$ r , g_q^0(), ^q A~, \\, 1 q r \\,$$ f()-_{q=1}^{r} g_q(^q) {2} pour tout $x $. Étant donné que \\( \\) et que \\( \\) est un compact de \\( ^n \\), il en découle que \\( \\) est bornée. Comme $A S^{n-1}$, pour tout $q\\{1,,r\\}$, les $^q$ sont également bornées ($S^{n-1}$ étant la sphère unité, les $^q$ sont des vecteurs uni",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_4",
      "text": "taires). Ainsi $\\{^q ~: \\}$ est un ensemble borné. Donc pour tout $}, il existe des réels \\( _q \\) et \\( _q \\) tels que~: $$ \\{^q : \\} . $$ Comme $(,,)$ est dense dans $^0()$ pour $q\\{1,,r\\}$ alors~: $$ C_{qj},_{qj},_{qj}, j\\{1,,m\\} $$ g_q(t)-_{j=1}^{m}C_{qj}(_{qj}t-_{qj}) {2r} t [_q,_q]. \\\\ En injectant la formule dans , on obtient~: && f() - _{q=1}^{r}(_{j=1}^{m}C_{qj}(_{qj}-_{qj}) ) \\\\ && = f() -_{q=1}^{r} g_q(^q) + _{q=1}^{r} (g_q(^q) - _{j=1}^{m}C_{qj}(_{qj}-_j) ) \\\\ && f() -_{q=1}^{r} g_q(",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_5",
      "text": "^q) + _{q=1}^{r} g_q(^q) - _{j=1}^{m}C_{qj}(_{qj}-_j) \\\\ && {2}+r{r}=. On a approché $f$ dans un espace de type $(, A,)$ à une précision $$ donnée.\\\\ Ainsi, si $(,,)$ est dense dans ^0()$} au sens de la convergence uniforme sur les compacts, toute fonction continue sur un compact $$ de $^n$ peut \\^etre approchée par la sortie d'un réseau de neurones telle que définie en (2.1). Il reste certes à montrer que $(,,)}=^0()$, mais cette première étape nous a permis de nous ramener au cadre unidimensio",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_6",
      "text": "nnel. (,,)$ est dense dans $^0()$} La deuxième étape correspond à la Proposition 3.4 et au Corollaire 3.5 de l'article de Pinkus . Elle consiste à montrer que tout élément $f$ appartenant à l'ensemble des fonctions continue à support compact peut être approché par une combinaison linéaire d'éléments, de $(, ,)$ où $$ et $$ sont des sous-ensembles appropriés de $$. Soit $ ^{}()$, $$ non polynomiale. Soit $$ un sous-ensemble de $$ dont on peut extraire une suite tendant vers zéro. Soit $$ un inter",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_7",
      "text": "valle ouvert de $$. Alors, $(,,)$ est dense dans $^0()$. \\\\ On commence par démontrer que $$(,,)} = ^0().$$ Soit $f$ une fonction continue. On veut donc montrer que $f$ est approchable par une combinaison linéaire d'éléments de $(,,)$ : $$>0, C_j, _j, _j, $$ $$ f(x)-_{j=1}^{m_j} C_j (_j x -_j) $$ pour tout $x $ o\\`u $$ est un compact de $$. Par définition de l'adhérence, la limite du taux d'accroissement de la fonction $ ( x-)$ est dans l'adhérence de $(,,)$. En effet $${h} (,,)$$ et donc $$ D_{",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_8",
      "text": "=0} = _{{h 0, =0}}{h} (,,)}.$$ La quantité $D_{=0}$ correspond en fait à la dérivée première de $ ( x-)$. De même on peut vérifier que les dérivées successives d'ordre supérieur de cette fonction appartiennent à $(,,)}$. Comme $$ n'est pas un polynôme, aucune de ses dérivée n'est identiquement nulle. Donc, pour tout $k$, il existe un $_k$ tel que $^{(k)}(-_k) 0$ où $^{(k)}$ est la dérivée $k$-ième de $$. Et d'après ce qui précède, $$ {d^k} ( ( x-)) (0) = x^k ^{(k)}(-_k) (,,)}.$$ Ainsi , x^{k} (,",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_9",
      "text": ",)}$}. Comme n'importe quel polynôme s'écrit comme combinaison linéaire de monômes, et que tout les monômes sont dans $(,,)}$ alors, tout les polynômes sont dans $(,,)}$.\\\\ D'après le théorème de Weierstrass, toute fonction \\( f ^0(K) \\) peut être approchée par une fonction polynomiale \\( p \\) à coefficients réels en norme uniforme. On vient de voir que tout polynôme est dans $(,,)}$. On en conclut que $f (,,)}$. On vient de montrer que $$(,,)} =^0()$$ au sens de la convergence uniforme. Le coro",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_10",
      "text": "llaire 3.5 dans Pinkus montre qu'on peut en fait remplacer $(,,)$ par $(,,)$ où $$ et $$ vérifient les conditions données au chapitre 2 dans le théorème auquel cette annexe est consacrée.\\\\ On veut maintenant démontrer que si $ ^0()$ (et non plus $ ^()$ comme à l'étape précédente), on a toujours $$(,,)} =^0().$$ Soit donc $$ une fonction continue. On sait qu'elle peut être régularisée par convolution (voir par exemple la démonstration de la proposition IV.21 dans ). Précisément, pour toute fonct",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_11",
      "text": "ion $ ^()$ à support compact, on peut construire la fonction $_ ^()$ telle que $$ _( x - ) = _{-}^ ( x - -y)(y)\\, dy.$$ En écrivant l'intégrale comme une somme de Riemann, on constate que $ _( x - ) (,,)}$. Une conséquence est que $$ (_,,)} (,,)} .$$ On raisonne alors par l'absurde. Si on suppose $(,,)} ^0()$, on ne peut en particulier pas approcher par des éléments de $(_,,)$ les fonctions continues. D'après l'étape 2 $_$ est donc nécessairement un polynôme et ce pour tout $ ^()$ à support comp",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "annexeb.tex.txt_chunk_12",
      "text": "act. Ce n'est possible que si $$ est un polynôme, cas exclu du théorème qu'on est en train de démontrer. C'est donc impossible. On a encore $(,,)} =^0()$. Le théorème est démontré.\n\n",
      "metadata": {
        "source_file": "annexeb.tex.txt",
        "chunk_index": 12
      },
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_1",
      "text": "Cette petite annexe vise \\`a observer le phénomène de collapse mentionné par (voir aussi paragraphe ) qui a pour conséquence la n\\'ecessité de fixer les paramètres de l'hypersphère recherchée. La structure de l'encodeur $_W$ donnée dans la figure . Dans les expérimentations à suivre, on considère une sphère en dimension 64 pour laquelle les hyper-paramètres ont été fixés comme $= 0.1$ et $ = 0.0003321558199348189$. La fonction de coût a optimiser est celle de la Deep SVDD. \\\\ [hbp] [node distanc",
      "metadata": {
        "source_file": "annexec.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_2",
      "text": "e=, scale=] (input) [rectangle, draw, rotate=90] { Input $(2)$}; (linear1) [rectangle, right of=input, draw, rotate=90] { Linear $(64)$}; (bn1) [rectangle, draw, right of=linear1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (linear2) [rectangle, draw, right of=lrelu1, rotate=90] { Linear $(64)$}; (bn2) [rectangle, draw, right of=linear2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (linear3) [rectangle, ",
      "metadata": {
        "source_file": "annexec.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_3",
      "text": "draw, right of=lrelu2, rotate=90] { Linear $(64)$}; [->] (input) -- (linear1); [->] (linear1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (linear2); [->] (linear2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (linear3); On s'intéresse tout d'abord cas où le centre et le rayon de la sphère ne sont pas optimisés ; le rayon est fixé à 0 et le centre est fixé à une valeur arbitraire (Deep soft-boundary SVDD). La figure montre l'évolution du rayon de l'hypersphère, de la moyenne de la no",
      "metadata": {
        "source_file": "annexec.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_4",
      "text": "rme de $_W(x)$ au cours des itérations et l'histogramme des $^2 - ||_{W^*}() - ||^2$ à la fin de l'algorithme.\\\\ [htpb] {0.26} {0.26} $} {0.44} () - ||^2$} $ fixés} On observe dans les sous-figures et que l'encodeur $_W$ apprend à projeter les points d'entrée $$ sur le centre de l'hypersphère puisque la norme de $_W(x)$ converge vers une valeur non nulle et la distribution de $-||_{W^*}() - ||^2$ est autour de $0$.\\\\ On se place maintenant dans le cas où le centre de l'hypersphère est fixé à une",
      "metadata": {
        "source_file": "annexec.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_5",
      "text": " valeur arbitraire et le rayon est fixé à 1. La figure montre les mêmes éléments que la figure mais pour le cas où le rayon de l'hypersphère est fixé à 1.\\\\ [htpb] {0.26} {0.26} $} {0.44} () - ||^2$} $ fixés} La méthode correspondante à cette expérimentation n'est pas la méthode SVDD car il n'y a pas d'optimisation concernant le rayon. L'encodeur $_W$ a placé 90\\ On considère maintenant le cas où les paramètres de l'hypersphère sont libres, c'est-à-dire que le centre et le rayon de l'hypersphère",
      "metadata": {
        "source_file": "annexec.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_6",
      "text": " sont optimisés pendant l'entraînement. La figure montre l'évolution du rayon de l'hypersphère (initialisé à 0), de la moyenne de la norme de $_W(x)$ au cours des itérations et l'histogramme des $^{*2} - ||_{W^*}() - ^*||^2$ après entraînement.\\\\ [htpb] {0.26} {0.26} $} {0.44} - ||_{W^*}() - ^*||^2$} $ libres} On observe dans que le rayon reste constant à 0 alors que $$ converge lentement vers une valeur fixe supérieure à 0. La sphère a donc dégénéré et les scores $||_{W^*}() - ^*||^2$ pour les ",
      "metadata": {
        "source_file": "annexec.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "annexec.tex.txt_chunk_7",
      "text": "données d'entraînement et surtout de test sont toutes proches de $0$. La méthode est donc inutilisable pour la détection d'anomalie.\\\\ Enfin, si on initialise le rayon de l'hypersphère à 1 tout en laissant les paramètres $$ et $$ libres, on observe le même phénomène de dégénérescence. Cela est illustré dans la figure .\\\\ [htpb] {0.26} {0.26} $} {0.44} - ||_{W^*}() - ^*||^2$} $ libres}\n\n",
      "metadata": {
        "source_file": "annexec.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_1",
      "text": "Ce chapitre est consacré à la construction de réseaux de neurones à couches hypersphériques. Leur définition est inspirée du modèle conforme défini par Hestenes et al. . Les neurones définissent des hypersphères en dimension \\(n\\), paramétrées par un vecteur de l’algèbre géométrique conforme \\(R^{n+1,1}\\) (les poids des neurones correspondent aux paramètres de l’hypersphère). Une approche similaire pour les couches denses a déjà été proposée par Banarer et al. . Ici, nous allons plus loin en int",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_2",
      "text": "roduisant également un modèle convolutif de couches hypersphériques.\\\\ L’implémentation de ces nouveaux types de couches, en utilisant des opérateurs d’algèbre linéaire ou de convolution classiques, est décrite en détail, ainsi que les contraintes pour la mise à jour des poids. Les premières expérimentations sur des données synthétiques et des bases d’images montrent que les variantes hypersphériques des couches denses et convolutives améliorent le comportement de la fonction de coût et permette",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_3",
      "text": "nt une convergence plus rapide, à nombre de paramètres égal. Cependant, ces couches se révèlent parfois plus sensibles à l’initialisation, ce qui peut entraîner une certaine instabilité. Nous nous penchons donc sur la question de l'initialisation. On montre d'abord pourquoi l'approche heuristique de Glorot et Bengio et donc la normalisation classique associée s'avèrent inefficaces dans le cadre des couches hypersphériques. Il faut donc comprendre comment une distribution suivant une loi de proba",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_4",
      "text": "bilité complexe se propage dans le réseau hypersphérique. Nous mettons alors en place une stratégie d'initialisation basée sur des propriétés asymptotiques des lois de type Gamma généralisées. La pertinence de la méthode est confirmée par les expérimentations numériques. Les réseaux de neurones \"classiques\" reposent sur un produit scalaire $ $ (appliqué localement dans le cas convolutif), où $$ représente l'entrée et $$ le vecteur de poids$ est augmenté pour intégrer le biais dans le vecteur des",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_5",
      "text": " poids: $ +b = (,b) (,1)$.} (ou le filtre de convolution). Pour un vecteur $$ de taille $n$, le nombre de paramètres de la couche est $n+1$. Une fonction d'activation peut ensuite être ajoutée. Si on utilise une activation \"\", la sortie de chaque couche partitionne alors $^n$ en deux sous-espaces, séparés par un hyperplan (plus généralement, selon le choix de la fonction d'activation, les valeurs positives du produit scalaire sont conservées, tandis que les valeurs négatives sont atténuées ou an",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_6",
      "text": "nulées). \\\\ L'idée développée dans cette thèse est de faire une partition de $^n$ par des hypersphères plutôt que par des hyperplans. On note que la partition induite dans ce cas est faite entre deux sous-espaces dont l’un est compact. \\\\ Les hypersphères pourraient être paramétrées de manière classique, si bien qu'à l'étape d'apprentissage, l’optimisation des centres et des rayons serait distincts. Cette méthode serait donc similaire à ce qui est classiquement utilisé pour les fonctions à base ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_7",
      "text": "radiale (réseaux dits RBF). Bien que cette approche soit valide, elle occulte le lien géométrique entre hyperplan et hypersphère : un hyperplan peut être considéré comme une hypersphère dont le centre est situé à l'infini et dont le rayon est infini. Pour répondre à ce besoin d'unification, nous adoptons dans la suite le formalisme des algèbres géométriques conformes, dans lequel hyperplans comme hypersphères sont représentés par un vecteur de dimension $n+2$. Dans ce cadre mathématique, Banarer",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_8",
      "text": " et al. ont déjà défini un modèle de neurone hypersphérique qui constitue la base de ce travail. Nous allons l'incorporer dans des réseaux à couches denses ou convolutives. Mais nous commençons par quelques rappels sur le formalisme de l'algèbre géométrique conforme. On décrit ici des éléments concernant les algèbres géométriques et le modèle conforme. Le vocabulaire est mathématique mais, il faut garder à l'esprit que l'idée derrière la mobilisation des outils algébriques qui suivent est de fou",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_9",
      "text": "rnir une représentation uniforme et ''simple'' des hyperplans et des hypersphères. Nous considérons l'espace euclidien $^n$, l'espace vectoriel sous-jacent $ R^n$ étant muni d'une base orthonormale $(e_1,,e_n)$. La construction suivante explique en particulier par l'utilisation de la projection stéréographique pourquoi le modèle d'espace auquel on aboutit est qualifié de conforme. On commence par plonger l'espace euclidien dans un espace de dimension $n+1$ au moyen de l'inverse d'une projection ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_10",
      "text": "stéréographique : { x}=x_1 e_1 + + x_n e_n {2(x_1 e_1 + + x_n e_n) x_1^2 + + x_n^2 + 1}+ {x_1^2 + + x_n^2 - 1 x_1^2 + + x_n^2 + 1} e_+, où $e_+$ désigne le point à l'infini. Ensuite on homogénéise le résultat obtenu en ajoutant le vecteur $e_-$. Ceci donne le plongement de $^n$ défini par : { x}=x_1 e_1 + + x_n e_n x_1 e_1 + + x_n e_n+{(x_1^2 + + x_n^2) 2}(e_++e_-)+{(e_--e_+) 2}\\\\ { x} { x}+{(x_1^2 + + x_n^2) 2}(e_++e_-)+{(e_--e_+) 2}= x L'intérêt de ce plongement est qu'il va permettre de consi",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_11",
      "text": "dérer les objets de base de la géométrie euclidienne comme des vecteurs. Pour rendre compte de cette idée, il est nécessaire d'introduire un peu de vocabulaire mathématique. Tout d'abord, on considère sur l'espace $^{n+1,1}$ muni de la base formée des vecteurs $e_1$,$$, $e_n$, $e_+$, et $e_-$ la forme quadratique Q_{n+1,1}=({ccccc}1 & 0 & & 0 & 0 \\\\0 & 1 & & 0 & 0 \\\\ & & & & \\\\0 & 0 & & 1 & 0 \\\\0 & 0 & & 0 & -1). Au couple $(^{n+1,1}, Q_{n+1,1})$, on associe l'algèbre géométrique conforme qui, f",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_12",
      "text": "ormellement, est le quotient de l'algèbre tensorielle de $^{n+1,1}$ par l'idéal bilatère engendré par les éléments du type $a a-Q_{n+1,1}(a)$. On dispose alors d'un produit, dit géométrique, tel que pour tout vecteur $a$ de l'algèbre géométrique conforme, on a aa=a^2=Q_{n+1,1}(a). Plus généralement, le produit géométrique de deux vecteurs $a$ et $b$ s'écrit ab = {2}(ab + ba)}_{}+ {2}(ab - ba)}_{} avec {{2}(ab + ba)}=a._i b=b._i a= B_{n+1,1}(a,b) $B_{n+1,1}$ étant la forme bilinéaire symétrique a",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 12
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_13",
      "text": "ssociée à $Q_{n+1,1}$, et {{2}(ab - ba)}=a b=-b a. Traditionnellement, le produit $a._i b$ est appelé produit interne, il s'agit d'un réel. Le produit extérieur $a b$ est un bivecteur qui peut s'interpréter comme l'analogue en dimension deux d'un vecteur, { i.e.} ``un morceau'' de plan vectoriel muni d'une orientation. Le bivecteur E=e_+e_-=e_+ e_- détermine une décomposition dite conforme ^{n+1,1}=^{n}^{1,1} où $$ désigne la somme directe et $^{1,1}$ le plan de Minkowski. Plus précisément, si n",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 13
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_14",
      "text": "ous notons $e_=e_++e_-$, $e_0=(e_--e_+)/2$ (voir Li { et al} ), et a= a_1 e_1 + + a_n e_n + a_ e_ + a_0e_0, un vecteur générique de $^{n+1,1}$, on peut décomposer $a$ en a=_E(a)+_E^{}(a), où $_E$ est la projection définie par _E(a)=(a._i E)E=a_ e_+a_0e_0, et $_E^{}$ est la réjection définie par _E^{}(a)=(a E)E=a_1 e_1 + + a_n e_n permet de retrouver la partie euclidienne de $a$. \\\\ Pour définir la projection $_E$ et la réjection $_E^{}$, il faut utiliser le produit interne et le produit extérieu",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 14
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_15",
      "text": "r entre un vecteur et un bivecteur. Comme nous ne les utiliserons pas par la suite, nous omettons ici les détails. On vérifie facilement que e_{0}^2 = e_{}^2 =0,~~e_{}._i e_0 =-1 Désignons par $ P^{n+1}(e_,e_0)$ l'hyperplan de $ R^{n+1,1}$ normal à $e_$ et contenant $e_0$, donné par l'équation e_._i(a-e_0)=0, et par $ N^{n+1}$ le cône nul de $ R^{n+1,1}$ donné par l'équation a^2=a._i a=0. La représentation conforme de l'espace euclidien $^n$ associée à la décomposition conforme donnée par $E=e_+",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 15
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_16",
      "text": " e_-=e_ e_0$, est l'horosphère H^n(e_, e_0)= N^{n+1} P^{n+1}(e_,e_0)=\\{a R^{n+1,1},\\ a^2=0,\\ e_(a-e_0)=0\\}. Dans la suite, nous noterons $ x$ le vecteur de $ R^{n+1,1}$ image du plongement du point ${ x}$ de l'espace euclidien $ R^n$ dans cette horosphère, { i.e.} x={ x}+{{ x}^2 2}e_+e_0. En notant $$ et $$ deux points de l'espace euclidien $ R^n$, on a en particulier, ._i =(+{2}^2 e_{} + e_0)._i(+{2}^2 e_{} + e_0)= + {2}^2 ._ie_{}}_{0}+ ._ie_0}_{0}\\\\ +{2}||||^2 (._ie_{}}_{0} + {2}^2 ._ie_{}}_{0",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 16
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_17",
      "text": "} + ._ie_{0}}_{-1}) + ._i}_{0} + {2}^2 ._ie_{}}_{-1} + ._ie_{0}}_{0}\\\\ = -{2}-^2 Cette formule permet de comprendre comment définir une hypersphère de $ R^n$ comme un vecteur de l'algèbre géométrique conforme et comment tester par un produit intérieur l'appartenance d'un point à cette hypersphère. Soit donc une hypersphère de centre $$ et de rayon $$, { i.e.} l'ensemble des $$ vérifiant~: -^2 = ^2. Avec les notations introduites précédemment, cette équation peut s'écrire sous la forme : ._i = -{",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 17
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_18",
      "text": "2}- ^2=-{2}^2. On en déduit qu’en utilisant $._i e_{}=-1$ : ._i = -{2} ^2 ._i - {2} ^2 = 0 \\\\ ._i - (-{2}^2)(-1)=0 ._i -({2}^2)(x._i e_{})=0 \\\\ ._i -{2}^2e_{})}_{}=0 ._i = 0, \\\\ avec = + e_0 + {2} ^2 e_{} - {2}^2 e_{} Par conséquent, dans l'algèbre géométrique conforme, l'hypersphère de centre $$ et de rayon $$ correspond au vecteur $$ de $^{n+1,1}$ mentionné précédemment. On constate par ailleurs que deux contraintes de normalisation doivent être satisfaites : {l} ^2 = ^2 >0,\\ \\ e_{}._i = -1. L",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 18
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_19",
      "text": "es propriétés que l'on retient pour la suite sont les suivantes : le point ${ x}$ est à l'intérieur de l'hypersphère si $$ 0 < ._i {2} ^2,$$ sur l'hypersphère si $$._i = 0,$$ à l'extérieur de l'hypersphère s'il vérifie $$._i < 0.$$ En particulier, ._i ={2}^2 x= c . Les figures et illustrent les variations du produit $._i $ pour une sphère de centre $(0, 0)$, de rayon égal à 5. [htbp] [height=10cm, width=10cm, grid= major , xlabel = {$x_0$} , ylabel = { $._i $} , xmin=-10,xmax=10, legend entries=",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 19
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_20",
      "text": "{${2}^2$}, ] coordinates {(0,12.25)}; coordinates {(-10,0) (10,0)}; table [x=a, y=b,col sep=comma] {sources/ProduitScalaire.csv}; ._i $} [H] ._i $} Il est possible de même de représenter l'hyperplan de $ R^n$ de vecteur normal unitaire $ n$ et passant par le point ${ a}$ par le vecteur $ h$ de l'algèbre géométrique conforme qui s'écrit h = n+ e_, avec $ = n$. On vérifie facilement que x._i h= n({ x}-{ a}). Si $x$ appartient à l'hyperplan représenté par $$ alors $._i=0$. Grâce à cette description",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 20
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_21",
      "text": ", on peut vérifier que l'hypersphère représentée par le vecteur s=({ c}+ { n})+{( c+ n)^2 2}e_+e_0-{( +)^2 2}e_ { i.e.} l'hypersphère de centre ${ c}+ { n}$ et de rayon $ +$ tend vers l'hyperplan $ h$ précédent lorsque $>0$ tend vers l'infini. En effet, par la renormalisation e_._i s=-1/, l'hypersphère correspond au vecteur s= {( c+ n) }+{( c+ n)^2 2}e_+{e_0}-{( +)^2 2}e_\\\\ =({ c}+ n)+({{ c}^2 2}+{ c} { n}+{ 2}-{ 2}--{^2 2})e_+{e_0}. Quand $>0$ tend vers l'infini, alors ce vecteur tend vers h= n",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 21
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_22",
      "text": "+( c n-)e_, avec a= c- n,\\ \\ n( c- n)= c n-. Ce type de calcul, donné en exemple, illustre bien la cohérence des définitions et des opérations du modèle conforme d'Hestenes. Les modèles de réseaux à couches hypersphériques se composent d'une ou plusieurs hypersphères, dont les paramètres, représentant le centre et le rayon, sont ajustés au cours de l'entraînement pour optimiser la séparation des données. Sans ajout de fonction d'activation, ces couches permettent de créer des frontières décision",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 22
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_23",
      "text": "nelles non linéaires, contrastant ainsi avec les couches traditionnelles. Dans les réseaux de neurones classiques, les couches sont généralement de type dense ou à convolution. Les couches de type dense permettent de réaliser des séparations complexes dans l'espace des caractéristiques, tandis que les couches à convolution appliquent ce principe de manière localisée, ce qui est particulièrement utile pour le traitement d'images. Nous montrons ici comment construire l'analogue de ces deux types d",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 23
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_24",
      "text": "e couches à partir du modèle conforme décrit précédemment. Une couche hypersphérique a été développée par en utilisant des hypersphères représentées par des vecteurs de \\(^{n+1,1}\\). Le schéma illustre un réseau de neurones avec une couche cachée utilisant une hypersphère. La première étape, intégrée dans l'implémentation de la couche hypersphérique, consiste à envoyer les données d'entrée dans l'espace de l'algèbre géométrique conforme en utilisant le plongement décrit par . Ensuite, il est néc",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 24
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_25",
      "text": "essaire de prendre en compte la contrainte d'unicité, qui peut être considérée selon plusieurs méthodes décrites dans la section . La sortie de la couche pour une hypersphère de centre $$ et de rayon $$ s'écrit~: y = -{2} Le signe de \\(y\\) détermine la position relative du point \\(\\) par rapport à l'hypersphère, conformément aux propriétés établies précédemment. Cette quantité est donc calculée par le produit interne $_i $. [htbp] [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 25
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_26",
      "text": "pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!120]; =[neuron, fill=green!100]; / in {1,...,2} (K-) at (0,-+1/2) {$x_{}$}; (K-3) at (0,-3+1/2) {$1$}; (K-4) at (0,-4+1/2) {${2}$}; / in {0} (J-) at (0,-1.75) {$y = ._i$}; / in {0} (K-1) edge (J-); / in {0} (K-2) edge (J-); / in {0} (K-3) edge[dashed,->,red] (J-); / in {0} (K-4) edge[dashed,->,red] (J-); On peut noter que la définition du produit interne induit les différences suivantes",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 26
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_27",
      "text": " sur le paramétrage des couches hypersphériques~: Couche dense ($n+1$ paramètres) : le produit $ $ est linéaire en $x$ et n'est pas borné Couche sphérique ($n+1$ paramètres) : le produit $ ._i $ est quadratique en les coordonnées de $$ et est majoré par $^2/2$ Comme expliqué précédemment, les paramètres du vecteur conforme représentant l'hypersphère \\(\\) sont ajustés par l'apprentissage. Ses paramètres encodent à la fois le centre (les $n$ premières coordonnées) et une forme implicite du rayon à",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 27
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_28",
      "text": " travers la composante en $e_{}$ qui sont ajustées par le processus d'apprentissage. L'extension du modèle de neurone hypersphérique à un filtre de convolution hypersphérique ne nécessite pas de nouveaux outils mathématiques. Cette extension sera décrite pour un filtre de convolution 2D discret, applicable au traitement des images ; toutefois, l'approche reste valable en toute dimension, indépendamment du mode de et des valeurs de choisies. La convolution standard est bien connue pour être équiv",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 28
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_29",
      "text": "alente au produit scalaire entre les versions vectorisées du bloc image et du filtre \\( F \\) auquel est ajouté un biais~: (I * F)[j,j']= _{d_j} _{d_{j'}} I[j+d_j, j'+d_{j'}] * F[d_j, d_{j'}] + b \\\\ = (I[j-d_j:j+d_j, j'-d_{j'}:j'+d_{j'}]) (F[-d_j:d_j, -d_{j'}:d_{j'}])+ b . où $d_j$ et $d_{j'}$ désignent les indices de parcours du filtre qui dépendent de la taille du filtre $d d$. De façon naturelle, on peut voir vec$(I[j-d_j:j+d_j, j'-d_{j'}:j'+d_{j'}])$ comme un vecteur $x_I(j,j')$ de $^{d^2}$ q",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 29
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_30",
      "text": "ue l'on va plonger dans l'algèbre géométrique conforme $^{d^2+1,1}$. Le filtre de convolution est remplacé par une sphère en dimension $^{d^2}$ paramétrée dans cette même algèbre. Si l'on note $$ cette \"nouvelle\" convolution, on en définit le résultat comme suit~: (I s)[j,j'] = x_I(j,j') ._i s La valeur de sortie de ce filtre possède donc les mêmes propriétés que le produit interne $._i$ défini dans la sous-section précédente (voir figure ). L'extension de ce filtre aux images à $c$-canaux ne po",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 30
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_31",
      "text": "se pas de difficultés. Dans cet esprit, on envisagera deux approches, ``Conv2d'' et ``DepthWiseConv2d'' (dans le cas où une hypersphère est définie par canal)~:\\\\ $(I s)[j,j'] = _c x_I(j,j',c) ._i s$ ($$ Conv2d)\\\\ $(I s)[j,j'] = _c x_I(j,j',c) ._i s_c$ ($$ DepthWiseConv2d) [H] Dans la base $\\{e_1, e_2,,e_n, e_0, e_{}\\}$, le vecteur $$ et la sphère $s$ de centre $$ et de rayon $$ s'expriment comme des vecteurs à $n+2$ coordonnées~: & := [x_1, x_2, , x_n, 1, {2} ||||^2]^{t}\\\\ (, ) & := [c_1, c_2, ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 31
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_32",
      "text": ", c_n, 1, {2} (||||^2-^2)]^{t} \\\\ Pour garantir l'efficacité du calcul du produit interne $._i$, celui-ci va être exprimé comme un produit matriciel $^{t} M $. On rappelle que les règles de calcul suivantes doivent être vérifiées : \\{\\ {l r} e_j ._i e_{j'} = _{jj'}, & j,j' \\{1,,n\\}\\\\ e_j ._i e_{0} = e_j ._i e_{} = 0\\\\ e_{} ._i e_0 = e_0 ._i e_{} = -1 . avec \\[ _{jj'} = 1 & j = j', \\\\ 0 & . \\] On peut vérifier que la matrice $M$ qui traduit les relations données dans le système s'écrit~:\\\\ M =( {",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 32
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_33",
      "text": "cccccc} 1 & 0 & & 0 & 0 & 0\\\\ 0 & 1 & & 0 & 0 & 0\\\\ & & & 0 & 0 & 0\\\\ 0 & 0 & & 1 & 0 & 0\\\\ 0 & 0 & & 0 & 0 & -1\\\\ 0 & 0 & & 0 & -1 & 0\\\\ ) \\\\ On peut alors vérifier (voir les calculs ci-dessous) les deux propriétés suivantes pour le produit de deux points ou d'un point et d'une hypersphère~:\\\\ ^{t} M = {2}(^2 - || - ||^2) ^{t} M = -{2}|| - ||^2 \\\\ Vérification : produit $ ._i $ entre deux points : ${lcl} ^{t} M & = & (^t) M \\\\ & = & (^t, 1 , {2} ^2) ( {ccc} Id_{n} & 0 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & -",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 33
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_34",
      "text": "1 & 0 ) ({c} \\\\ 1\\\\ {2} ^2 ) \\\\ & = & (^t, -{2} ^2, -1) ({c} \\\\ 1\\\\ ^2 )\\\\ \\\\ & = & ^t - {2} ( ^2 - ^2)\\\\ \\\\ & = & -{2} - ^2 $ Vérification : produit entre un point et une hypersphère : ${lcl} ^{t} M & = & (^t, 1 , {2} ^2) ({ccc} Id_n & 0 & 0\\\\ 0 & 0 & -1\\\\ 0 & -1 & 0 )({c} \\\\ 1\\\\ ^2 - ^2}{2} ) \\\\ & = & (^t, -{2} ^2, -1) ({c} \\\\ 1\\\\ ^2 - ^2}{2} )\\\\ \\\\ & = & {2}(^2 - - ^2) $ Pour le filtre hypersphérique, le calcul de $^{t} M $ peut être divisé en trois étapes distinctes : La convolution standard",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 34
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_35",
      "text": " de l'image avec le filtre correspondant aux $n$ premières coordonnées de $$ redimensionnées à la taille $d d$. L'opération $-{2}||x||^2$ revient à faire la convolution de l'image par un filtre constant de valeur $-{2}$ et de taille $d d$. Le résultat est multiplié par l'avant-dernière coordonnée de $$ qui vaut 1. Une multiplication terme à terme est effectuée pour obtenir le tenseur contenant les coefficients correspondant au produit entre la composante $e_0$ de $$ et la composante $e_{}$ de $$",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 35
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_36",
      "text": ". Le résultat de la convolution hypersphérique est la somme de ces trois termes. Par ailleurs, si l'image traitée contient plusieurs canaux, une nouvelle somme selon les canaux de profondeur permet de reproduire le comportement du réseau de type Conv2D. Une hypersphère est représentée de manière unique par un vecteur \\(\\) dont l'avant-dernière coordonnée est fixée à 1, conformément à la contrainte \\( e_{} = -1\\). Pour optimiser les paramètres de l'hypersphère, c'est-à-dire les coordonnées de son",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 36
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_37",
      "text": " centre \\(\\) et la valeur de son rayon \\(\\), tout en respectant cette contrainte, plusieurs approches peuvent être envisagées :\\\\ Une première méthode consiste à effectuer la descente du gradient directement sur les $n+2$ coordonnées puis normaliser le vecteur résultat pour garantir \\( e_{} = -1\\). La normalisation consiste à diviser le vecteur dans la base $\\{e_1, e_2,,e_n, e_0, e_{}\\}$ par la composante en $e_0$. \\\\ Une autre approche fixe la composante \\(e_0\\) de \\(\\) à 1, et l'optimisation e",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 37
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_38",
      "text": "st alors effectuée uniquement sur les \\(n+1\\) autres éléments du vecteur. Cette méthode simplifie l'optimisation en réduisant le nombre de paramètres à ajuster, tout en maintenant la contrainte nécessaire.\\\\ C'est la méthode qui a été privilégiée dans les codes utilisés pour cette thèse.\\\\ Enfin, une perspective possible est de développer une méthode de descente de gradient qui opère directement dans l'espace des sphères, en garantissant que le produit \\(e_0 e_{}\\) reste égal à -1 tout au long d",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 38
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_39",
      "text": "u processus d'optimisation. Cela permettrait de maintenir la contrainte \\(s ._i e_{} = -1\\) de manière plus intégrée, évitant ainsi la nécessité de re-normalisation après chaque mise à jour. Dans ce paragraphe, nous évoquons deux autres familles de réseaux dont les noms laissent entrevoir une certaine parenté avec ceux manipulés dans cette thèse : les réseaux de neurones à fonctions à base radiale (RBF) et les réseaux de neurones de Clifford. Les fonctions à base radiales (RBF) sont en particuli",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 39
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_40",
      "text": "er bien connues pour leur capacité à approcher des fonctions non linéaires. Ce sont des fonctions dont la valeur dépend uniquement de la distance entre un point d'entrée et un centre fixe. L'approximation d'une fonction $f$ donnée par un réseau RBF avec $J$ fonctions radiales prend donc la forme~: f(x) _{j=1}^J w_j _j(|| - _j||) avec $_j : ^+ $. Un exemple classique est celui du noyau gaussien, $_j(|| - _j||) = e^{-_j^2|| - _j||^2}$ où $_j$ est un paramètre d'échelle s'apparentant au rayon d'une",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 40
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_41",
      "text": " sphère. Le lien avec les réseaux de neurones à couches hypersphériques sera davantage détaillé dans la section , en particulier dans le cas $n=1$ où une formulation analytique des sorties d'un réseau à une couche hypersphérique permet de clairement discerner les différences ou les similarités avec celles d'un réseau RBF. L'algèbre géométrique conforme du modèle d'Hestenes étant un cas particulier des algèbres géométriques, tous les travaux portant sur les réseaux de neurones dits de Clifford (i",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 41
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_42",
      "text": "nitialement introduits par Buchholz et Sommer , pour enrichir les réseaux neuronaux avec des propriétés géométriques) peuvent naturellement être rapprochés (puis exploités) de ce qui est fait ici. Les neurones de Clifford étendent le modèle du perceptron classique en remplaçant les opérations réelles par des opérations dans l'algèbre de Clifford. Deux types de neurones Clifford sont généralement utilisés~:\\\\ Neurone de Base Clifford (BCN) : Dans un neurone de Clifford, les entrées et les poids s",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 42
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_43",
      "text": "ont les composantes des multivecteurs appartenant à une algèbre de Clifford $C_{p,q}$. La combinaison linéaire classique est remplacée par le produit géométrique noté $_{p,q}$ (avec $$ un biais). La fonction de propagation d'un neurone Clifford de base est définie par~: $f() = _{p,q} + $ (multiplication à droite) $f() = _{p,q} + $ (multiplication à gauche) où $$, $$, $ C_{p,q}$ sont des multivecteurs. Comme dans notre cas, le produit scalaire usuel est remplacé par un produit géométrique. On not",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 43
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_44",
      "text": "era que la structure est complexifiée par l'absence de commutativité.\\\\ Neurone Spineur Clifford (SCN) : le produit géométrique est étendu de manière à inclure des transformations orthogonales (comme les rotations) appliquées aux entrées. Les spineurs, qui sont des éléments de Clifford agissant comme des opérateurs de rotation, sont utilisés pour transformer les vecteurs d’entrée de manière contrôlée. La transformation de $$ s'écrit alors~: $y= _{p,q} _{p,q} + $, où $$ peut représenter l'inversi",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 44
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_45",
      "text": "on, la réversion ou la conjugaison par un neurone spineur Clifford. Les réseaux de neurones de Clifford sont construits en connectant plusieurs neurones de Clifford. On trouve dans la littérature trois catégories principales ~: Clifford Multilayer Perceptron (CMLP) : utilise des BCN avec des fonctions d'activation à valeurs réelles. Spinor Clifford Multilayer Perceptron (SCMLP) : utilise des SCN avec des fonctions d'activation à valeurs réelles. Clifford MLP avec Fonctions d'Activation à Valeurs",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 45
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_46",
      "text": " Clifford (FCMLP) : utilise des fonctions d'activation à valeurs dans une algèbre de Clifford. L'apprentissage des neurones de Clifford s'effectue par une extension de la descente de gradient classique. Il faut noter que le produit de deux éléments non nuls d'une algèbre de Clifford peut être nul, ce qui induit quelques difficultés pour l'étape de rétropropagation du gradient. On utilise donc des involutions (une fonction qui, appliquée deux fois à un multivecteur, retourne le multivecteur d'ori",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 46
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_47",
      "text": "gine ; cela peut être l'inversion, la réversion ou la conjugaison). Par exemple, l'algorithme de rétropropagation pour les CMLP (Clifford Multilayer Perceptrons) utilise une involution unique déterminée par l'algèbre de Clifford sous-jacente. Les outils développés dans cette thèse ne présentent pas autant de complexité technique pour leur implémentation. Les expérimentations qui suivent ont été mises en place pour valider les modèles de couches hypersphériques construits à partir d'un ensemble d",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 47
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_48",
      "text": "e neurones ou de filtres de convolution hypersphériques. Comme nous nous positionnons sur des briques élémentaires de bas niveau, nous avons volontairement choisi des architectures de réseau simples afin de limiter l'écueil du sur-apprentissage et des difficultés d'interprétation. \\\\ Nos expérimentations sont présentées en deux parties, sans puis avec convolution. couches denses} Deux jeux de données synthétiques ont été utilisés pour étudier la performance des réseaux à couches hypersphériques.",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 48
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_49",
      "text": " Le premier nommé est construit de façon à avoir 3 classes quasiment linéairement séparables. Celui-ci contient 350 points en dimension 2, ce qui donne de plus l'opportunité d'illustrer les résultats par différentes images.\\\\ Le second nommé est constitué d'une série de 450 points en dimension 2 répartis en 3 classes non linéairement séparables avec un taux de chevauchement non négligeable. Cette structure nous permettra de visualiser et de comparer la forme des frontières de décision pour diffé",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 49
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_50",
      "text": "rentes architectures.\\\\ Les paramètres ayant servi à générer ces jeux de données sont donnés dans . Les deux jeux de données ont été séparés en ensemble d'apprentissage (66.66\\ L'architecture de base que nous considérons est un perceptron multi-couches de faible profondeur. Dans un premier cas, les couches du réseau (noté PMC) seront des couches denses classiques. Le réseau nommé GeoPMC ne contient quant à lui que des neurones hypersphériques. On a évalué l'influence de différents paramètres : L",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 50
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_51",
      "text": "e nombre de couches cachées varie de 0 à 2. Le nombre de neurones par couche cachée prend les valeurs 2, 3 ou 10. La batch normalisation et la fonction d'activation s'appliquent ou non sur toutes les couches cachées. Chaque réseau se termine par une couche de 3 neurones classiques ou hypersphériques avant de passer par la fonction d'activation softmax car il s'agit d'un problème de classification ; celle-ci n'est jamais précédée d'un . Pour faciliter la lecture des résultats, chaque architecture",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 51
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_52",
      "text": " du réseau est codée par une chaîne de caractères : un chiffre correspond au nombre de neurones par couche, \"r\" à l'utilisation de la fonction d'activation relu, \"b\" à une batch normalisation et \"sf\" à la fonction d'activation softmax. La taille du batch est de 30, l'algorithme d'optimisation est Adam . Bien que l'ensemble des configurations ci-dessus ait été testé, seul un ensemble de configurations efficaces a été sélectionné pour ce manuscrit. Les figures et reportent le taux de bonne prédict",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 52
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_53",
      "text": "ion () sur les jeux de données de validation au cours des itérations. Pour des architectures PMC et GeoPMC identiques, on constate que l' augmente plus rapidement pour les modèles GeoPMC. L'échec du modèle GeoPMC \"b 2 r b 2 r b 3 sf\" s'explique probablement par son architecture qui contribue à l'inhibition de certains neurones () .[H] [height=9cm, width=9cm, grid= major , xlabel = {Epoch} , ylabel = {Accuracy} , xmax=200, legend entries={geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 53
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_54",
      "text": "3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, }, legend style={at={(0.5,0)},anchor=south west,nodes={scale=0.6, transform shape}}], ] table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_3_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_10_3_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_2_2_3_bn_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_3_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {dat",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 54
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_55",
      "text": "a/easy_PMC_10_3_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_2_2_3_bn_relu_accuracy.csv}; pour les données de validation } [H] [height=9cm, width=9cm, grid= major , xlabel = {Epoch} , ylabel = {Accuracy} , xmax=200, legend entries={ geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, }, legend style={at={(0.5,0)},anchor=south west,nodes={scale=0.6, transform shape}}], ] table [x=Step, y=Value,col sep=c",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 55
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_56",
      "text": "omma] {data/dif_geoPMC_3_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_10_3_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_2_2_3_bn_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_3_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_10_3_relu_accuracy.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_2_2_3_bn_relu_accuracy.csv}; : pour les données de validation } Pour aller plus loin, la table rep",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 56
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_57",
      "text": "orte l' en fin d'entraînement des 14 réseaux les plus performants. Excepté les trois premiers cas, l'ensemble des réseaux efficaces sur le sont également sur . [H] {|c|c|c|c|c|} {|c|}{}&{|c|}{}&{|c|}{}\\\\ {|c|}{Architectures} & GeoPMC & PMC & GeoPMC & PMC\\\\ 10 r : 3 sf & 92\\ b 2 r : b 2 r : b 3 sf & 41\\ 3 r : 3 sf & 67.2\\ 3 sf & 96.5\\ b 2 : b 2 : b 3 sf & & & 91.3\\ 10 : 3 sf & & 96\\ 10 : 10 : 3 sf & & 94\\ 2 : 2 : 3 sf & & 95.7\\ 3 : 3 : 3 sf & & 95.7\\ b 10 : b 3 sf & \\ b 10 r : b 3 sf & & & 92\\ b ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 57
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_58",
      "text": "3 r : b 3 sf & & & 92\\ b 3 sf & 97.4\\ b 2 : b 3 sf & & & 91.8\\ Hormis l'architecture \"10 r : 3 sf\" qui s'avère être plus performante pour les réseaux à couches hypersphériques et le réseau \"b 2 r : b 2 r : b 3 sf\" qui lui semble être meilleur que le modèle classique sur les données de , l'ensemble des résultats d' ne sont pas significativement différents. Étant donnée la taille des jeux de données de validation, un écart de 1\\ Afin de mieux voir l'impact de la fonction d'activation et de la batc",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 58
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_59",
      "text": "h normalisation, les tableaux et présentent les résultats de l'apprentissage par architecture. \\\\ [H] {|c|c|c|c|c|} {|c|}{GeoPMC}\\\\ Architectures & $$ & & bn & bn + \\\\ 10 : 3 sf & & 92\\ 3 sf & 95\\ 3 : 3 sf & 94\\ 2 : 3 sf & 92.2\\ 3 : 3 : 3 sf & 98.3\\ {|c|}{PMC}\\\\ Architectures & $$ & & bn & bn + \\\\ 10 : 3 sf & 96\\ 3 sf & 97.4\\ 3 : 3 sf & 95.6\\ 2 : 3 sf & 96.5\\ 3 : 3 : 3 sf & 95.7\\ : Comparaison de l'impact de et de la batch normalisation} [H] {|c|c|c|c|c|} {|c|}{GeoPMC}\\\\ Architectures & $$ & & b",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 59
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_60",
      "text": "n & bn + \\\\ 10 : 3 sf & 92\\ 3 sf & & ND & 92.4\\ 3 : 3 sf & 92.6\\ 2 : 3 sf & & 89.6\\ 3 : 3 : 3 sf & 90\\ {|c|}{PMC}\\\\ Architectures & $$ & & bn & bn + \\\\ 10 : 3 sf & & 92.6\\ 3 sf & & ND & & ND\\\\ 3 : 3 sf & & & 92\\ 2 : 3 sf & 92.6\\ 3 : 3 : 3 sf & & 48\\ : Comparaison de l'impact de et de la batch normalisation} De manière générale, la présence du semble dégrader davantage la qualité des prédictions sur les réseaux à couches hypersphériques, d'autant plus si le nombre de couches cachées croît. En con",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 60
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_61",
      "text": "séquence, le ne semble pas être la fonction d'activation la plus adaptée sachant que $._i$ peut être un scalaire très négatif (cf. figure ). Une étude supplémentaire s'avère nécessaire pour en trouver une plus appropriée. En ce qui concerne la batch normalisation, elle permet dans le cas des données de , d'améliorer considérablement les résultats. Son utilisation semble moins pertinente pour les données issues du fichier .\\\\ En regardant maintenant les valeurs de la fonction de perte () sur le j",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 61
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_62",
      "text": "eu de données de validation (figure et ), il est possible d'observer que les modèles GeoPMC convergent plus vite vers des valeurs plus petites. [H] [height=9cm, width=14cm, grid= major , xlabel = {Epoch} , ylabel = {Loss} , xmax=150, legend entries={geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, }, legend style={at={(0.5,0.5)},anchor=south west,nodes={scale=0.6, transform shape}}], ] table [x=Step, y=Value,col sep=comma] {da",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 62
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_63",
      "text": "ta/easy_geoPMC_3_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_10_3_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_geoPMC_2_2_3_bn_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_3_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_10_3_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/easy_PMC_2_2_3_bn_relu_loss.csv}; {font=} : Courbes des fonctions de pertes } [H] [height=9cm, width=14cm, grid= major , xlabel = ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 63
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_64",
      "text": "{Epoch} , ylabel = {Loss } , xmax=150, legend entries={ geoPMC : 3 sf, geoPMC : 10 r 3 sf, geoPMC : b 2 r b 2 r b 3 sf, PMC : 3 sf, PMC : 10 r 3 sf, PMC : b 2 r b 2 r b 3 sf, }, legend style={at={(0.5,0.5)},anchor=south west,nodes={scale=0.6, transform shape}}], ] table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_3_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_10_3_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_geoPMC_2_2_3_bn_relu_loss.csv}; table [x=St",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 64
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_65",
      "text": "ep, y=Value,col sep=comma] {data/dif_PMC_3_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_10_3_relu_loss.csv}; table [x=Step, y=Value,col sep=comma] {data/dif_PMC_2_2_3_bn_relu_loss.csv}; : Courbes des fonctions de pertes} En considérant $ ^2$ comme le domaine initial sur lequel le modèle est évalué, la pré-image est définie comme un sous-ensemble $$ associé à un ensemble d'éléments de sortie définis par $ ._i ()$. En d'autres termes, pour une hypersphère $$, la pré-image est déf",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 65
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_66",
      "text": "inie par : \\[ \\{ ^2 ._i () = y \\} \\] où $y $ est une constante. Dans ce contexte, les courbes de niveau du produit $ ._i (x_k)$, où $$ est un vecteur associé à une hypersphère et $()$ est la représentation de l'exemple $$ dans un espace transformé (tout le réseau hormis la dernière couche), peuvent être interprétées comme des pré-images. Les lignes correspondant aux frontières de décision correspondent à l'ensemble des points où le produit $ ._i ()$ est nul. Comme les expérimentations sont effec",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 66
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_67",
      "text": "tuées sur des données 2D, il est possible de visualiser la pré-image d'un neurone hypersphérique paramétré par une hypersphère $s$ en affichant les bandes de niveau pour les valeurs positives de $ ._i $ (figure et ). [H] {.46} ._i=0$} {.46} } : } Si l'hypersphère à afficher est située dans la couche 0 ({ i.e.} pas de non-linéarité) (cf. figure ) ou qu'il n'existe pas de couches cachées (voir figure ), les bandes de niveau sont circulaires concentriques. \\\\ [H] {.45} {.45} : Bandes de niveaux pou",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 67
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_68",
      "text": "r\\\\ geomPMC \"b 3 r : b 3 r : b 3 sf\"} La figure permet d'illustrer la proximité entre les pré-images des hypersphères (situées avant la couche softmax) et les frontières de décision. Cela peut paraître évident au premier abord, mais si l'on se réfère à la figure , on sait que la frontière de décision entre deux hypersphères, en l'absence de non-linéarité, est un hyperplan... Il convient également de noter que les centres des hypersphères (colonne de gauche en jaune) diffèrent des centres des cla",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 68
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_69",
      "text": "sses (colonne de droite). [h] {} : geoPMC \"3 r : 3 r : 3 sf\"} {} : geoPMC \"10 r : 3 r : 3 sf\"} La différence entre deux modèles GeoPMC et PMC de même architecture est flagrante si l'on compare leurs frontières de décision (cf. figure ). Si l'on regarde leur forme, PMC correspond à une succession de lignes brisées, alors que GeoPMC produit une succession d'arcs de cercle. [h] {.45} {.45} : Comparaison des frontières de décision} convolutions hypersphériques} Cette section propose une comparaison ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 69
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_70",
      "text": "entre les filtres hypersphériques et Conv2d sur le jeu de données MNIST. Il s'agit d'une base de 70000 mini-images étiquetées, en niveaux de gris, de taille $28 28$ représentant l'écriture manuscrite des chiffres de 0 à 9. C'est donc un problème de reconnaissance à 10 classes.\\\\ Dans cette expérimentation, une architecture est testée avec la séquence de couches suivante~: une batch normalization ou non une couche hypersphérique pour GeoPMC / une couche Dense pour PMC / une couche à filtres hyper",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 70
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_71",
      "text": "sphériques pour GeoConv2d / une couche Conv2d pour Conv2d avec un certain nombre de filtres de taille $ 3 3$ une fonction ou non une batch normalization ou non une couche Dense ou hypersphérique avec un certain nombre de neurones une activation softmax Le tableau donne les résultats d' pour l'ensemble des configurations testées. [htbp] {|c|c|c|c|c|} {|c|}{}&{|c|}{Dense}&{|c|}{Convolution}\\\\ {|c|}{Architectures} & GeoPMC & PMC & GeoConv2d & Conv2d\\\\ 10 sf & & & {|c|}{ND} \\\\ 32 : 10 sf & 93\\ 32 r ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 71
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_72",
      "text": ": 10 sf & 10\\ b 32 : b 10 sf & 86.4\\ b 32 r : b 10 sf & 10\\ 512 : 10 sf & 93\\ 512 r : 10 sf & 10\\ b 512 : b 10 sf & 93\\ b 512 r : b 10 sf & 10\\ pour différents modèles} Une première observation concerne l’incapacité de GeoPMC à fonctionner en \"grande\" dimension avec une activation . Le taux d'erreur de 90\\ car les hypersphères sont définies en dimension 9 au lieu de 784. Plus généralement, il serait pertinent de se demander si les couches hypersphériques sont sensibles à la malédiction de la dim",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 72
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_73",
      "text": "ension. On constate la même amélioration lors du passage des modèles GeoPMC vers GeoConv2d que celui de PMC vers Conv2d. D’après nos premières expériences, GeoConv2d et Conv2d produisent des résultats identiques. L'initialisation des paramètres dans une couche comportant plusieurs neurones hypersphériques représente un défi complexe, mais qu'il fallait relever au vu des expérimentations menées précédemment. Pour se convaincre encore de l'importance de la question de l'initialisation, on peut aus",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 73
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_74",
      "text": "si évoquer dans ce préambule le comportement du volume d'une hypersphère en grande dimension. On rappelle en effet que le volume d'une hypersphère en fonction de la dimension $n$ pour un rayon $$ fixé est le suivant~: $$V() = {2} n} ^n}{({2} n + 1)} .$$ On peut constater dans la figure que le volume d'une hypersphère tend rapidement vers zéro lorsque la dimension $n$ de l'espace des données augmente. Pour un rayon $$ proche de 1, le volume devient quasiment nul dès que la dimension atteint 40. C",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 74
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_75",
      "text": "ela devient problématique pour les données de dimensions encore plus élevées et peut donc compliquer les étapes d'initialisation. En effet, comme le volume décroît fortement avec l'augmentation de la dimension, à la limite, tout point de l'espace se situe en dehors des hypersphères comme l'on peut le voir sur la figure (le produit $ ._i $ décroît linéairement vers des valeurs négatives au fur et à mesure que la dimension augmente). [H] [H] ._i $ en fonction de la dimension de l'espace des donnée",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 75
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_76",
      "text": "s} Le paragraphe est organisé de la façon suivante. \\\\ Nous commençons au paragraphe par une présentation des méthodes aujourd'hui les plus utilisées pour l'initialisation des réseaux à couches denses. C'est en effet de ces méthodes que nous avons décidé de nous inspirer, en particulier de l'approche heuristique de Glorot et Bengio . D'autres stratégies d'initialisation auraient pu être envisageables. Par exemple, à l'instar des réseaux utilisant des fonctions radiales, il est possible de déterm",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 76
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_77",
      "text": "iner les centres à l'aide d'un algorithme de clustering et d'ajuster les rayons en fonction de la dispersion des données autour de ces centres. Cependant, ce type d'approche souffre de deux défauts. D'abord, il est crucial de ne pas confondre les centres des hypersphères avec ceux des clusters. Ensuite, et surtout, une telle stratégie qui sépare les centres et les rayons des sphères et en plus les traite avec des outils totalement différents, va à l'encontre de la philosophie de cette thèse, bas",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 77
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_78",
      "text": "ée sur des outils d'algèbre géométrique et un apprentissage le plus direct possible.\\\\ Nous choisissons donc une stratégie de normalisation à la Glorot. Pourtant, et c'est l'objet du paragraphe suivant, nous montrons rapidement que la méthode usuelle n'est pas du tout appropriée. Nous en illustrons les conséquences. Nous identifions également la cause : une distribution normale passée dans un neurone hypersphérique ne suit plus une loi normale. Le paragraphe présente les calculs de variance qui ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 78
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_79",
      "text": "justifient la pertinence de la méthode d'initialisation que nous allons proposer. La loi de probabilité adaptée à la structure des neurones hypersphériques est la loi Gamma Généralisée. Une difficulté est que sa complexité permet très peu de calculs explicites et que ses propriétés sont mal connues. Nous la contournons en utilisant quelques propriétés asymptotiques des lois Gamma Généralisées. Le paragraphe est à la fois très calculatoire et très probabiliste. Le lecteur peut passer les détails ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 79
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_80",
      "text": "et aller directement en Section pour une version synthétique des règles d’initialisation obtenues et pour des expérimentations qui permettent à la fois de confirmer la pertinence de ces règles et d’illustrer les arguments mathématiques sous-jacents. Un test de la méthode d'initialisation proposée est présenté au paragraphe , sur des données synthétiques suivant une distribution normale, composées de points en dimension 4. } L'initialisation des réseaux de neurones est une question qui a été larg",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 80
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_81",
      "text": "ement explorée. Dans ce manuscrit, nous ne présentons pas d'état de l'art mais faisons un focus sur un type d'approche, dont des exemples emblématiques sont les travaux de Glorot (adaptés aux fonctions d'activation type tangente hyperbolique) ou de He (adaptés aux fonctions d'activation }). Comme déjà mentionné, nous pensons en effet ces stratégies plus adaptables aux réseaux hypersphériques. Pour une revue de l'état de l'art, nous mentionnons par exemple les articles , , ainsi que .\\\\ Par initi",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 81
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_82",
      "text": "alisation { à la Glorot}, nous désignons des stratégies consistant à une forme de normalisation (voir aussi LeCun et al. ) des points du réseau sensée garantir la propagation d'un signal à travers les couches profondes, en évitant qu'il ne se déforme trop. Typiquement on ne veut pas que l'amplitude du signal explose ou au contraire tende vers zéro. Simultanément l'approche vise à maintenir les poids dans une plage de valeurs raisonnable. En pratique, si la distribution des éléments $x_i$, $i \\{1",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 82
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_83",
      "text": ", , n\\}$, en entrée du réseau suit une loi normale, on choisit la distribution des poids du réseau de telle sorte que les éléments $y_j$, $j \\{1, , m\\}$, en sortie du réseau suivent la même loi normale (même moyenne, même variance). Pour que les calculs soient faisables, on suppose que les poids du réseau suivent également une loi normale.\\\\ Nous allons détailler les approches de Glorot et He dans les lignes qui suivent. L'objectif est de comprendre ces méthodes dans le contexte classique des ré",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 83
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_84",
      "text": "seaux de neurones avant de les adapter au cas plus complexe des neurones hypersphériques. On se place donc ici dans le cas `classique'' où la sortie d'un neurone de type dense est donnée par la forme linéaire suivante~: y_j = _{i=1}^{n} w_{ji} x_i les $w_{ji}$ désignant les poids du réseau dont l'initialisation doit être choisie. On suppose que $x_i$, $1 i n$, sont des variables aléatoires indépendantes et identiquement distribuées, suivant la loi normale de moyenne $_x = 0$ et d'écart-type $_x$",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 84
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_85",
      "text": ". On veut que les sorties $y_j$, $1 i m$, suivent encore loi normale de moyenne $_x = 0$ et d'écart-type $_x$. On fait le pari qu'on va arriver à nos fins en faisant des hypothèses relativement simple sur la structuration des poids : On suppose que $w_{ji}$, $1 j m$, $1 i n$, sont des variables aléatoires indépendantes et identiquement distribuées, suivant la loi normale de moyenne $_w = 0$ et d'écart-type $_w$. On va maintenant calculer la variance des $y_j$, $1 i m$, définis par et jouer sur l",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 85
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_86",
      "text": "e seul paramètre libre du problème, c'est-à-dire $_w$ pour arriver à nos fins. Pour la suite, au vu de la formule , on a besoin de calculer en particulier la variance de produits de variables aléatoires indépendantes. On rappelle donc que si $X$ et $Y$ sont des variables aléatoires indépendantes, alors (XY) = ^2(X) (Y) + ^2(Y) (X) + (X) (Y). En effet, on a (XY) &=& (X^2Y^2)-^2(XY) = (X^2,Y^2)}_{0 }+(X^2)(Y^2)-^2(XY) \\\\ &=& - ^2(XY) \\\\ &=& (X) (Y)+ ^2(X) (Y)+^2(Y) (X) + ^2(X) ^2(Y)-^2(XY)}_{0 } \\",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 86
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_87",
      "text": "\\ \\\\ La sortie d'un neurone est exactement donnée par . On a donc $$ (y_j)= (_{i=1}^{n}w_{ji} x_i)=_{i=1}^{n}(w_{ji} x_i) $$ En appliquant la formule établie dans la remarque précédente, on obtient (y_j) = _{i=1}^{n} (w_{ji}) (x_i) Les distributions étant identiques, on arrive finalement à : $$ (y_j)=n _w^2 _x^2.$$ Puisqu'on veut $(y_j) = _x^2$, on conclut que le résultat est obtenu si $$ n _w^2 = 1.$$ Le travail n'est pas terminé. En effet pour l'apprentissage il faut aussi envisager l'étape de",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 87
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_88",
      "text": " rétro-propagation. Notons $L$ la fonction de perte. On veut maintenant que $$ ( { x } ) = ( { y } ).$$ Intuitivement, les gradients doivent conserver une certaine homogénéité au fil des passages entre couches. Or $ { x } = { y } { x }$ (dérivation en chaîne) où la différentielle ${ x }$ se calcule facilement grâce à la formule définissant $y$ comme une fonction linéaire de $x$. On a : { x_i} = _{j=1}^m w_{ji} { y_j}. On remarque facilement la similitude des définitions et . Comme la moyenne des",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 88
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_89",
      "text": " $w_{ji}$ est nulle ($_w=0$), les moyennes des autres termes dans le sont aussi. Pour le calcul des variances, seul le nombre de termes dans la somme change. On arrive donc à la condition $$ m _w^2 = 1.$$ Si $m n$, il faut finalement faire une hypothèse sur $_w$ qui permette de faire un compromis entre les conditions $ n _w^2 = 1$ et $ m _w^2 = 1$. On aboutit ainsi à la condition sur l'initialisation des poids suivante : _w^2={n+m} . \\\\ On aurait pu aussi supposer que la distribution des poids s",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 89
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_90",
      "text": "uit une loi uniforme sur $$. On a alors var$(w_j)=r_w^2/3$, si bien que la condition ci-dessus devient $r_w = $. \\\\ Dans ce cas, l'équation est remplacée par $$ y_j = f( _{i=1}^{n} w_{ji} x_i )$$ où $f$ désigne la fonction d'activation. Les calculs de variances sont alors bien sûr plus techniques. Néanmoins, si la fonction $f$ est régulière, on peut l'approcher par une fonction linéaire { via} son développement limité en 0 (puisque les entrées sont supposées de moyenne nulle). Les calculs ci-des",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 90
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_91",
      "text": "sus ne sont donc que très peu modifiés. On peut le faire par exemple si la fonction d'activation $f$ est une fonction logistique ou une tangente hyperbolique : au voisinage de 0, $ x$ (résultat ci-dessus inchangé), $1/1+e^{-x} 1/2+x/4$ (résultat ci-dessus inchangé à la constante près). Si la fonction $f$ n'est pas régulière, mais si elle suffisamment simple pour calculer les intégrales correspondant à la définition des variances, on peut également adapter les calculs précédents : c'est le cas pa",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 91
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_92",
      "text": "r exemple de la fonction . Les résultats sont résumés ci-dessous. \\\\ Le tableau présente une synthèse des résultats concernant les écarts types et les bornes de l'intervalle considéré, en fonction de l'utilisation de la loi normale ou uniforme pour l'initialisation des poids.\\\\ [h!] {|c|c|c|} activation & Loi normale & Loi uniforme \\\\ - & $_w = {n + m}}$ (Glorot) & $r_w = {n + m}}$ (Glorot) \\\\ & $_w = {n + m}}$ (Glorot) & $r_w = {n + m}}$ (Glorot) \\\\ & $_w = 4 {n + m}}$ (Glorot) & $r_w = 4 {n + ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 92
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_93",
      "text": "m}}$ (Glorot) \\\\ & $_w = {n + m}}$ (He) & $r_w = {n + m}}$ (He) \\\\ Notre stratégie va consister à adapter l'approche présentée au paragraphe précédent pour des couches hypersphériques, { i.e.} des sorties de la forme y_{j} = _{i=1}^{n} (2 x_{i} s_{ji}-x^{2}_{i}-s^{2}_{ji})+^{2}_{j} Les calculs sont bien sûr rendus plus techniques par la forme quadratique de la sortie hypersphérique. Ils sont détaillés au paragraphe suivant. Mais on va d'abord voir qu'on ne peut pas se placer dans un cadre aussi ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 93
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_94",
      "text": "simple que celui qui précède. Plus précisément, travailler sur des lois normales et centrées ne permet pas d'obtenir une initialisation pertinente. Nous allons en donner une illustration pratique. Puis nous le justifierons par des arguments probabilistes. Nous considérons des données, les entrées $x_i$, $1 i n$, qui suivent une loi normale et centrée. Pour l'initialisation des hypersphères, les centres sont initialement tirés aléatoirement selon une loi normale centrée, avec un écart-type ajusté",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 94
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_95",
      "text": " en fonction de la dimension $n$ et du nombre de neurones dans la couche obtenu pour des couches classiques ; ils respectent la règle établie au paragraphe mais dans le cas où d'une loi normale { centrée}.}. Les rayons des hypersphères sont initialisés à 1. On peut observer la propagation du signal à travers les trois premières couches à la Figure . [H] Clairement, l'initialisation n'est pas efficace. \\\\ On observe une explosion de l'écart-type des valeurs dès la sortie des premières couches hyp",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 95
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_96",
      "text": "ersphériques. Ce phénomène se manifeste également dans les couches à filtre de convolution hypersphériques. La figure illustre la distribution des données d'entrée et de sortie un tenseurs de taille $(3,1,28,28)$ initialisés selon une loi normale. Cela correspond, par exemple, à un batch de trois images de taille $28 28$. Chaque colonne présente un tenseur différent, tandis que chaque ligne montre l'évolution des distributions à travers les couches successives, depuis la première couche jusqu'à ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 96
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_97",
      "text": "la quatrième couche à filtre de convolution hypersphérique. [H] Dans ces expérimentations, au-delà de l'explosion des écarts type, un élément frappant est le fait qu'on perd très vite la symétrie de la distribution initiale. En fait on voit dans la figure que la distribution à la sortie de la première couche ne suit déjà plus une loi normale. L'hypothèse de normalité de la distribution au fil des couches étant à la base de tous les calculs précédents, c'est évidemment un souci : on sait conserve",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 97
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_98",
      "text": "r la moyenne et la variance d'une distribution normale au travers de la couche hypersphérique mais dès la première sortie la distribution n'est plus normale et plus rien ne sera assuré à partir de la deuxième couche. Ce point peut être confirmé par les éléments de probabilités fondamentales qui suivent. Il est la principale difficulté de ce travail d'initialisation. La densité de probabilité d'une loi normale d'espérance $$ et d'écart-type $$ est donnée par $$ x {} e^{-{2}({})^2}.$$ Si on compos",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 98
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_99",
      "text": "e cette fonction avec une fonction affine $x wx+b$, caractéristique d'une couche dense classique, on obtient une densité $$ x {} e^{-{2}({})^2}$$ qui correspond toujours à une loi normale. L'approche {\\`a la Glorot} a du sens : une distribution normale est transformée par la couche dense en une distribution normale. Mais si la densité de Gauss est composée avec la fonction carrée, fonction qui apparaît dans l'opérateur hypersphérique ({ cf. )}, la structure de la loi normale disparaît : $$ x {} ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 99
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_100",
      "text": "e^{-{2}({})^2}.$$ La couche hypersphérique ne peut donc transformer une distribution normale en une distribution normale. En fait (voir par exemple ), si une variable aléatoire réelle $X$ suit une loi normale centrée d'écart-type $$, noté $X (0,^2)$, alors $X^2$ suit une loi Gamma, $X^2 G(1/2,2^4)$. Cette dernière n'est pas stable par le passage au carré : si $X$ suit une loi Gamma, alors $X^2$ suit une loi dite { Gamma Généralisée}. On a enfin abouti à une loi suffisamment stable pour nos opéra",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 100
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_101",
      "text": "tions hypersphériques car si $X$ suit une loi Gamma Généralisée alors toute puissance de $X^p$, $p ^*$, suit aussi une loi Gamma Généralisée. Ce point est confirmé par la forme de la densité d'une loi Gamma Généralisée, donnée par une fonction $$ x e^{-(x/a)^p}}{(d/p)},$$ où $d>0$ et $p>0$ sont des paramètres de forme, $a$ est un paramètre d'échelle et $$ désigne la fonction Gamma. La prépondérance de la distribution Gamma Généralisée aux sorties hypersphériques est aussi confirmée par la forme ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 101
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_102",
      "text": "des sorties observées dans la Figure (l'impression visuelle peut être confortée par des travaux de fitting tels que ). Pour les premières propriétés des lois Gamma Généralisées, nous renvoyons le lecteur par exemple à ou où la bibliographie est bien présentée. Pour reproduire les calculs du paragraphe , des propriétés plus spécifiques sont nécessaires. La difficulté est qu'il faut calculer la variance et la moyenne de produits et de sommes impliquant au moins une distribution Gamma Généralisée.\\",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 102
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_103",
      "text": "\\ De ce point de vue : La somme de deux variables aléatoires réelles suivant deux lois Gamma Généralisées ne suit pas { a priori} une loi Gamma Généralisée (voir ) puisque la somme de deux distributions normales suit une loi normale. Voir par exemple }. Le produit de deux variables aléatoires réelles suivant respectivement une loi Gamma Généralisée et une loi normale ne suit pas une loi Gamma Généralisée (voir ). Le même problème se pose en fait pour le produit de deux variables aléatoires réell",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 103
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_104",
      "text": "es suivant une loi normale. Cependant, voir par exemple , les moments caractéristiques de la loi produit sont calculables et on peut donc la rapprocher d'une loi normale. Pour obtenir des formules explicites pour les calculs de variance, peu de choix s'offrent à nous. \\\\ On peut penser à la méthode de Stein pour déterminer les fonctions densités de probabilité au fil des couches. On peut consulter pour des calculs de ce type dans le contexte des lois Gamma. On voit que c'est extrêmement techniqu",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 104
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_105",
      "text": "e. De plus, le calcul des variances et des moyennes { via} l'intégration des densités de probabilité doit ensuite être approché numériquement. Pour les premières briques dans le contexte Gamma Généralisée, on peut consulter , , et . \\\\ La question de l'approximation numérique est d'autant plus sensible que des distributions Gamma Généralisées de forme très semblables peuvent avoir des paramètres $(a,d,p)$ très différents (voir ). Au vu des éléments précédents, nous allons mettre en place la stra",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 105
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_106",
      "text": "tégie suivante :\\\\ Une loi Gamma Généralisée est caractérisée par trois paramètres. La méthode { à la Glorot} consiste à en imposer seulement deux.\\\\ L'idée est d'influer sur le degré de liberté restant pour forcer les distributions Gamma Généralisées en sortie de chaque couche à s'approcher d'une distribution normale. \\\\ Pour parvenir à rapprocher chaque sortie d'une distribution normale, en grande dimension, nous serons aidés par le théorème central limite. Au vu de l'expérimentation précédent",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 106
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_107",
      "text": "e ce n'est évidemment pas suffisant.\\\\ Afin de ne pas introduire dans le problème de variable supplémentaire, nous allons peser sur les moyennes et les variances. Illustrons la méthode avec quelques calculs simples. On considère une variable aléatoire réelle $X$ suivant une loi Gamma Généralisée, $X (a,d,p)$. On sait alors que $X^2 (a^2,d/2,p/2)$. \\\\ Comparons les moyennes : && (X) = a {p})}{({p})}, \\\\ && (X^2) = a^2 {p})}{({p})} =(X) a {p})}{({p})} . Dans le cas où $p=1$ (c'est le cas où la loi",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 107
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_108",
      "text": " Gamma Généralisée est une loi Gamma, en sortie de première couche si l'entrée est gaussienne par exemple) et pour un paramètre de forme $d 1$ (ce qui est le cas observé), on voit que $$ (X^2) = a(d+1) (X).$$ Ainsi, la moyenne se décale vers la droite. Cet effet indésirable s'observe d'ailleurs dans les sorties hypersphériques ci-dessus (avec un décalage vers la gauche à cause du signe ``$-$'' dans ). Un moyen de limiter ce décalage est de réduire la valeur de $a$. Et un moyen direct d'influer s",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 108
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_109",
      "text": "ur la valeur de $a$ est de réduire la variance de la distribution $X$. Comparons maintenant les variances : && (X) = a^2 {p})({p})-({p})^2}{({p})}, \\\\ && (X^2) = a^4 {p})({p})-({p})^2}{({p})} . Une fois encore, réduire la valeur de $a$ permet de contrôler l'explosion de la variance. Ces considérations sur la partie carrée de la sortie hypersphérique sont confortées par les travaux de Aroian et al. (voir aussi pour des expérimentations numériques) qui établissent que le produit de $X (_x,_x^2)$ p",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 109
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_110",
      "text": "ar $S (_s,_s^2)$ tend vers une distribution normale si {_x} + {_s} 1 . la stratégie consiste à assurer des sorties normales en diminuant la variance des données en entrée en initialisant les poids avec une moyenne non nulle On reprend donc les calculs { à la Glorot} dans le paragraphe suivant, en les adaptant aux sorties hypersphériques, mais en conservant l'hypothèse de normalité. Cette partie est très calculatoire. Tous les détails sont fournis pour assurer la reproductibilité des résultats. N",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 110
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_111",
      "text": "ous invitons le lecteur qui veut aller directement aux résultats à consulter le paragraphe suivant. Comme dans l’approche classique, le calcul des variances entre les entrées et les sorties vise à établir une condition pour maintenir une variance constante du signal à travers les couches du réseau. En imposant l’égalité entre la variance des entrées et celle des sorties, on peut relier les paramètres de l’hypersphère aux caractéristiques des données d’entrée. Cela permet d’imposer des contrainte",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 111
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_112",
      "text": "s sur les paramètres du modèle afin d’assurer une propagation adéquate du signal. L'objectif principal est de déterminer les valeurs d'initialisation des poids du réseau pour minimiser les variations indésirables du signal durant l'apprentissage. Ici, on fixe le rayon des hypersphères et on joue sur la distribution des centres. )} Pour l'initialisation, on introduit un paramètre supplémentaire $ >0$ dans le réseau. Ce paramètre nous permettra de gagner suffisamment de latitude pour parvenir à no",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 112
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_113",
      "text": "s fins. En particulier, il va jouer le rôle de facteur d'échelle pour ramener les données à des quantités à variance petite (conformément à la stratégie construite ci-dessus). On considère donc que les éléments $y_j$ du vecteur de sortie $Y$, sont donnés par $y_{j}= ._i_j}{}$, c'est-à-dire : y_{j} = {2} 1 j m, le nombre $m$ étant le nombre de sphères (neurones hypersphériques) dans la couche. On suppose les données identiquement distribuées suivant une loi normale : pour tout $i \\{1, , n\\}$, $x_",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 113
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_114",
      "text": "i (_x, _x^2)$. On fait une hypothèse similaire pour les sorties $y_j$ et les poids du réseau $s_{ji}$ : pour tout $i \\{1, , n\\}$, tout $j \\{1, , m\\}$, $y_j (_y, _y^2)$, $s_{ij} (_s, _s^2)$. Le rayon $$ est supposé fixe. Pour faciliter la lecture, on rappelle le résultat suivant. Soit $x$ une variable aléatoire telle que $X (, ^2)$. Les moments centrés peuvent être calculés en fonction des moments d'ordre $p$ en développant l'expression $(x-)^p$ puis en appliquant les propriété de linéarité de l'",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 114
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_115",
      "text": "espérance . On a donc $$ {ll} & = \\\\ ^2 & = - 2 { } + ^2 \\\\ ^2 & = - 2 {} + ^2 \\\\ $$ $$ {ll} & = \\\\ 0 & = - 3 { } + 3 ^2{ } -3 ^3 \\\\ 0 & = - 3 {(^2 + ^2)} + 3 ^2{} -3 ^3 \\\\ $$ $$ {ll} & = \\\\ 3 ^4 & = - 4 { } + 6 ^2{ } -4 ^3 { } + ^4\\\\ 3 ^4 & = - 4 {(3^2+^3)} + 6 ^2{(^2+^2)} -4 ^3 {} + ^4\\\\ $$ Le calcul des quatre premiers moments donne en particulier les résultats suivants~: $${l l } = &= ^2+^2\\\\ \\\\ = 3^2+^3 & = 3^4+6^2^2+^4. $$ Le calcul de l'espérance de $y_j$ donne, pour tout $1 j m$ :\\\\ ${ll",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 115
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_116",
      "text": "} (y_{j}) & =({2})\\\\ \\\\ & =({2})+({2})\\\\ \\\\ & ={}(x_{i} s_{ji} )-{2}(x^{2}_{i} )-{2}(s^{2}_{ji} )+{2}(^{2}_{j} )\\\\ \\\\ & ={}(x_{i})(s_{ji})-{2}(x^{2}_{i} )-{2}(s^{2}_{ji} )+{2} $ Pour assurer que $(y_{j})= _x$, il faut donc : {ll} _x &= {}_x _s-{2}(_x^2+_x^2)-{2}(_s^2+_s^2)+{2}\\\\ \\\\ & = {2}\\\\ \\\\ & = {2}\\\\ égalité qui revient à~:\\\\ {ll} ^2 & = 2 _x + n ou~:\\\\ {ll} & =\\\\ \\\\ Passons à la recherche d'une condition induite par l'égalité des variances en entrée et en sortie. On rappelle que si $X (,^2)",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 116
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_117",
      "text": "$, alors var$(X^2) = -^{2} = 2 ^2 (^2 + 2 ^2 )$. En utilisant la formule , on calcule la variance de $2x_i s_{ji}$ de la façon suivante~: {ll} (2x_i s_{ji}) & =4 (s_{ji}) (x_i) + 4 ^2 (x_i) +4 ^2 (s_{ji})\\\\ \\\\ &=4 _s^2 _x^2 + 4 _x^2 _s^2 + 4 _s^2 _x^2 Or on a~: {l l } (x_i)= _x^2 & (x_i^2)= 2 _x^2 (_x^2 + 2 _x^2 )\\\\ {l l } (s_{ji})= _s^2 & (s_{ji}^2)= 2 _s^2 (_s^2 + 2 _s^2 ) {l l } ( 2x_i s_{ji}-x_i^2, -s_{ji}^2) & = -4 _s _x _s^2\\\\ ( 2x_i s_{ji},-x_i^2 ) & = -4 _s _x _x^2\\ L'annexe détaille les",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 117
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_118",
      "text": " calculs des termes de covariance. \\[ {ll} (y_{j}) & = ({2})\\\\ \\\\ & = {4^2}() { } \\\\ \\\\ & = {4^2} (2 x_{i} s_{ji} - x^{2}_{i} - s^{2}_{ji})\\\\ \\\\ & = {^2} (x_{i} s_{ji}) + {4^2} (x^{2}_{i}) + {4^2} (s^{2}_{ji})\\\\ \\\\ & + {2^2}(2x_i s_{ji}-x_i^2, -s_{ji}^2) + {2^2}(2x_i s_{ji},-x_i^2) \\\\ \\] En utilisant les relations --, on obtient l'expression suivante~: (y_j) &={4 ^2} (4_s^2_s^2 + 4_s^2_x^2 - 8_s_x_s^2 - 8_s_x_x^2 + 4_x^2_s^2 + 4_x^2_x^2 + 2_s^4 + 4_s^2_x^2 + 2_x^4 )\\\\ \\\\ &= {2 ^2} (_s^2 + _x^2) ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 118
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_119",
      "text": "( 2(_x - _s)^2 +_s^2 + _x^2 ) On veut que var$(y_j)=_x^2$ pour tous $i \\{1, , n\\}$ et $j \\{1, , m\\}$. Cela se traduit par l'equation~: {l } 2_s^2{_s^2} + 2_s^2_x^2 - 4_s_x{_s^2} - 4_s_x_x^2 + 2_x^2{_s^2} + 2_x^2_x^2 + {_s^4} + 2{_s^2}_x^2 + _x^4 - {n}_x^2=0 \\\\ L'équation est une équation bicarrée en $^2_s$. On peut donc la résoudre explicitement avec la méthode du discriminant. En ne conservant que la solution qui peut être positive, on arrive à _s^2 = -(_s- _x )^2- _x^2 + {n}} \\\\ Garantir la po",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 119
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_120",
      "text": "sitivité de $^2_s$ revient à s'assurer que son numérateur est positif étant donné que le coefficient devant $^4_s$ est positif, c'est à dire la condition suivante~: _x^2 (-2_s^2 + 4_s_x - 2_x^2 - _x^2 + {n}) >0 Pour s'en assurer, il faut cette fois résoudre une autre équation du second degré en $_s$. Il existe une solution réelle si son discriminant satisfait $ _{} = - 8 _x^2 + {n}^2 > 0$. On en tire la condition suivante sur $$~: 16 ( {n} - {2}) >0 \\\\ c'est-à-dire~: \\\\ > {2} _x^2 } \\\\ On vérifi",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 120
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_121",
      "text": "e alors que la solution donnée par est bien définie si $_s$ satisfait~:\\\\ _s \\\\ Pour résumer, les calculs précédents aboutissent aux conditions suivantes~:\\\\ \\{{ll} ^2 & = 2 _x + n \\\\ \\\\ _s^2 & = {n}_x^2 }- . pour $$ vérifiant . Nous nous sommes également intéressés au cas des couches de neurones à filtres convolutifs ({ cf} Annexe ), afin de déterminer comment la covariance entre deux éléments de la sortie est reliée aux contraintes du système d’équations établi précédemment. Des travaux comme ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 121
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_122",
      "text": "ont de plus montré comment exploiter la structure de covariance d'un filtre convolutif peut permettre d'améliorer l'initialisation. Dans cette partie, l'impact de la variance de la fonction de coût sur les poids est étudié. La sortie d'un neurone de la couche $l$ est désormais notée $^l$, définie par $${ll} ^l_j = {}^l}._i & = {}\\\\ \\\\ & = {} $$\\\\ où $^l_j$ correspond à la partie euclidienne du vecteur $S^l$, soit les coordonnées du centre de l'hypersphère. On a ainsi, \\\\ $${ll} ^{l+1}_{j'} = & {",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 122
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_123",
      "text": "} $$\\\\ Si $f$ représente la fonction d'activation telle que $f'(0) = 1$, la sortie de la couche est définie par ^{l+1} = f(^l)$}.\\\\ On note $L$ la fonction de coût. Pour adapter au cas hypersphérique le raisonnement effectué dans le cas classique pour obtenir l'équation _{}$ la matrice de poids des centres des hypersphères, alors on note $^{l+1}_{j}$, la $j$ ième ligne de $S^{l+1}_{}$ qui correspond au vecteur du centre pour un $j$ donné.}, on calcule la dérivée partielle de $^{l+1}$ par rapport",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 123
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_124",
      "text": " à $^{l+1}_{j}$~: \\\\ ^{l+1}_{}}{ ^{l+1}_{j}}={} \\\\ ce qui conduit à l'équation suivante~:\\\\ { ^{l}_{j}} = {}(^{l+1}_{j}- ^{l+1}_{j}){ ^{l+1}_{}} soit~: { ^{l}_{}} = {}(S^{l+1}_{}- ^{l+1}_{}){ ^{l+1}_{}} \\\\ avec $S^{l+1}$, la matrice dont les coefficients sont les centres des hypersphères. L'opération $(S^{l+1}_{}- ^{l+1}_{})$ consiste donc à soustraire le vecteur $^{l+1}$ à chaque colonne.\\\\ De , on tire d'abord un résultat sur les moyennes :\\\\ $$( { ^{l}_{j}} ) = {} (S^{l+1}_{j}- ^{l+1}_{j}) ( ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 124
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_125",
      "text": "{ ^{l+1}_{j}} ) = {} (_s^{l+1} - _x^{l+1}) ( { ^{l+1}_{j}} ).$$ On a assuré au paragraphe précédent que la moyenne des distributions de sortie reste constante au cours du processus : $_x^{l+1} = _x$. Ainsi\\\\ ( { ^{l}_{j}} ) = {} (_s^{l+1} - _x) ( { ^{l+1}_{j}} ). On peut aussi calculer.} les variances dans : {ll} ({ ^{l}_{}} ) & = {^2}\\\\ On cherche une initialisation permettant de limiter les écarts entre les moyennes et les variances des gradients successifs ${ ^{l}_{j}} $ et ${ ^{l+1}_{j}} $ a",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 125
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_126",
      "text": "u cours de la rétro-propagation. En gardant à l'esprit que $_s $ est seulement défini par l'intervalle donné dans , le choix de $_s$ et $$ tels que $m _s - _x / 1$ permet de limiter les variations de moyenne dans , en gardant à l'esprit que l'équation limite la borne supérieure de $$ pour que $m(_x^2+_s^2)/^2$, c'est-à-dire $m_x/$, reste d'un ordre non négligeable. Ceci dit, d'autres choix sont possibles. On voit par exemple dans que si on accepte un peu de variations dans l'espérance des gradie",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 126
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_127",
      "text": "nts, cela évite la dégénérescence de leur variance... C'est le principe que nous utilisons dans la suite.\\\\ On réunit maintenant les conditions établies de à . D'abord, on introduit un paramètre $'$ (produit de $$ par l'accroissement de moyenne qu'on s'autorise pendant la rétro-propagation) et on choisit pour $_s$ la valeur maximale autorisée par l'intervalle dans . On obtient ainsi un système qui donne deux valeurs possibles pour $_s-_x$~:\\\\ \\{{ll} _s - _x & = {m}\\\\ \\\\ _s - _x & = {n}-{2}} . Da",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 127
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_128",
      "text": "ns la première équation de , on a $' = C $ où $C$ représente le taux de croissance des moyennes : C = ( { ^{l}_{j}} )}{( { ^{l+1}_{j}} )}. Par exemple si $C=1$, les moyennes ne changent pas. Si $C=2$, à chaque fois qu'on traverse une couche la moyenne est doublée etc. \\'Evidemment on cherche une stratégie qui n'augmente pas les moyennes de façon trop significative. D'autant que la croissance des moyennes conduit à une croissance des variances à cause de . Dans la deuxième équation de , on a $'= ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 128
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_129",
      "text": "m {n}-{2}}$. Comme, d'après , il faut $'=C > C $, on a forcément ${n}-{2}} > (C/m){2}^2_x}$. En passant au carré la dernière égalité et la dernière inégalité, on en tire les conditions suivantes sur $$ : && C^2 ^2 = m^2( {n}-{2}), \\\\ && ^2 > {2} ( {m^2} +1 )_x^2. La condition va se substituer à la condition (on note en particulier que implique ). De la condition on peut tirer une expression de $$ : = {2(m^2-C^2n)} } qui n'a de sens que si la variation $C$ de la moyenne des gradients est contrôlé",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 129
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_130",
      "text": "e par C {} . \\\\ { Récapitulons : } \\\\ On choisit $C$ vérifiant et d'un ordre de grandeur raisonnable pour que la relation ne conduise pas à l'explosion de la moyenne des gradients. Nous considérons la variable $$ petite pour calculer $C = {}-$. La soustraction de la valeur $$ permet de garantir que l'inégalité est maintenue. \\\\ On définit ensuite $$ par . \\\\ On remarque alors que $${^2} = {m \\, n\\, _x}.$$ Il ne reste plus qu'à contrôler cette dernière quantité, en utilisant si besoin la marge de",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 130
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_131",
      "text": " manoeuvre qu'on a sur $C$, pour que l'équation assure que la variance des gradients n'explose pas et ne dégénère pas non plus.\\\\ \\\\ Une fois l'étape précédente terminée, on a une valeur définitivement établie pour $$. Celle ci est fixé pour l'entraînement. \\\\ On peut alors choisir $_s$ et $_s$ par . Comme mentionné précédemment, nous avons deux expressions pour définir $_s$. Afin de garantir la condition établie par , on propose donc considérer le min entre les deux expressions obtenues pour $_",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 131
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_132",
      "text": "s$. Ainsi on pose~: _s = (_x + {m}; _x + {n}-{2}} ) Dans les paragraphes suivants sont présentées les expérimentations numériques correspondant à l'ensemble des règles d'initialisation que nous venons de développer. On rappelle ici de façon synthétique les règles d'initialisation issues des calculs des paragraphes précédents. Nous présentons ensuite un certain nombre d'expérimentations qui permettent à la fois de confirmer la pertinence de ces règles et d'illustrer les arguments mathématiques pr",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 132
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_133",
      "text": "écédents. Un test sur un cas plus réaliste est présenté au paragraphe . Dans le tenseur de sortie, la composante $y_{j}= ._i_j}{}$ associée à l'hypersphère $ _j$ correspondant à la sortie d'un neurone s'écrit~: y_{j} = {2} Les conditions , et nous permettent de proposer l'algorithme d'initialisation des paramètres des hypersphère suivant pour une couche hypersphérique~:\\\\ [ colback=gray!5, colframe=blue!75!black, fonttitle=, title={Initialisation des paramètres des hypersphères} ] [H] _1, , _K\\}",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 133
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_134",
      "text": "_k$ tel que $_k ^n$, le nombre d'hypersphères $m$, $={}$. } $, les centres $_j = (s_{j1}, , s_{jn})$ et les rayons $_j$ des hypersphères} \\\\ $${cc} _x = & {Kn} _{k,i} x_{ki} \\\\ ^2_x = & {Kn} _{k,i} x_{ki}^2 - _x^2 $$ \\\\ $$C = {}- := {2(m^2-C^2n)} } $$ \\\\ $$_s := (_x + {m}; _x + {n}-{2}} )$$ \\\\ $$ ^2 := 2 _x + n $$ \\\\ $$ _s^2 := {n}_x^2 }- $$ \\\\ $} { $} { Initialiser les $s_{ji}$ aléatoirement tels que $s_{ji} (_s, _s^2)$ } Les paramètres de l'hypersphère $$ sont définis par le centre $_j =(s_{j1",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 134
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_135",
      "text": "}, , s_{jn})$ et le rayon $$ (cf équation ). } La méthodologie d’initialisation proposée nécessite une inférence préalable couche par couche du modèle. En effet, pour pouvoir appliquer l’algorithme d’initialisation à une couche hypersphérique donnée, il est indispensable de disposer des tenseurs d’entrée associés à cette couche. Cela implique qu'il faut exécuter une passe avant (forward pass) du réseau couche par couche. Dans les sous-paragraphes suivants, on illustre la pertinence des règles d'",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 135
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_136",
      "text": "initialisation proposées en observant la façon dont un ''signal'' est transformé au fil de ses passages dans des neurones hypersphériques. Pour des raisons de place, nous ne montrons que la distribution de données initiales et la distribution à la sortie des deux premières couches. Il faut savoir que lorsque nous annonçons que les résultats sont mauvais, ils le sont dès la sortie de la quatrième couche. Lorsque nous les annonçons bons, c'est parce que nous les avons testé sur une centaine de cou",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 136
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_137",
      "text": "ches. La figure donne par exemple les résultats à la sortie de 50 couches pour~: \\\\ Une distribution normale avec des points de dimension 4\\\\ Des données extraites du jeu IRIS .\\\\ Une distribution composée de trois clusters de points, chacun suivant une loi normale multivariée. [htbp] [t]{0.45} [t]{0.45} [htbp] Pour les expériences qui suivent, nous utilisons un réseau de neurones à deux couches cachées hypersphériques, contenant chacune $N$ hypersphères. Lorsque $N > 1$, la sortie du neurone es",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 137
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_138",
      "text": "t obtenue en moyennant l'ensemble des produits $._i $ calculés. Il n'y a pas d'activation : nous observons simplement l'enchaînement successif des couches. Le jeu de données utilisé est un ensemble de points synthétiques de dimension 10\\,000, distribués selon une loi normale de moyenne $_x$ et de variance $_x^2$. Les valeurs utilisées pour les expérimentations sont indiquées au-dessus des figures correspondantes.\\\\ Dans cette section, la valeur $$ est définie par $ {2}^2_x} + _{}$ car les condit",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 138
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_139",
      "text": "ions données par l'étude du gradient de la fonction de coût ne sont pas prises en compte. [htbp] = 1$: \\\\ $_{x0} = 0.099984$, $_{x1} = 0.080297$, $_{x2} = 0.036692$, \\\\ $_{x0} = 1.000006$, $_{x1} = 1.075929$, $_{x2} = 1.177815$, \\\\ $_{s1} = 0.005229$, $_{s2} = 0.014190$, \\\\ $_{s1} = 0.000000$, $_{s2} = 0.000000$.} : Les écarts-types \\(_{x1}\\) et \\(_{x2}\\) diminuent significativement (0.080297 et 0.036692), ce qui indique une perte de variance. Les moyennes \\(_{x1}\\) et \\(_{x2}\\) augmentent légèr",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 139
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_140",
      "text": "ement par rapport à \\(_{x0}\\), ce qui montre une dérive du signal. [H] = 1$: \\\\ $_{x0} = 0.099984$, $_{x1} = 0.031161$, $_{x2} = 0.019281$, \\\\ $_{x0} = 1.000006$, $_{x1} = 0.998604$, $_{x2} = 0.997637$, \\\\ $_{s1} = 0.000011$, $_{s2} = 0.000001$, \\\\ $_{s1} = 1.382000$, $_{s2} = 2.150632$.} : Les écarts-types \\(_{x1}\\) et \\(_{x2}\\) restent proches de \\(_{x0}\\), ce qui indique une bonne préservation de la variance. Les moyennes \\(_{x1}\\) et \\(_{x2}\\) restent proches de \\(_{x0}\\), ce qui suggère une",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 140
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_141",
      "text": " propagation plus fidèle du signal. : Lorsque \\(_s = 0\\), les écarts-types et les moyennes se dégradent plus rapidement, que dans le cas où \\(_s 0\\), les écarts-types diminuent, mais les moyennes restent très proches de la valeur initiale, ce qui montre une meilleure stabilité. Il est plus intéressant de fixer les moyennes $_s$ proche de la valeur de la borne supérieur. (le cas proche de la valeur de la borne inf donne des résultats similaires). = 0$ et $_x = 1$ (non petit), cela ne fonctionne p",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 141
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_142",
      "text": "as} [H] = 1$: \\\\ $_{x0} = 0.999842$, $_{x1} = 0.593819$, $_{x2} = 0.400592$, \\\\ $_{x0} = 1.000060$, $_{x1} = 0.985483$, $_{x2} = 0.967814$, \\\\ $_{s1} = 0.000747$, $_{s2} = 0.000302$, \\\\ $_{s1} = 1.631132$, $_{s2} = 2.195358$.} : Les écarts-types \\(_{x1}\\) et \\(_{x2}\\) diminuent trop fortement (0.593819 et 0.400592), ce qui indique une perte d'information excessive. Malgré une moyenne relativement stable, la propagation devient moins efficace en raison de la réduction de la variance. : Lorsque \\(",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 142
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_143",
      "text": "_x\\) est grand, les écarts-types diminuent fortement, et le signal tend vers une loi gamma de signe inverse, ce qui montre une instabilité dans la propagation du signal. [H] = 1$: \\\\ $_{x0} = 0.099984$, $_{x1} = 0.031154$, $_{x2} = 0.019243$, \\\\ $_{x0} = 1.000006$, $_{x1} = 0.999603$, $_{x2} = 0.999568$, \\\\ $_{s1} = 0.000011$, $_{s2} = 0.000001$, \\\\ $_{s1} = 1.382000$, $_{s2} = 2.151655$.} [H] = 1$: \\\\ $_{x0} = 0.099984$, $_{x1} = 0.031153$, $_{x2} = 0.019238$, \\\\ $_{x0} = 1.000006$, $_{x1} = 0.",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 143
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_144",
      "text": "999987$, $_{x2} = 0.999986$, \\\\ $_{s1} = 0.000011$, $_{s2} = 0.000001$, \\\\ $_{s1} = 1.382000$, $_{s2} = 2.152048$.} : Lorsque \\(N\\) augmente (\\(N=10\\) et \\(N=1000\\)), la variance \\(_x\\) reste bien préservée. La moyenne \\(_x\\) est presque inchangée, suggérant une stabilité accrue avec un grand \\(N\\). Une meilleure propagation du signal est observée avec un grand \\(N\\), validant son rôle bénéfique. : Lorsque \\(N\\) est grand, les écarts-types diminuent, mais les moyennes restent extrêmement proches",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 144
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_145",
      "text": " de la valeur initiale, ce qui montre une excellente stabilité. On observe une conséquence du théorème central limite. $ sur $_s$} [H] = 10$: \\\\ $_{x0} = 0.099984$, $_{x1} = 0.038070$, $_{x2} = 0.036201$, \\\\ $_{x0} = 1.000006$, $_{x1} = 0.998295$, $_{x2} = 0.997497$, \\\\ $_{s1} = 0.000011$, $_{s2} = 0.000000$, \\\\ $_{s1} = 1.504375$, $_{s2} = 11.328631$.} [H] = 1000$: \\\\ $_{x0} = 0.099984$, $_{x1} = 0.094492$, $_{x2} = 0.094444$, \\\\ $_{x0} = 1.000006$, $_{x1} = 0.998323$, $_{x2} = 0.998099$, \\\\ $_",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 145
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_146",
      "text": "{s1} = 0.000002$, $_{s2} = 0.000000$, \\\\ $_{s1} = 11.694160$, $_{s2} = 1001.364937$.} [H] = 10$: \\\\ $_{x0} = 0.999842$, $_{x1} = 0.613114$, $_{x2} = 0.585020$, \\\\ $_{x0} = 1.000060$, $_{x1} = 0.983484$, $_{x2} = 0.970680$, \\\\ $_{s1} = 0.000755$, $_{s2} = 0.000068$, \\\\ $_{s1} = 1.754479$, $_{s2} = 11.404689$.} [H] = 1000$: \\\\ $_{x0} = 0.999842$, $_{x1} = 0.946954$, $_{x2} = 0.946493$, \\\\ $_{x0} = 1.000060$, $_{x1} = 0.983402$, $_{x2} = 0.981176$, \\\\ $_{s1} = 0.000171$, $_{s2} = 0.000002$, \\\\ $_{s",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 146
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_147",
      "text": "1} = 12.026035$, $_{s2} = 1001.573476$.} [H] = 10000$: \\\\ $_{x0} = 0.999842$, $_{x1} = 0.987210$, $_{x2} = 0.987161$, \\\\ $_{x0} = 1.000060$, $_{x1} = 0.994143$, $_{x2} = 0.993295$, \\\\ $_{s1} = 0.000020$, $_{s2} = 0.000000$, \\\\ $_{s1} = 102.073974$, $_{s2} = 10001.604402$.} [H] = 100000$: \\\\ $_{x0} = 0.999842$, $_{x1} = 0.991597$, $_{x2} = 0.000009$, \\\\ $_{x0} = 1.000060$, $_{x1} = 0.999441$, $_{x2} = 0.999449$, \\\\ $_{s1} = 0.000002$, $_{s2} = 1.000000$, \\\\ $_{s1} = 1002.079502$, $_{s2} = 0.00000",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 147
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_148",
      "text": "0$.} : Lorsque \\(\\) est très grand (\\(_ = 10000\\) ou \\(100000\\)), les écarts-types sont bien préservés, mais les moyennes dérivent légèrement. Cependant, dans le cas de \\(_ = 100000\\), on observe un effondrement de l'écart-type dans la deuxième couche, ce qui indique une perte totale de variance. Cela montre que il y a une valeur limite pour \\(\\) très grand, afin de garantir la propagation du signal. Pour une propagation efficace du signal dans un réseau de neurones, il est essentiel de : Initia",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 148
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_149",
      "text": "liser \\(_s\\) à une valeur non nulle pour éviter la dégradation de la variance et de la moyenne. Maintenir \\(_x\\) petit permet d'assurer une propagation stable du signal. Augmenter \\(N\\) pour améliorer la robustesse et la stabilité du réseau. Contrôler \\(\\) pour éviter l'instabilité, surtout lorsque \\(_x\\) est grand (on revient sur ce point au paragraphe suivant qui concerne le paramètre $q_i$). Lorsque \\(\\) est grand, cela améliore la propagation du signal jusqu'à une valeur limite qui peut entr",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 149
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_150",
      "text": "aîner une instabilité dans le réseau, surtout lorsque \\(_x\\) est petit. Il est donc crucial de trouver un équilibre entre \\(\\) et \\(_x\\) pour maintenir une propagation stable du signal. Dans chaque légende de figure (en haut) apparaît un paramètre $q_i$, l'indice $i$ désignant le numéro de la couche. Celui-ci est calculé en fonction des paramètres d'entrée de la couche $i$ et des paramètres de poids calculés pour cette couche : $$ q_i = {_x} + {_s} .$$ Le paramètre $q_i$ permet de vérifier si le",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 150
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_151",
      "text": " critère est satisfait. Les figures ci-dessus, en particulier la dernière image de la figure permettent de vérifier que l'utilisation du critère peut être un garde-fou très efficace pour éviter un effondrement de la variance du signal de sortie de la couche.\\\\ Afin de mieux maîtriser la valeur de $$ et de tenir compte du nombre d'hypersphères utilisées (paramètre $m$ de l'algorithme proposé), il est insuffisant de se limiter aux conditions données par et . C’est pourquoi nous avons également étu",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 151
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_152",
      "text": "dié l’impact de la variance de la fonction de coût, en suivant un raisonnement analogue à celui présenté dans . Cette démarche a permis de redéfinir le paramètre $$ (cf. équation ) et d’assurer la propagation du signal à travers les couches, quel que soit le nombre de neurones choisis dans la couche hypersphérique. Le but de cette expérience est de vérifier la propagation du signal dans un réseau, en testant un jeu de données réel, afin d’évaluer si l’algorithme proposé pour l’initialisation fon",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 152
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_153",
      "text": "ctionne correctement (voir ). Pour cela, nous avons construit un réseau de neurones comportant 10 couches cachées hypersphériques.\\\\ Trois configurations ont été testées, avec respectivement 8, 64 et 4096 hypersphères par couches cachées (la valeur est indiquée dans la légende). Les modèles, partagent la même architecture générale, sans fonction d’activation ni normalisation par lot (BatchNorm). \\\\ Nous avons pris $ = {}$ pour calculer $C = {}-$.\\\\ Les figures , et présentent la distribution des",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 153
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_154",
      "text": " sorties de chaque couche cachées à l’initialisation, c’est-à-dire avant l’entraînement. Les figures , et montrent quant à elles les distributions des sorties après entraînement.\\\\ [H] {avant} l'entraînement (cas de 8 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {après} l'entraînement (cas de 8 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {avant} l'entraînement (cas de 64 hypersphères par couche cachée, pour un réseau à 10 couches cachées} ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 154
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_155",
      "text": "[H] {après} l'entraînement (cas de 64 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {avant} l'entraînement (cas de 4096 hypersphères par couche cachée, pour un réseau à 10 couches cachées} [H] {après} l'entraînement (cas de 4096 hypersphères par couche cachée, pour un réseau à 10 couches cachées} On peut observer que à l’initialisation, les moyennes des distributions de sorties restent stables au fur et à mesure que l’on empile les couches, et que la variance n’explose",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 155
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_156",
      "text": " pas. Cela permet de préserver le signal à la sortie du réseau.\\\\ Pour les trois configurations qui ont été testées (avec 8, 64 ou 4096 neurones par couche cachée), les taux d’accuracy obtenus sont respectivement de 98.3\\ Il faut considérer un nombre suffisamment grand d'hypersphères (4096) pour observer que la méthode d'initialisation proposée contraint la distribution des sorties à tendre vers une loi normale. Le modèle de neurone hypersphérique de Banarer a été étendu pour définir un filtre d",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 156
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_157",
      "text": "e convolution hypersphérique adapté aux tenseurs d'ordre $l$ à plusieurs canaux. Cette extension consiste à transformer les coefficients du filtre ainsi que le biais en coordonnées sur une hypersphère, permettant ainsi l'optimisation de ces paramètres. Les expérimentations réalisées sur des cas simples visaient à valider et comparer ces modèles sur des architectures simples, telles que les couches denses ou Conv2d. Les résultats montrent que les taux de prédiction restent globalement similaires ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 157
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_158",
      "text": "entre les réseaux classiques et les réseaux à couches hypersphériques pour une architecture de type Perceptron Multicouches. Il a été observé que les fonctions de perte convergent plus rapidement et atteignent des valeurs plus faibles avec les couches hypersphériques. De plus, en adaptant des réseaux convolutifs avec des filtres hypersphériques, les résultats de prédiction demeurent comparables à ceux obtenus avec des filtres classiques. Cependant, lors de la transition d'un réseau dense hypersp",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 158
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_159",
      "text": "hérique vers un réseau convolutif avec filtres hypersphériques, l'augmentation du taux de bonne prédiction est similaire à celle observée avec les réseaux classiques. La question de l'initialisation des poids dans les couches hypersphériques a été explorée afin de garantir la possibilité d'enchaîner plusieurs couches successives. La sortie d'une couche hypersphérique dépend de plusieurs paramètres, dont le facteur d'échelle $$ que nous avons introduit. Ce dernier joue un rôle essentiel dans la d",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 159
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_160",
      "text": "istribution des sorties, et une fois fixé, il permet de déterminer à la fois les valeurs des paramètres hypersphériques et les conditions d'initialisation : en particulier, l'intervalle auquel doit appartenir la moyenne des centres, ainsi que la variance de ces derniers lorsqu'ils sont initialisés selon une loi normale.\\\\ La méthode proposée permet non seulement de propager efficacement le signal à travers des couches hypersphériques successives, mais aussi d’augmenter le nombre de neurones par ",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 160
      },
      "embedding": []
    },
    {
      "id": "chapitre1.tex.txt_chunk_161",
      "text": "couche. Cela ouvre des perspectives intéressantes pour l'intégration des couches hypersphériques dans des architectures plus complexes. Néanmoins, l'ajustement des paramètres d'initialisation reste à affiner en fonction des différentes fonctions d'activation utilisées.\n\n",
      "metadata": {
        "source_file": "chapitre1.tex.txt",
        "chunk_index": 161
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_1",
      "text": "Dans ce chapitre, nous explorons l’utilisation d’une couche hypersphérique pour résoudre des problèmes de régression, en nous intéressant notamment à sa capacité d’approximation. Cette étude s’inscrit dans le prolongement naturel des travaux sur l'approximation universelle dans les réseaux neuronaux classiques. Rappelons que dans ce cadre, une question fondamentale concerne la capacité d’une couche dense, composée d’un nombre fini \\( J \\) de neurones classiques (dotés de fonctions d’activation s",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_2",
      "text": "tandard comme la sigmoïde ou ), à approcher une fonction donnée. Dans ce chapitre, on s'intéresse à l'approximation d'une fonction continue à support compact $ f ^0(K) $, où $K$ est un compact de $^n$. Derrière cette question, il y a l'idée que c'est la capacité d'un réseau de neurones à approximer une grande variété de fonctions qui en fait un outil particulièrement adapté à de nombreuses tâches, telles que la classification ou la régression.\\\\ Nous adoptons ici la convention usuelle de la comm",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_3",
      "text": "unauté pour l'approximation d'une fonction de $^n$ vers $$. Cependant, dans le chapitre suivant, afin d'éviter toute ambiguïté, nous réserverons la notation $n$ pour désigner la dimension de l'entrée et introduirons $m$ pour la dimension du plongement. Par cohérence, nous avons choisi de garder $J$ pour le nombre de fonctions ridge aussi bien que pour le nombre de sphères.\\\\ Bien que différentes architectures de réseaux de neurones soient possibles, nous posons ici la question de savoir si une c",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_4",
      "text": "ouche composée de neurones hypersphériques possède des capacités d'approximation intéressantes. Plus précisément, peut-on, grâce à une combinaison linéaire des $ J $ sorties de ces neurones, approximer avec une précision arbitraire toute fonction $ f ^0(K)$, $K$ compact de $^n$, au sens de la norme uniforme ?\\\\ Pour répondre à cette question, nous commençons par rappeler quelques notions concernant le théorème d’approximation universelle dans le cadre classique des réseaux à une couche cachée. D",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_5",
      "text": "ans ce cadre, le théorème est dit universel : il garantit la capacité d’un réseau dense, composé d’un nombre fini de neurones classiques avec des fonctions d’activation appropriées, à approcher toute fonction continue définie sur un compact de $^n$. L'approche classique pour démontrer le théorème d'approximation universelle repose sur des propriétés et des méthodes analytiques qui ne peuvent pas toutes être directement appliquées dans le contexte des neurones à couches hypersphériques.\\\\ Cela no",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_6",
      "text": "us a conduit à adopter une approche différente. Plus précisément, nous établissons un théorème d’approximation pour les fonctions continues définies sur un compact de $$, en nous appuyant sur un théorème de Schwartz. Ce dernier exploite les combinaisons linéaires de translations. Cette approche, simple et efficace, présente ici une limitation notable : elle n’est pas universelle au sens large, car elle impose que les fonctions à approcher soient scalaires.\\\\ \\\\ Ce paragraphe est un rappel sur le",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_7",
      "text": "s propriétés d’approximation bien établies des réseaux de neurones denses à une couche cachée. Nous y rappelons les conditions permettant de démontrer que ces réseaux peuvent approximer toute fonction continue \\( f ^0(K) \\), où \\(K\\) est un compact de \\(^n\\), au sens de la norme uniforme. Ces résultats posent une base essentielle pour ensuite discuter des capacités d’approximation des neurones dans des architectures non conventionnelles. \\\\ Dans cette perspective, tout d’abord la sortie d’un rés",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_8",
      "text": "eau de neurones dense à une couche cachée est décrite, ce qui permet d’illustrer le rôle des combinaisons linéaires des sorties des neurones, avec application d’une fonction d’activation, dans le processus d’approximation. Cette formalisation conduit naturellement à introduire la notion de fonction . Les fonctions permettent, en effet, de simplifier l’analyse en réduisant la question d’approximation à l’étude de propriétés de fonctions unidimensionnelles. Enfin, cette section pose la question de",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_9",
      "text": " la densité des espaces fonctionnels engendrés par les sorties de réseaux, c’est-à-dire de déterminer si ces espaces sont suffisamment riches pour permettre une approximation uniforme de toute fonction continue définie sur un compact donné. \\\\ {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!80]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,..",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_10",
      "text": ".,3} (K-) at (0,-1.8-3) {$x_{}$}; / in {1,...,5} (J-) at (2,-1.8-1.5) { $ _{}. + b_{}$}; / in {1,...,5} (K-1) edge (J-); / in {1,...,5} (K-2) edge (J-); / in {1,...,5} (K-3) edge (J-); (I) {$ _{j=1}^{5}_j(_{j}.+b_{j})$}; in {1,...,5} (J-) edge (I); ^3$ et à valeur dans $$} Dans le cas classique des couches denses, le schéma correspond à l'architecture du réseaux de neurone considéré. On notera \\[ = (x_1, , x_n) ^n \\] les valeurs d'entrée, et \\[ H()=(_1 + b_1, , _J + b_J) ^J \\]\\\\ est le vecteur c",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_11",
      "text": "omposé des éléments de sortie de chaque neurone de la couche cachée avant application de la fonction d'activation $ : $. On peut écrire la sortie d'un réseau de $J$ neurones en une couche cachée comme suit~: \\\\ y() = _{j=1}^{J} _j (_j + b_j) , \\\\ les \\( _j = (w_{j,i})_{i=1}^n ^n \\) (respectivement \\( b_j \\)), \\( j = 1, , J \\), étant des vecteurs poids (respectivement des biais). Les coefficients $_j$ de la combinaison linéaire sont également des scalaires. Le produit scalaire de deux vecteurs \\(",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_12",
      "text": " \\) et \\( = (x_i)_{i=1}^n \\) de \\( ^n \\) est noté \\( \\) et défini par~:\\\\ \\[ = _{i=1}^{n} w_i x_i. \\]\\\\ Si on considère une fonction donnée, supposée à valeur scalaire, \\( f : ^n \\), alors, dans l'idéal, le but est d'écrire la fonction $f$ sous une forme correspondant à la structure d'un réseau de neurones~: \\[ f() = _{j=1}^{J} _j (_j + b_j) = y(). \\] Comme la structure du réseau est fixée, les inconnues sont la dimension \\( J \\) de la couche cachée ainsi que les paramètres \\( _j, _{j}, b_j \\), ",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 12
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_13",
      "text": "\\( 1 j J \\), qui dépendent de la fonction \\( f \\). Il faut noter cependant que la fonction $$ ne doit pas dépendre de $f$, sinon cela impliquerait que la fonction d'activation change selon la fonction à approcher. L'égalité ci-dessus pour une classe infinie de fonctions est donc ``difficile\" à obtenir. On cherche plutôt une approximation.\\\\ En reprenant l’écriture de la sortie d’un réseau de neurones, il est possible d’observer que, pour tout choix de \\( w_{j,i} \\) et \\( b_j \\), les termes \\( (_",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 13
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_14",
      "text": "{j} + b_j) \\) apparaissant dans la somme peuvent être identifiés comme des fonctions , définies selon la définition suivante~: [Pinkus ] Une fonction est une fonction à plusieurs variables $h : ^n $ de la forme )$} où $g : $ et $= (a_1, , a_n) ^n $. Il est possible d’écrire la sortie du réseau sous la forme d'une combinaison linéaire de fonctions ridge~:\\\\ $${ll} y( ) & = _{j} _{j} ( _{ j} _{}+b_{j})\\\\ \\\\ & := _{j} _{j} g_j(_j )\\\\ \\\\ & \\{g() : ^n \\} $$ en posant $g_j(x):=(x+b_j)$. On voit en par",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 14
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_15",
      "text": "ticulier que la sortie du réseau appartient à un espace vectoriel engendré par des fonctions ridge. Il faut donc voir quel type de fonction peut être approchée efficacement par cet espace vectoriel engendré. C'est l'objet du paragraphe suivant. Dans son article \"Approximation theory of the MLP model in neural networks\", A. Pinkus effectue un état de l'art sur la question de densité qui consiste à montrer que l'ensemble () := \\{ ( - b_{}): b , ^n\\}$} est dense dans ^0(^n)$}. L’objectif est d’appr",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 15
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_16",
      "text": "ocher toute fonction continue sur un compact par une combinaison linéaire de fonctions . En terme de densité, il s’agit de montrer que, toute fonction \\( f ^{0}( ^n) \\) appartient à l'adhérence de $()$~:\\\\ f \\{( + b): ^n, b\\}}= ()}, l'adhérence $()}$ étant considérée au sens de la norme qu'on choisit pour mesurer la qualité de l'approximation de $f$ par une fonction de $()$ : on utilise dans ce chapitre la norme uniforme}$ est la notation pour désigner l'adhérence au sens de la norme uniforme}. ",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 16
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_17",
      "text": "\\\\ Les étapes du raisonnement détaillé dans sont les suivantes~:\\\\ Étape 1~: Se ramener au cas unidimensionnel. Cette première étape correspond à la proposition 3.3 de l'article de Pinkus (cf. Annexe ) Étape 2~: Montrer que tout élément ^{0}(K)$}, peut être approché par une combinaison linéaire d'éléments de $(,,)$ où (,,) = vect\\{( -)~: , \\, \\} avec $^{}()$, $ $ un sous-ensemble de $$ contenant une suite tendant vers 0 et $ $ un intervalle ouvert quelconque de $$. Cette étape correspond à la pr",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 17
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_18",
      "text": "oposition 3.4 de l'article de Pinkus (cf. Annexe ). Étape 3~: Réduire l'hypothèse de régularité sur l'activation. A l'étape 2, on a supposé $^{}()$. Ici on montre qu'on peut se contenter de l'hypothèse $^{0}()$ (cf. Annexe ). Cela permet d'établir le théorème suivant~:\\\\ [colframe=blue, colback=white!10, title=] Soit $ ^{0}()$. Alors, $()$ est dense dans $^{0}( ^{n})$ au sens de la convergence uniforme sur un compact si et seulement si $$ n'est pas un polynôme. La démonstration de Pinkus est dét",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 18
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_19",
      "text": "aillée en annexe car elle est très différente de celle que nous allons construire dans la suite. Au delà de la référence mentionnée ci-dessus de Pinkus , la question de l'approximation universelle d'une fonction par un réseau dense a été très largement étudiée. On retiendra par exemple les éléments suivants : Pour un nombre de couches donn\\'ees, un nombre de neurones par couche $m$ potentiellement très grand ($m$) : l'approximation universelle est \\'etablie pour toute activation continue qui ne ",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 19
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_20",
      "text": "soit pas un polyn\\^ome (condition n\\'ecessaire et suffisante, travaux de Cybenko et Hornik, 1989 , voir aussi mentionné ci-dessus) ; Pour un nombre de couches qui tend vers l'infini mais $n+1$ neurones par couche : l'approximation universelle est \\'etablie avec l'activation pour toute fonction $f$ Lebesgue int\\'egrable (Lu et al. 2017 ; le nombre de neurones par couche peut \\^etre diminu\\'e si $f$ est continue) ;\\\\ Pour un nombre de couches qui tend vers l'infini mais $n+3$ neurones par couche :",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 20
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_21",
      "text": " l'approximation universelle est \\'etablie avec une activation non affine pour toute fonction $f$ continue ; Pour un cadre r\\'ealiste, avec un nombre de couches et de neurones par couche fix\\'es : Ismailov et Guliyev ont construit une fonction d'activation, du type sigmo\\\" de (donc croissante de 0 \\`a 1 et $^$) qui permet d'atteindre l'approximation universelle par un r\\'eseau \\`a 3 couches et $2n+2$ neurones par couche si $n 2$ (et 2 couches \\`a 2 neurones si $n=1$). La preuve est bas\\'ee sur l",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 21
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_22",
      "text": "e th\\'eor\\`eme de superposition de Kolmogorov ; l'activation est construite algorithmiquement. Pour l'approximation d'une classe beaucoup plus large de fonctions avec des activations dans des r\\'eseaux de taille contr\\^ol\\'ee, on pourra consulter , ou encore (toujours une taille contr\\^ol\\'ee, mais une activation beaucoup plus complexe) . Dans cette section, nous abordons la question de l'approximation des fonctions continues par des réseaux de neurones à couche hypersphérique, en examinant une ",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 22
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_23",
      "text": "approche différente de celle discutée précédemment. \\\\ Nous nous appuyons sur le théorème de Schwartz , qui établit que des fonctions continues à support compact peuvent être approximées de manière uniforme par des combinaisons linéaires de translatées d'une fonction donnée. Cette idée est particulièrement pertinente dans le contexte des réseaux de neurones, car les sorties peuvent être assimilées à une combinaison de fonctions translatées. En ce sens, l'application de ce théorème permet de mont",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 23
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_24",
      "text": "rer que ces réseaux peuvent effectivement approximer des fonctions continues sur des compacts de $$, à travers des combinaisons linéaires de translatées de fonctions spécifiques. De façon analogue à l'équation , la sortie du réseau à couche hypersphérique à $J$ neurones (donc $J$ sphères en dimension $n$) peut être réécrite comme~:\\\\ y() = _{j=1}^{J} _{j}{2} _{i=1}^{n} )}_{(-{2}\\|-\\|^2_{}+{2}_j^2)} \\\\ où $_j $, $_j ^n$, $_j $, $1 j J$.\\\\ On choisit de construire une approche plus directe que la ",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 24
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_25",
      "text": "stratégie utilisée pour le réseau à couche dense du paragraphe précédent. Le prix à payer est de se restreindre au cas des fonctions à une variable ($n=1$).\\\\ Il s'agit de montrer si un réseau composé d'une couche cachée hypersphérique permet l'approximation de fonctions de \\( \\) vers \\( \\). La sortie du réseau à couche hypersphérique dans le contexte où l'on considère les fonctions scalaires s'écrit, avec une entrée à une dimension, de la façon suivante~: y(x) = _{j=1}^{J} _{j} ( -{2} ) avec $_",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 25
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_26",
      "text": "j $, les centres $c_j $, les rayons $_j $, $1 j J$.\\\\ L'architecture du réseau de neurones considéré ($J$ sphères en dimension 1) peut se résumer par le schéma . [H] [shorten >=1pt,->,draw=black!50, node distance=2.5cm] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!120]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] (K-1) at (-2,-1) {$x_1=:x$}; (K-3) at (-2,-3+1/2) {$1$}; (",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 26
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_27",
      "text": "K-4) at (-2,-4+1/2) {${2}$}; / in {1,...,5} (H-) at (0,-+1) { $_{} _i$}; in {1} in {1,...,5} (K-) edge (H-); in {3,4} in {1,...,5} (K-) edge [dashed,red] (H-); (J) at (4,-2) { $y_i = _{j=1}^5 _j(_j._i_i)$}; in {1,...,5} (H-) edge (J); La question de densité dans le cas considéré consiste à montrer qu'une combinaison linéaire de sorties de neurones d'un couche hypersphérique peut approcher l'ensemble des fonctions continues à support compact au sens de la convergence uniforme. Autrement dit, l'en",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 27
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_28",
      "text": "semble $ \\{(-{2});c, ^{*}\\}$ est-il dense dans l'ensemble des fonctions continues à support compact, soit \\{(-{2});c, ^{*}\\}}= ^0() , pour toute activation $ ^0()$, au sens de la norme uniforme sur les compacts ? On commence par énoncer le théorème suivant, dû à Schwartz {Schwartz}~:\\\\ [colframe=blue, colback=white!10, title=] {Schwartz} Si $$ et $$ sont deux fonctions continues à supports compacts, $ 0$, alors $$ est limite uniforme sur tout compact de combinaisons linéaires des translatées de ",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 28
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_29",
      "text": "$$. Un énoncé équivalent, où on reconnaît déjà la structure de la sortie d'un réseau de neurones, est le suivant. Soit \\( \\) un compact de \\( \\). Il existe donc des couples \\( (a_j, _j) ^2 \\) pour \\( j \\), tels que~: _{J + } _{x K} (x) -^{J}_{j=1} _j (x-a_j)=0 . \\\\ Soit $:=\\{(x-a), a\\}$. Chaque élément de la somme dans appartenant à $$, la limite de cette somme pour $J +$ est dans l'adhérence $}$ de $$. Cette limite $$ étant par ailleurs dans $C^0_c()$ ^0_c()$ l'ensemble des fonctions continues ",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 29
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_30",
      "text": "à support compacts}, on déduit du théorème de Schwartz que $$C^0_c() }$$ Tout élément de $$ étant une fonction continue à support compact alors $ C^0_c()$. Il suit que $} C^0_c()$. Cette inclusion réciproque conduit à \\{(x-a); a\\}}= ^0_c() Pour établir un théorème d'approximation dans notre cas spécifique, on va appliquer cette propriété à deux cas distincts~: Les rayons des sphères sont fixés et égaux. Les rayons des sphères ne sont pas fixés (cas général). On commence par le cas des rayons fix",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 30
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_31",
      "text": "és. Pour $ $ donné, on introduit la fonction $_{}$ définie sur $$ par~: $$_{}(x)= (-{2}).$$ Une sortie hypersph\\'erique centr\\'ee en $c $ s'\\'ecrit comme une translat\\'ee de $_$ : $$_{}(x-c) = (-{2})$$ On suppose que $^{0}_{c}()$ et $ 0$, le Th\\'eor\\`eme 1 s'applique et assure l'existence de $(c_j,_j) ^2$, $j $, tels que :\\\\ $$ _{J} _{x K} (x) -^{J}_{j=1} _j (-{2}) =0 $$ ce qui permet de conclure en reprenant un raisonnement analogue à l'obtention de l'équation que~: pour tout $ $ donné _}= ^0_c",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 31
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_32",
      "text": "() où $_ = \\{(-{2});c\\}$. [{ Lien avec les fonctions à bases radiales (RBF)}] Ici on peut faire le lien entre un réseau à couche hypersphérique (RHS) et un réseau de fonctions de base radiales (RBFN). La sortie d'un RBFN s'exprime comme : y() = _{j}^J _j (_j \\| - _j\\|) La fonction $$ est dite à base radiale car elle ne dépend que de la distance euclidienne entre $$ et $$ dans $^n$. Pour rapprocher les sorties des deux types de réseaux, les sphères doivent être dégénérées (\\( _j = 0 \\)) en points",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 32
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_33",
      "text": ". L'activation qui suit la couche hypersphérique joue un rôle analogue à celui du noyau dans les fonctions à base radiale. La sortie d'un RHS pour l'activation $ (x) = x $ (resp. $ (x) = e^x $) est par exemple proportionnelle à la sortie d'un RBFN en utilisant le noyau quadratique $ (x) = x^2 $ (resp. le noyau $ (x) = e^{-{2}x^2} $). { En prenant $=0$ dans le résultat précédent, on re-prouve donc le fait que le RBFN est également un approximateur universel en dimension 1 (conformément à ).} Pass",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 33
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_34",
      "text": "ons au cas g\\'enéral où les rayons ne sont pas fixés. Soit $$ une fonction continue à support compact. D'après ce qui précède en choisissant $ =0$ dans , elle peut être utilisée comme fonction d'activation pour approcher $f$ sous la forme $$ f(x) _{j=1}^{J_1} _j ( (x-c_j)^2 ) $$ pour certains $J_1 $, centres $c_j $ et coefficients $_j $, $1 j J_1$. Par ailleurs, $$ peut aussi être approchée { via} des translat\\'ees de l'activation $$ grâce au théorème de Schwartz ( On pose $X_j = (x-c_j)^2 $): $",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 34
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_35",
      "text": "$ (X_j) _{j'=1}^{J_2} _{j'} ( -{2} (X_j - _{j'}^2) )$$ pour certains $J_2 $, rayons $_{j'} $ et coefficients $_{j'} $, $1 j' J_2$. En assemblant ces deux dernières approximations, on obtient $$ f(x) _{j=1}^{J_1} _j _{k=1}^{J_2} _k ( -{2} ((x-c_j)^2 - _k^2) ) $$ qui peut se ré-écrire plus simplement $$ f(x) _{j=1}^{J} _j ( -{2} ((x-c_j)^2 - _j^2) ) .$$ Finalement on a donc établi le théorème suivant~: \\\\ [colframe=blue, colback=white!10, title=] Soit $ _c^{0}()$ une fonction d'activation continue",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 35
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_36",
      "text": " à support compact non identiquement nulle. Alors, $_ = \\{(-{2});c\\}$ pour tout $ $, et $_{c} = \\{(-{2});c, \\, \\}$ sont denses dans $^{0}()$ au sens de la convergence uniforme sur un compact. Le théorème précédent a été démontré pour une activation $ C^0_c() $. Mais cette hypothèse peut être relaxée. Le résultat se généralise par exemple au cas où $$ L^1()$$ pour peu qu'on remplace la norme uniforme par la norme $L^1$. En effet, pour le prouver, il suffit d'utiliser le fait que l'espace de Lebes",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 36
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_37",
      "text": "gue $L^1$ est par définition l'adhérence de l'ensemble des fonctions continues à support compact pour la norme $L^1$ et le résultat du théorème précédent. Les expérimentations menées n'ont pas pour but d'offrir une étude exhaustive des capacités d'approximation des réseaux de neurones, mais plutôt de valider empiriquement les résultats théoriques établis précédemment. Dans cette optique, nous présentons ci-dessous les résultats obtenus pour différentes fonctions tests. Afin de vérifier expérimen",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 37
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_38",
      "text": "talement la conclusion établie dans la partie précédente (Théorème ), l'approximation de fonctions continues à support compact à une variable a été testée. Pour cela, des réseaux de \\( J \\) neurones sur une couche cachée de type hypersphérique ou dense ont été utilisés. Pour chaque réseau, on a testé des valeurs de $J$ de 2 à 64. Comme fonction de perte, l'erreur quadratique moyenne (EQM) a été utilisée, et le jeu de données (composé de 512 points) a été séparé selon les proportions suivantes : ",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 38
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_39",
      "text": "\\( {2} \\) pour l'entraînement, \\( {4} \\) pour la validation et \\( {4} \\) pour le test. Les données ont été échantillonnées sur l'intervalle \\( [-0.05, 1.05] \\). L'erreur finale a été évaluée sur le jeu de test.\\\\ Dans les tests numériques qui vont suivre, différentes fonctions d'activation ont été testées puis sélectionnées et appliquées aux neurones de la couche cachée (cf. Fig )~: (sans activation)} $$ ~: & \\\\ x & x $$ (Rectified Linear Unit)} $$ ~: & [0, + \\\\ x & (0, (1, {5} + {2})) $$ (fonct",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 39
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_40",
      "text": "ion de base radiale)} $$ ~: & ]0,1] \\\\ x & e^{-x^2} $$ )} $$ ~: & ]-1,1[ \\\\ x & }{e^x + e^{-x}} $$ } $$ ~: & ]0,2[ \\\\ x & (x) + 1 $$ } $$ ~: & [-1,1] \\\\ x & (x) $$ _{x} \\)} $$ ~: & \\\\ x & ( ._i }{}) e^{-( ._i }{})^2}, . $$ La figure montre l'évaluation des fonctions d'activation sur le produit interne $._i$, en considérant une sphère $$ de centre 0 et de rayon 1 et en faisant varier $x$ sur l'intervalle $[-,]$. En pointillé, sont représentées les fonctions d'activations usuelles. La courbe verte",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 40
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_41",
      "text": " correspondant à l'activation \"linear\" montre le produit interne $._i$, c'est-à-dire la sortie du neurone sans activation. [htpb] $} Les preuves précédentes ont été construites en commençant par utiliser des fonctions d'activation à support compact. Dans le but de se rapprocher au plus près des conditions, l'attention a été portée sur les fonctions \\( L^2 \\) qui s'annulent au bord d'un compact sur lequel on souhaite approcher une fonction. C'est pourquoi la fonction \\( (x) := (x) + 1 \\) a été aj",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 41
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_42",
      "text": "outée. De même, la fonction \\( _{xd} \\) a été introduite, possédant la propriété de conserver le signe de \\( ._i \\). Enfin, la fonction sinus a été incluse comme non-linéarité, car elle permet de modéliser efficacement des signaux complexes . Dans un premier temps, l'objectif a été d'approcher deux fonctions simples construites pour être à support compact : la fonction Triangle, facilement approchable par une couche dense, et la fonction Omega, correspondant à l'activation d'une couche à un neur",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 42
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_43",
      "text": "one hypersphérique. Dans un deuxième temps, deux fonctions plus complexes ont été étudiées : la fonction \\( \\), qui est la combinaison linéaire (+ ) de 32 sphères aléatoires, et la fonction \\( ({x}) \\), qui oscille fortement autour de 0. Chaque fonction à approcher a été mise à 0 en dehors de l'intervalle \\( K=[0, 1] \\). La figure illustre les fonctions considérées.\\\\ [H] \\\\ La figure montre l'approximation des fonctions Triangle et Omega. La légende indique qu'il est nécessaire de monter à 64 n",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 43
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_44",
      "text": "eurones pour bien approximer également pour les intervalles $[-0.05. 0]$ et $[1, 1.05]$, c'est-à-dire en dehors du compact $K=[0,1]$. [htbp] Il est à noter que le modèle de neurones à couche hypersphérique permet de réduire les erreurs au bord, mais présente quelques oscillations sur les parties purement linéaires de la fonction. Cependant, avec la fonction d'activation \\( _{x} \\), une EQM de \\( 8 10^{-6} \\) est obtenue, inférieure à l'EQM maximale obtenue dans le cas des réseaux à couche dense,",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 44
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_45",
      "text": " qui est égale à \\( 2 10^{-5} \\). \\\\ La difficulté d'entraîner les modèles à couche hypersphérique est également notable. En effet, pour la fonction Omega, construite à partir du produit interne avec une hypersphère suivi d'une activation , on pourrait s'attendre à ce qu'un seul neurone suffise pour approximativement retrouver les paramètres de l'hypersphère. Cependant, cela n'est pas le cas, et il faut au moins deux neurones (deux hypersphères) pour obtenir une approximation efficace.\\\\ Les gra",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 45
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_46",
      "text": "phiques et représente les EQM, en fonction du nombres de neurones pour les fonctions plus complexes. Les courbes continues représentent les résultats obtenus avec les modèles de neurones à couche hypersphérique, tandis que les courbes en pointillés représentent les résultats obtenus avec les modèles à couche dense.\\\\ [H] [H] Les EQM restent dans les mêmes ordres de grandeur, quel que soit le modèle utilisé. On peut observer que pour l'approximation de $Sph32$, l'utilisation de la fonction d'acti",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 46
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_47",
      "text": "vation $sinus$ permet d'obtenir de bons résultats avec peu de neurones. Cependant, que ce soit pour les modèles à couche dense ou hypersphérique, en augmentant suffisamment le nombre de neurones, l'utilisation de la fonction d'activation $Gauss_{x}$ a permis de réduire les erreurs obtenues pour l'ensemble des fonctions que nous avons approximées.\\\\ L'approximation de fonctions a également été testée en fixant le rayon \\( \\) des sphères. Aucun changement significatif n'a été noté sur les résultat",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 47
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_48",
      "text": "s obtenus, les EQM restant toujours dans les mêmes ordres de grandeur.\\\\ Dans ce chapitre, on a choisi de faire un focus sur l'approximation de fonctions scalaires. On sait en effet (voir par exemple le Th. 3.2 dans Pinkus ) qu'un résultat d'approximation des fonctions de $$ dans $$ peut être étendu aux fonctions de $^n$ dans $^{n'}$ avec $n,n' $ quelconques. \\\\ Les résultats expérimentaux permettent de confirmer le résultat théorique suivant : un réseau de neurones à une couche hypersphérique e",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 48
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_49",
      "text": "st capable d'approcher en norme uniforme sur tout compact toute fonction continue de \\( \\) à support compact. Une nouvelle fonction d'activation \\( }_{x} \\) a été proposée. Il a également été vérifié que l'approximation est possible, malgré le fait que les fonctions et ne soient pas à support compact (mais \\( L^2 \\)), ce qui correspond au résultat théorique du paragraphe (on rappelle que la convergence en norme $L^1$ implique la convergence presque partout).\\\\ Les expériences menées comparent au",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 49
      },
      "embedding": []
    },
    {
      "id": "chapitre2.tex.txt_chunk_50",
      "text": "ssi les performances des approximations obtenues avec des couches hypersphériques et des couches denses classiques. \\\\ Par rapport à la démonstration présentée dans l’article , nous avons proposé une approche simplifiée en allégeant certains aspects techniques de la preuve initiale. Les résultats théoriques comme les expérimentations ont permis d'illustrer les qualités d'approximation universelle des réseaux hypersphériques.\n\n",
      "metadata": {
        "source_file": "chapitre2.tex.txt",
        "chunk_index": 50
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_1",
      "text": "Après un bref état de l'art sur les méthodes de d\\'etection d'anomalies, ce chapitre explore l'application de couches hypersphériques dans les réseaux neuronaux pour ce type d'application, en mettant l'accent sur les techniques de Support Vector Data Description (SVDD) et de Deep SVDD. Une adaptation du Deep SVDD est introduite en incorporant après la dernière couche linéaire une couche hypersphérique définie dans l'algèbre géométrique conforme. De plus, une nouvelle méthode appelée Deep M-SPH S",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_2",
      "text": "VDD est proposée afin d'étendre l'approche à des multi-sphères, permettant ainsi au modèle de capturer des groupes distincts de points de données normales.\\\\ De nouvelles fonctions de perte conçues pour éviter l'intersection et l'inclusion des sphères sont également présentées. \\\\ Des expériences préliminaires sont menées sur un ensemble de données synthétiques, ainsi que des évaluations sur les ensembles de données MNIST et CIFAR-10. Ces comparaisons sont utilisées pour évaluer la performance d",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_3",
      "text": "e la méthode proposée et déterminer s'il est préférable, pour un nombre fixe de paramètres, d'utiliser une seule hypersphère de haute dimension ou plusieurs hypersphères de dimension inférieure. La détection d'anomalies est une \\'etape souvent cruciale dans l'exploration de données, y compris pour la phase de pr\\'eparation d'un \\'eventuel apprentissage. Le but est d'identifier des points de données significativement différents des autres dans un ensemble, souvent appelés valeurs aberrantes ou an",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_4",
      "text": "omalies. Ces points de données ne se conforment pas au modèle de distribution ``habituel'', ce dernier \\'etant d'ailleurs difficile \\`a d\\'efinir. Dans certains cas, le concept de valeurs aberrantes est distingué de celui des anomalies . En effet, une valeur aberrante englobe à la fois le bruit et l'anomalie. La détection de nouveauté consiste à repérer des motifs ou comportements inhabituels par rapport à la distribution observée des données d'apprentissage. La détection d'anomalies pose des dé",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_5",
      "text": "fis notables. En effet, les anomalies sont souvent imprévisibles jusqu'à ce qu'elles se produisent, interdisant d'exploiter le contexte. { A contrario}, la normalité d'une donnée peut varier en fonction du contexte. L'anomalie est ainsi difficile \\`a d\\'efinir intuitivement. Si on ambitionne une d\\'etection automatique, d'autres difficultés s'ajoutent. Par exemple, en raison de leur rareté, faible proportion et diversité, il peut parfois être impossible d'étiqueter les anomalies. Le déséquilibre",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_6",
      "text": " entre les classes complique également la détection.\\\\ Il existe plusieurs approches pour la détection d'anomalies. Les différentes méthodes, y compris celles basées sur l'apprentissage profond, sont passées en revue dans . L'article les classe selon trois approches principales : l'apprentissage profond pour l'extraction de caractéristiques, l'apprentissage de représentations de la normalité et l'apprentissage de scores d'anomalies.\\\\ Dans la catégorie de l'extraction de caractéristiques, on ret",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_7",
      "text": "rouve des méthodes visent à réduire les données de haute dimension en représentations caractéristiques de basse dimension. On suppose que ces représentations extraites préservent les informations discriminantes pour aider à séparer les anomalies des données normales, même si le modèle original n'a pas été entraîné pour détecter les anomalies. Une approche courante consiste à utiliser des réseaux pré-entraînés tels que VGG ou RESNET puis transférer les sorties vers un détecteur d'anomalies tels q",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_8",
      "text": "ue SVM ou SVDD .\\\\ L'apprentissage de représentations de la normalité vise à capturer les schémas réguliers suivis par les données, en utilisant des méthodes comme les autoencodeurs et les réseaux antagonistes génératifs (GAN). Par exemple, les méthodes basées sur la reconstruction apprennent à reconstruire les données normales et détectent les anomalies en mesurant la différence entre l'entrée et la sortie. L'apprentissage de scores d'anomalie apprend directement les scores d'anomalie via des r",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_9",
      "text": "éseaux neuronaux, résumant les instances normales avec un modèle discriminant à une classe.\\\\ Les m\\'ethodes d'apprentissage profond doivent permettre de relever les défis de la détection d'anomalies tels que des faibles taux de rappel, des anomalies complexes, la haute dimensionnalité et le déséquilibre des données. Elles ont en particulier l'avantage de pouvoir intégrer des sources de données diverses pour une solution complète. Nous présentons ici un bref état de l'art des méthodes de détecti",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_10",
      "text": "on d'anomalies, en mettant l'accent sur la définition d'anomalie propre à chacune des méthodes. Cette méthode de détection d'anomalie repose sur l'hypothèse que les données normales suivent une distribution normale multivariée, tandis que les anomalies s'écartent de cette distribution. Il s'agit donc d'une méthode paramétrique qui se base sur une estimation robuste de la matrice de covariance des données (Minimum Covariance Determinant) . Rappelons que la matrice de covariance est une mesure de ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_11",
      "text": "la dispersion des données autour de leur moyenne, et qu'elle est utilisée pour déterminer la forme de la distribution des données. La distance de Mahalanobis est définie par la formule suivante~: $$D_M(, ) = - )^ ^{-1} ( - )}$$ où $$ est le point à comparer, $$ est la moyenne du groupe de points, et $$ est la matrice de covariance du groupe de points. On remarque que cette distance tient compte de la corrélation entre les variables, contrairement à la distance euclidienne ($ = I$).\\\\ L'algorithm",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_12",
      "text": "e recherche un sous-ensemble de $h$ points qui minimise le déterminant de la matrice de covariance, ce qui revient à identifier la région la plus compacte et donc la plus représentative de la distribution principale des données -- celle où la densité de points normaux est la plus élevée. Pour garantir une estimation robuste et statistiquement valide, la taille $h$ du sous-ensemble doit satisfaire la condition suivante : $$ h ( (1 - )K ,\\; {2} ) $$ où $K$ est le nombre d'observations en dimension",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 12
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_13",
      "text": " $m$ et $$ la proportion maximale d'anomalies que l'on souhaite tolérer. Plusieurs sous-ensembles de taille $h$ sont générés aléatoirement, et chacun est raffiné par itérations successives afin d'approcher la configuration minimisant effectivement le déterminant. Pour chaque ensemble: La matrice de covariance et la moyenne sont calculées, et la distance de Mahalanobis est calculée pour chacun des $h$ points. Parmi les $n$ points initiaux, les $h$ points ayant plus petites distances sont choisis ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 13
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_14",
      "text": "pour former un nouveau sous-ensemble. On remarque qu'un algorithme de partitionnement est suffisant pour cette étape; il n'est pas nécessaire d'effectuer un tri complet. Les étapes 1. et 2. sont répétées jusqu'à ce que le déterminant de la matrice de covariance ne diminue plus ou est nul. Le sous-ensemble final est considéré comme robuste. Le sous-ensemble final est celui qui minimise le déterminant de la matrice de covariance.\\\\ Une version rapide de l'algorithme, appelée FastMCD , accélère enc",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 14
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_15",
      "text": "ore le processus de recherche par l'utilisation de deux heuristiques: \\'Elagage précoce des sous-ensembles les moins prometteurs après deux étapes de l'algorithme précédent. Lorsque le nombre de points est grand (i.e.\\ supérieur à 600 en 1999), une stratégie de type ``diviser pour régner'' a été proposée afin de repartir la recherche de sous-ensembles support avant d'en fusionner les meilleurs et de faire un raffinement final. Le score d'anomalie est calculé à partir de la distance de Mahalanobi",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 15
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_16",
      "text": "s. Les points dont la distance dépasse un seuil prédéfini sont considérés comme des anomalies potentielles. Cette méthode est sensible aux données aberrantes et aux violations de l'hypothèse de normalité multivariée. Elle est implémentée dans la librairie scikit-learn sous le nom Elliptic Envelope. L'algorithme Isolation Forest (iForest) est conçu pour détecter les anomalies dans un ensemble de données en utilisant une approche basée sur l'isolement. La méthode commence par construire un ensembl",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 16
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_17",
      "text": "e d'arbres d'isolation (iTrees), chacun étant un arbre de décision binaire qui partitionne aléatoirement les données.\\\\ Pour chaque arbre d'isolation, l'algorithme sélectionne aléatoirement un attribut (une caractéristique des données) et une valeur de séparation parmi les valeurs possibles de cet attribut (entre le minimum et le maximum). Ce découpage crée deux sous-groupes de données, que l'on continue à partitionner récursivement jusqu'à ce que chaque point de donnée soit isolé. Le nombre de ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 17
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_18",
      "text": "partitions nécessaires pour isoler un point $x$ est égal à la longueur $h(x)$ du chemin de la racine de l'arbre à ce point. Les anomalies, étant plus rares et séparées des autres points, seront en moyenne isolées plus rapidement, ce qui se traduit par des longueurs de chemin plus courtes. À partir d'un certain nombre d'arbres construits, l'algorithme estime la longueur moyenne \\( E(h(x)) \\) du chemin pour chaque point $x$ dont il résulte un score d'anomalie suivant: $$ (x) = 2^{-{c(K)}} $$ où $c",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 18
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_19",
      "text": "(K) = 2H(K-1)-{K}$ représente la longueur moyenne du chemin d'une recherche infructueuse dans un arbre binaire équilibré avec $K$ points avec $H(i)$ le i-ème nombre harmonique.\\\\ Les points dont le score est proche de 1 (ou supérieur à un seuil prédéfini) sont considérés comme des anomalies potentielles (car facilement isolées, i.e. $E(h(x))$ tend vers 0). À l'opposé, les points normaux obtiennent un score bien inférieur à 0.5 (car nécessitant plus de partitions pour être isolés). Si les données",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 19
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_20",
      "text": " sont uniformément réparties dans l'espace des caractéristiques, le score moyen est alors proche de 0.5 pour peu que le nombre d'arbres soit suffisamment grand. L'algorithme est rapide car hautement parallélisable, efficace et robuste aux valeurs aberrantes. L'algorithme Local Outlier Factor (LOF) est une méthode de détection d'anomalies qui identifie des points aberrants en comparant leur densité locale à celle de leurs voisins. L'idée principale est que les anomalies se trouvent dans des régio",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 20
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_21",
      "text": "ns de faible densité locale par rapport à leur voisinage immédiat. On doit donc se doter en premier lieu d'une distance $D$ entre les points, puis construire une mesure de densité locale à partir des distances entre les points et leurs $k$ plus proches voisins. Pour cela, on note $D_k(p)$ la distance d'un point $p$ à son $k$-ième le plus proche voisin. La distance d'accessibilité (RD) entre deux points $p$ et $q$ est définie comme $$_k(p, q) = \\{D_k(q), D(p, q)\\}$$ On aura remarqué l'asymétrie d",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 21
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_22",
      "text": "e la formule. Ainsi, tous les $k$ plus proches voisins de $q$ seront considérés comme équivalents pour $q$ suivant RD. La densité d'accessibilité locale (LRD) est définie comme l'inverse de la distance moyenne d'accessibilité de $p$ par rapport à ses voisins $q$: $$_{k'}(p) = ({k'} _{q N_k(p)} (p, q))^{-1}$$ où $N_{k'}(p)$ est l'ensemble des $k'$ plus proches voisins de $p$ où $k'$ est un hyper-paramètre désignant le nombre minimal de points pour former un groupement. Cette mesure permet de quan",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 22
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_23",
      "text": "tifier la densité locale de $p$ par rapport à ses voisins et sera d'autant plus grande (resp. faible) que $p$ est entouré de points proches (resp. éloignés).\\\\ Le score LOF d'un point $p$ est obtenu en normalisant la densité locale moyenne autour de $p$ par rapport à celle de $p$: $$_{k'}(p) = ( (p)} _{k'}(q)}{k'}) / _{k'}(p)$$ Si le score satisfait $_{k'}(p) > 1$, alors la densité du point $p$ est faible et le label correspondant à une anomalie lui est attribué. Une illustration est donnée dans",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 23
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_24",
      "text": " la figure où le rayon de chaque cercle est proportionnel au score d'anomalie de son centre. On constate que les points les plus éloignés des autres obtiennent un score élevé. [htbp] La méthode One-class SVM (OC-SVM) est une méthode de détection d'anomalies qui repose sur l'apprentissage non supervisé. Elle est basée sur les machines à vecteurs de support (SVM) qui sont classiquement utilisées pour la classification binaire supervisée. En modifiant la fonction à optimiser, on peut adapter les SV",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 24
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_25",
      "text": "M à un problème de détection d'anomalies. Commençons par décrire les machines à vecteurs de support (SVM) . Les machines à vecteurs de support (SVM) sont des modèles d'apprentissage supervisé qui peuvent être utilisés pour la classification ou la régression. Pour la classification binaire, l'objectif des SVM est de trouver un hyperplan qui sépare les données en deux classes. On distingue deux types de SVM~: Les SVM linéaires, qui cherchent à séparer les données par un hyperplan linéaire. Les SVM",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 25
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_26",
      "text": " à noyaux qui permettent de séparer non linéairement les données à l'aide d'une fonction noyau. Le cas du noyau linéaire nous ramène au premier cas. Lorsque les données sont linéairement séparables, il existe une infinité d'hyperplans qui peuvent séparer les données. Parmi ces hyperplans, l'unique plan qui maximise la marge de séparation entre les deux classes est appelé hyperplan optimal. La marge est définie comme la distance entre l'hyperplan et les points les plus proches de chaque classe. C",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 26
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_27",
      "text": "ertains points, appelés vecteurs de support, se trouvent sur la marge et définissent l'hyperplan optimal. Nous allons détailler le problème d'optimisation des SVM dans le cas linéairement séparable.\\\\ Soit $$ un hyperplan de $^m$ défini par l'équation $ + w_0 = 0$. La fonction de décision est la suivante: y( ) = sgn ( . + w_0) Pour un point générique $ ^m$, on peut le décomposer en deux parties~: sa projection $_{}$ sur l'hyperplan $$ et un vecteur orthogonal à cet hyperplan colinéaire à $$ (voi",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 27
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_28",
      "text": "r figure ). Afin de quantifier la marge de séparation, on introduit la distance signée $r$ d'un point $$ à l'hyperplan~: $$r() = . + w_0}{||||}$$ [H] Après normalisation de $$ et $w_0$, les points se situant sur la marge coté positif (resp. négatif) se trouvent à une distance signée de 1 (resp. -1). Maximiser la marge tout en classant correctement l'ensemble des points $\\{(x_k,y_k) \\{1, -1\\})\\}_k$ s'écrit comme un problème de maximisation sous contraintes, à savoir $$,w_0)}{} {||||} y_k (. + w_0",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 28
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_29",
      "text": ") 1 k \\{1,,K\\} $$ On remarquera que les contraintes intègrent naturellement le cas des exemples positifs et négatifs. Ce problème peut se reformuler comme le problème de minimisation sous contraintes suivant: ,w_0)}{} {2} ||||^2 \\\\ & y_k (. + w_0) 1 k \\{1,,K\\} La solution de ce problème est unique en raison de la convexité stricte du premier terme et de la convexité des contraintes . Le lecteur intéressé par la résolution effective peut se reporter à ce même ouvrage. Lorsque les données ne sont ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 29
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_30",
      "text": "pas linéairement séparables dans l'espace initial, on cherche un autre espace de représentation, généralement de plus grande dimension, dans lequel on espère faciliter la séparabilité des données par un hyperplan. Pour effectuer cette transformation, nous utilisons une fonction de plongement \\(\\), illustrée dans la figure .\\\\ En effet, le premier sous-graphique montre un ensemble de données non linéairement séparable dans un espace 2d. Les points rouges et bleus représentent deux classes distinc",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 30
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_31",
      "text": "tes. La distribution des données est telle qu'il est impossible de tracer une ligne droite qui sépare les deux classes de manière correcte. Cette disposition montre clairement qu'une séparation linéaire est impossible dans cet espace.\\\\ Le second sous-graphique montre les mêmes données après qu'elles aient été projetées dans un espace de dimension 3 via un plongement explicite. Cela signifie que chaque point \\( (x_1, x_2) \\) est transformé en un vecteur dans l'espace de caractéristiques par la f",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 31
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_32",
      "text": "onction suivante de \\( ^2 ^3 \\) : $$ (x_1, x_2) = (x_1^2, x_2^2, 2 x_1 x_2) $$ Cette projection transforme les coordonnées \\((x_1, x_2)\\) de chaque point en une représentation dans un espace à trois dimensions. Comme on peut le voir, le plan en jaune permet une séparation linéaire des données dans cet espace transformé. Le graphique illustre donc le principe des méthodes à noyaux pour traiter des problèmes non linéairement séparables. [htbp] Dans le cadre de ce manuscript, nous nous limitons au ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 32
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_33",
      "text": "cas où $$ est connu explicitement puisque réalisée par un réseau de neurones. Dans ce cadre, l'espace $F$ des caractéristiques qui est l'image de $^n$ par $$ est un sous-ensemble de $^m$. Le problème d'optimisation pass\\'e dans l'espace caractéristique $F$ devient : ,w_0)}{{}} & {2} ||||^2 \\\\ & y_k( . (_k) + w_0) 1 , k \\{1,,K\\} où le vecteur $w$ est maintenant un vecteur de $^m$.\\\\ Le lagrangien associ\\'e \\`a est donn\\'e par : (,w_0,) = {2} ||||^2 - _{k=1}^{K} _k (y_k -1) où les $_k 0$ sont les ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 33
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_34",
      "text": "multiplicateurs de Lagrange. En observant le terme de droite dans cette minimisation, on constate que pour tout point $_k$ qui n'étant pas dans la marge ($y_k -1 > 0$), la solution doit vérifier $_k = 0$ pour être un minimum. La recherche des points critiques annulant la diff\\'erentielle du lagrangien ($(,w_0,)}{ } = 0$ et $(,w_0,)}{ w_0} = 0$) amène les conditions suivantes :\\\\ = _{k=1}^{K} _k y_k (_k)\\\\ 0 = _{k=1}^{K} _k y_k \\\\ D'après notre remarque précédente, on en déduit que seul les point",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 34
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_35",
      "text": "s sur la marge, vérifiant $y_k -1 = 0$, contribuent à la définition de $$. Ils sont appelés vecteurs de support. En remplaçant $$ par $_{k=1}^{K} _k y_k (_k)$ dans le lagrangien, on élimine les variables $$ et $w_0$ pour obtenir le problème d'optimisation (dit dual) suivant:\\\\ () = _{k=1}^{K} _k -{2}_{k, k'=1}^{K} _k_{k'} y_k y_{k'} (_k). (_{k'}) \\\\ \\\\ _{k=1}^{K} _k y_k =0 _k 0 k \\{1,,K\\} La prédiction initiale (équation ) dans l'espace dual s'écrit : y() = sgn (_{k=1}^{K} _k y_k ().(_k)+w_0) = ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 35
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_36",
      "text": "sgn (_{_k SV} _k y_k ().(_k)+w_0) \\\\ où $SV$ désigne l'ensemble des vecteurs de support. variables\")} En pratique, l'existence d'un hyperplan séparateur n'est pas garantie malgré un plongement dans un espace de dimension supérieure, dans le cas d'étiquetage contradictoire, . Afin d'autoriser à violer la contrainte $y_k(. + w_0) 1$, on va introduire des nouvelles variables $ 0$ permettant cela. Le problème d'optimisation ~: ,w_0)}{} {2} ||||^2 \\\\ \\\\ y_k( . _k + w_0) 1 k \\{1,,K\\} \\,. devient ,w_0,",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 36
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_37",
      "text": " _k)}{} {2} ||||^2 + C_{k=1}^{K} _k\\\\ \\\\ y_k( . _k + w_0) 1-_k, _k 0, k \\{1,,K\\} \\,. où $C > 0$ est un hyper-paramètre de la méthode. On notera que: L'exemple est bien classé si et seulement si $_k 0; 1]$ est un paramètre de la $ SVM$ permettant obtenir des garanties statistiques sur la marge. En fixant le paramètre de $$ à 1 dans la $ SVM$, on établit une équivalence entre SVM classique et ce cas particulier. Il suit que : \\(\\) est une borne supérieure sur la fraction des points d'apprentissage",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 37
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_38",
      "text": " qui peuvent être mal classés (i.e., ceux pour lesquels \\(_k > 1\\)). \\(\\) est une borne inférieure sur la fraction des points qui sont des vecteurs supports (i.e., ceux pour lesquels \\(_k > 0\\)). Cette paramétrisation par $$ est ainsi plus naturelle.\\\\ En observant les contraintes dans l'équation , on remarque que soit $_k=0$, soit $_k 1 - y_k( . _k + w_0)$. On peut intégrer directement ces contraintes dans le critère à optimiser: ,w_0)}{} {2} ||||^2 + C_{k=1}^{K} (0, 1 - y_k( . _k + w_0))\\\\ ce ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 38
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_39",
      "text": "qui correspond à l'écriture d'une fonction de perte { hinge}. \\\\ Dans la section précédente, nous avons décrit un algorithme de classification binaire basé sur la maximisation de la marge. On se place maintenant dans le cas où le jeu de données est constitué principalement de données normales. L'objectif de décrire ces données de façon à pouvoir identifier d'autres qui s'en écarteraient. L'algorithme One Class SVM (OC-SVM) repose sur la recherche de l'hyperplan $(w, 0)$ qui sépare les données de",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 39
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_40",
      "text": " l'origine (voir figure ). [htbp] Un nouveau terme $$, assimilable à une distance de l'hyperplan à l'origine apparaît dans la minimisation : , , _k)}{} {2} ||||^2 + C_{k=1}^{K} _k - \\\\ \\\\ ._k -_k, _k 0, k \\{1,,K\\} \\\\ Le paramètre $C = { K}$, avec $ ]0, 1]$, contrôle toujours le taux de points du mauvais coté de l'hyperplan via les variables de tolérance $_k$. La fonction de décision (normal / anormal) met en lumière le lien entre $$ et un potentiel terme $w_0$ (cf. équation ) y() = sgn (_{k=1}^{",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 40
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_41",
      "text": "K} _k ().(_k) - ) On serait tenté donc d'écrire $ = -w_0$ et garder l'optimisation de l'algorithme précédent. Toutefois, on remarque que l'on maximise à la fois la marge (via la minimisation de $||w||^2/2$) et $$ (distance signée à l'origine) (cf. figure ). Comme précédemment, on peut transformer les contraintes par la fonction convexe de perte { hinge} pour en faire un problème d'optimisation non contrainte: , )}{} {2} ||||^2 - + C_{k=1}^{K} (0, - . _k)\\\\ Cette formulation peut être rapprochée ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 41
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_42",
      "text": "de la formulation classique du problème de classification binaire avec SVM (équation ), où un terme de biais est également présent. Dans le cadre des OC-SVM, ce terme de biais \\(w_0\\) permet d'ajuster l'hyperplan de telle sorte qu'il ne passe pas nécessairement par l'origine mais optimise la marge par rapport à l'ensemble des données normales, considérées comme appartenant à une seule classe. Les méthodes de type SVM sont connues pour être qualitativement performantes mais avoir du mal à passer ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 42
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_43",
      "text": "à l'échelle. Afin de remplacer l'optimisation du problème primal ou dual, il est nécessaire d'en adapter la formulation. Dans , le problème d'optimisation est réécrit pour une approche par les réseaux de neurones en réinjectant les contraintes à l'aide des variables { slack}. Le probl\\`eme revient \\`a un problème d'optimisation quasi-quadratique. En effet, en tenant compte de l'inégalité ^t _k + w_0 _k$} (la marge est à $1$ de la frontière de décision) et de la condition $_k 0 $, les contraintes",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 43
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_44",
      "text": " peuvent être retrouv\\'ees en introduisant le maximum entre 0 et $\\{1 - ^t _k + w_0\\}$. La solution du problème minimise donc aussi la fonction de coût suivante : $$(, w_0; \\{_k\\}_k) = {2} ||||^2 + C_{k=1}^{K} \\{0,1- ^t _k+w_0 \\} + w_0$$ Le réseau de neurones consiste en une simple couche linéaire avec un bias. Cela revient à utiliser la fonction de perte de { hinge} (terme de droite) avec un terme de régularisation. Il est naturel de se demander \\`a quel point la solution obtenue par la résolut",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 44
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_45",
      "text": "ion du problème d'optimisation convexe (via l'implémentation dans ) diff\\`ere de celle obtenue par une descente de gradient pour la fonction de perte ci-dessus. Une illustration est pr\\'esent\\'ee dans la figure pour un jeu de données synthétiques en dimension 2. Après avoir extraites les équations cart\\'esiennes ($y = a x + b$) des droites correspondantes aux frontières de décision pour les solutions trouvées, on observe que la différence entre les valeurs des $a$ (resp. des $b$) est de l'ordre ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 45
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_46",
      "text": "de $1 10^{-3}$ (resp. $1 10^{-2}$). On peut donc conclure à une quasi-equivalence des deux méthodes sur ce jeu de données. [htbp] {0.49} {0.49} \\`A ce stade, et même si nous n'avons pas poursuivi l'idée, on peut imaginer adapter l'idée de la méthode OC-SVM au cas d'une sphère qui sépare le mieux le point euclidien $$ ($e_0$ dans le modèle conforme) des données en maximisant son rayon: {} {2} ^2 - C_{k=1}^{K} _k \\\\ \\\\ _i _k _k, _k 0 k \\{1,,K\\}, \\\\ avec $C={ K} >0$ et le produit int\\'erieur not\\'e",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 46
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_47",
      "text": " ici $_i$. On remarque immédiatement que ne pas fixer le centre aboutit à un problème mal posé; le centre pourrait sans doute être fixé ailleurs qu'en $$. L'optimisation par Adam ne s'effectue que sur le rayon alors que le centre de la sphère sera fixé et minimise la fonction de perte $-{2} ^2 + { K} _k \\{0, _i _k\\}$. La figure montre les résultats obtenus pour différentes valeurs de $$ sur les données précédentes. De manière cohérente, on constate que le taux de couverture des points augmente e",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 47
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_48",
      "text": "n même temps de $$. [ht] [b]{0.32} [b]{0.32} [b]{0.32} Les méthodes précédemment décrites cherchaient à séparer par un hyperplan (ou une hypersphère) les données de l'origine. Nous nous intéressons maintenant au cas où l'on cherche à englober les données dans l'hypersphère la plus petite possible avec une marge de tolérance. Cette approche nous est apparue comme la plus à même d'être développée dans le cadre de l'algèbre conforme. Nous allons commencer par décrire la méthode Support Vector Data ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 48
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_49",
      "text": "Description (SVDD) puis allons voir comment elle peut être adaptée à l'apprentissage profond à l'instar de ce que nous avons vu pour la \"Deep One Class SVM\". La méthode Support Vector Data Description (SVDD) vise à trouver les paramètres de centre et de rayon ($c$, $$) de la plus petite hypersphère englobante pour un ensemble de points. La formulation du problème introduit les variables de relaxation qui permettent à certains points de violer ces contraintes et de se retrouver en dehors de l'hyp",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 49
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_50",
      "text": "ersphère. Comme dans le cas des SVM, le travail est fait apr\\`es plongement par $$ des donn\\'ees dans un espace de caractéristiques. Pour ${ccccc} & : & ^n & & F $ une fonction de plongement associée au noyau $$, une sphère de centre $ F$, de rayon $ $, et $C = 1/ K$ la variable de contrôle avec $ ]0,1]$, le problème d'optimisation associ\\'e \\`a une SVDD s'écrit ainsi~:\\\\ ,, )} ^2 + C_{k}^{} _k\\\\ \\\\ ~||(_k) - ||^2_{F} ^2 + _k, _k 0 k \\{1,,K\\} . où $||.||_{F}$ désigne la norme 2 dans $F$.\\\\ Le La",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 50
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_51",
      "text": "grangien associ\\'e \\`a est d\\'efini par~: (,, ,, ) &=& ^2 + C _{k}^{} _k - _{k}^{} _k -_{k}^{} _k _k \\\\ & =& ^2(1-_{k}^{}_k) + _{k}^{} (C-_k-_k) _k + _{k}^{} _k ||(_k)-||^2_{F} où $_k$ et $_k$ sont les multiplicateurs de Lagrange tels que, pour tout $ k \\{1, , K\\}$, $_k 0$ et $_k 0$. Les points critiques v\\'erifient $}{ } = 0$, $}{ } = 0$ et $}{ _k} = 0$, c'est-\\`a-dire 2 _{k}^{} _k ( - (_k)) = 0 = _{k} _k (_k) \\\\ 2 (1-_{k}^{} _k ) = 0 _{k}^{} _k = 1 \\\\ C - _k - _k = 0 _k = C - _k k \\{1,,K\\} \\'E",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 51
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_52",
      "text": "tant données les positivités de $_k$ et $_k$, on peut éliminer la dépendance en $_k$ en imposant l'encadrement $0 _k C$. À partir des deux premières égalités du système que l'on injecte dans le lagrangien ci-dessus, on obtient la formulation dite duale du problème que l'on va maximiser et qui ne dépend plus que de $_k$: ( ) = _{k}^{} _k (_k).(_k) - _{k,k'}^{}_k _{k'} (_k).(_{k'}) \\\\ sc. 0 _k C _{k}^{} _k =1 \\,. Ceci est à nouveau un problème d'optimisation quadratique sous-contraintes. D'après l",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 52
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_53",
      "text": "es conditions de complémentarité de Karush-Kuhn-Tucker (Remarque 7.3, p. 198 ), la solution optimale vérifie pour tout $k \\{1,,K\\}$: _k^{*} = 0 \\\\ ^{*}_k ^{*}_k = 0 \\,. Les vecteurs de support (SV) qui sont l'ensemble des points qui satisfont $_k^{*}> 0$ définissent à eux seuls le centre de la sphère. Géométriquement, il s'agit des points qui sont exactement à la surface de la sphère optimale ($0<_k^{*}< C, ^{*}_k=0$) ou à l'extérieur grâce aux variables de tolérance ($_k^{*} = C, ^{*}_k>0$). Se",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 53
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_54",
      "text": "uls les vecteurs support interviennent dans la définition du centre: $$^* = _{_k SV} ^*_k (_k)$$ Toutefois ce centre n'a pas besoin d'être explicitement calculé. Pour $ ^n$ et $()$ son plongement dans $F$, la distance entre $()$ et le centre s'exprime uniquement par des produits scalaires avec les vecteurs de support: D^2_{}(,):=||()-||^2_{F}= ().()-2_{_k SV}^{} _k^{*} ().(_k) + _{_k, _{k'} SV} _k^{*}_{k'}^{*} (_k).(_{k'}) Afin d'établir un critère d'anomalie, il reste à calculer le rayon de la ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 54
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_55",
      "text": "sphère à partir de l'équation pour un vecteur de support $^<$ strict ($_k^{*}< C$) sur la sphère optimale ${^*}^2 = D^2_{}(^<,)$. Le score d'anomalie associé à la SVDD est: () = D^2_{}(,^*) - {^*}^2 Ainsi, le score est positif pour les points en dehors de l'hypersphère, et seront donc considérés comme une anomalie.\\\\ Avec un effort minimal, ce modèle peut prendre en compte des anomalies connues en imposant que celles-ci soient à l'extérieur de la sphère avec une certaine tolérance. Pour résoudre",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 55
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_56",
      "text": " le problème quadratique à partir de sa formulation duale, il est nécessaire, dans un premier temps, de réécrire le problème sous forme matricielle, afin de procéder ensuite à l'optimisation à l'aide de la librairie .\\\\ En notant $ = (_1, , _K)^T$, $0_K = (0, 0, ..., 0)^T$, $1_K = (1, 1, ..., 1)^T$ et $$ la matrice de Gram dont le terme général est $_{ij}= (_i).(_j)$, }{} ^t - ^t ()\\\\ \\\\ galit: } 1_K^t =1 \\\\ galit: } -Id_K \\\\ Id_K \\\\ 0_K \\\\ C~1_K En général, les $_k$ étant linéairement indépenda",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 56
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_57",
      "text": "nts, la matrice de Gram est par construction définie positive. La fonction objective à minimiser est dans ce cas strictement convexe. Les contraintes forment de plus un ensemble convexe fermé. On en conclut à l'existence d'une solution unique. Une façon d'adapter la SVDD au contexte des réseaux de neurones et de reproduire la méthodologie qui a permis de passer de la \"One Class SVM\" (voir p. ) à la \"Deep One Class SVM\" (voir p. ): les contraintes sur les variables { slack} sont intégrées dans la",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 57
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_58",
      "text": " fonction de perte charnière ({ hinge loss}) . Le réseau de neurones dont les paramètres sont notés $W$ définit explicitement le plongement $_W: ^n F ^m$, espace dans lequel est calculé la SVDD (cf. figure ). La fonction objective est donc _{, W} ^2 + { K} _k (0, ||_W(_k) - ||^2_F - ^2) + {2} ||W||^2 Le plongement $_W$ sera pénalisé pour tout point $_W(_k)$ à l'extérieur de la sphère. Le réseau de neurones est régularisé par une pénalité $L_2$ afin de rendre le plongement dans $F$ plus robuste. ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 58
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_59",
      "text": "On remarque que le centre ne fait pas partie de l'optimisation afin d'éviter une solution triviale. En effet, si on laisse $$ libre, la sphère va s'effondrer sur le cas dégénéré $=_0$, $=0$ où $_0$ est la sortie du réseau pour $W=0$ (tous les poids du réseau sont nuls). Le centre $$ doit donc être fixé et différent de $_0$.\\\\ Les auteurs démontrent que deux conditions supplémentaires sont nécessaires pour éviter cet effondrement(appelé également \"collapse\"). Il n'existe pas de terme de biais app",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 59
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_60",
      "text": "renable dans les couches cachées; le réseau serait capable de trouver un $W^*$ tel que $_W() = $ pour tout $$ et $^*=0$. Les fonctions d'activations ne doivent pas être bornées. Cette condition découle de la précédente car si une fonction d'activation sature un neurone en dehors de 0 quelque soit son entrée, ce neurone pourra jouer le rôle d'un terme de biais dans la couche suivante. Il est donc possible d'utiliser l'activation mais pas l'activation sigmoïde. Le score d'anomalie pour cette métho",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 60
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_61",
      "text": "de est: $$() = ||_{W^*}() - ||^2_F - {^*}^2$$ Afin de faciliter l'interprétation et la comparaison entre modèles, on peut adimensionner ce score en le divisant par ${^*}^2$. [htbp] {0.9} {0.33} {60} in {1,...,15} { {rand*360} { rand*1.5} (:) circle (1pt); } in {1,...,} { {rand*360} {rand*1.5} (:) circle (1pt); } {0.33} [shorten >=1pt,->,draw=black!50, node distance=2cm] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!1",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 61
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_62",
      "text": "00]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0.5,--0.5) {$x_{}$}; / in {1,...,3} (J-) at (1.5,-) {$h_{}$}; / in {1,...,3} (K-1) edge (J-); / in {1,...,3} (K-2) edge (J-); [decorate,decoration={brace,amplitude=10pt},yshift=-0.5cm] (-2,0) -- (2,0) node [black,midway,yshift=0.6cm] {$_W()$}; {0.33} (0,0) circle (0.5*); {60} in {1,...,15} { {rand*360} { + rand*0.5} (:) circle (1pt); } in {1,...,} { {rand*360} {rand*0.5} (:) circle (1pt); } )$ et SVDD} Si l",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 62
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_63",
      "text": "'on est dans le cas où la (quasi-)totalité des données sont normales, les mêmes auteurs ont proposé une version simplifiée du précédent algorithme appelée \"One Class Deep SVDD\" pour laquelle la notion de sphère disparait en prenant $ = 0$. Par extension, on peut dire que tous les points sont considérés comme à l'extérieur de la sphère et on cherche de minimiser la variance des données normales autour d'un centre $ F$: _W {K} _k ||_W(_k) - ||^2_F + {2} ||W||^2 En cherchant la transformation $_W$ ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 63
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_64",
      "text": "qui contracte les données normales vers $$, on espère que les anomalies en seront éloignées. Il est évident que le réseau doit être suffisamment régularisé pour maintenir une \"certaine\" quantité de variance et éviter l'effondrement sur $$. Comme précédemment, il convient de fixer $$ différent de $_0$ obtenu pour $W=0$. Nos tests ont montré qu'il est possible d'utiliser des biais apprenables et des activations sur toutes les couches hormis la dernière. \\\\ Le score d'anomalie pour cette méthode es",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 64
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_65",
      "text": "t simplement la distance à $$: $$() = ||_{W^*}() - ||^2_F$$ Cependant, en pratique, il est nécessaire de fixer un seuil à ce score. Pour cela, nous avons utilisé la méthode de qui détermine un seuil \"optimal\" en maximisant le score AUC-ROC (décrit dans la section ). Ce post-traitement appliqué au score d'anomalie pour chacune des méthodes décrites ne garantit pas que la frontière de décision de l'hypersphère (généralement le seuil est à 0) coïncide avec le seuil optimal trouvé.\\\\ Une amélioratio",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 65
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_66",
      "text": "n a été proposée dans afin de préserver suffisamment d'information et de structure. L'idée est de pré-entraîner un auto-encodeur pour lequel $_{W}$ serait l'encodeur. Le vecteur $$ serait alors défini comme la sortie moyenne de $_{W}$. La poursuite de l'entraînement consiste à minimise la perte auquel on a rajouté l'erreur de reconstruction de l'auto-encodeur en norme $L_2$. Les fonctions à base radiale sont des fonctions dont la valeur dépend de la distance par rapport au centre $ ^n$ de la fon",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 66
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_67",
      "text": "ction. Une fonction à base radiale $()$ est de la forme~: (, \\{, \\}) = (||-||) où $ ^n$ est le vecteur d'entrée, $$ est le centre de la fonction, $$ un paramètre de contrôle sur la largeur de la fonction et $$ une fonction donn\\'ee. On considérera $||||$ comme la norme euclidienne. Une des fonctions à base radiale les plus couramment utilisées est la fonction Gaussienne, définie par~: (, \\{, \\}) = e^{--||^2}{2^2}} Les fonctions à base radiale sont souvent utilisées, tr\\`es efficacement, dans le ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 67
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_68",
      "text": "cadre de méthodes d'approximation ou de classification. Elles servent de fonctions de base pour représenter des fonctions plus complexes (cf. approximation universelle par les fonctions à base radiale dans le chapitre précédent). Elles sont aussi structurellement assez proches des fonctions de sortie hypersph\\'eriques. On d\\'etaille donc la m\\'ethode Deep Radial Basis Function Data Descriptor (D-RBFDD) pour que le lecteur la différencie bien des outils que nous introduirons par la suite. La sort",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 68
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_69",
      "text": "ie d'une couche RBF s'écrit~: (, \\{w_j, _j, _j\\}) = _{j=1}^J w_j ( _j|| -_j|| ) . où $J$ est le nombre de fonctions radiales. Dans la littérature, les points centraux $_j$ des fonctions radiales sont initialisés par clustering et les facteurs d'échelle fixés à 1.\\\\ Le principe de la méthode RBFDD est d'utiliser une couche RBF puis une activation $g$ pour régresser la valeur de normalité de $1$. Afin d'éviter une activation qui sature pour la valeur $1$, les auteurs utilisent la fonction recomman",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 69
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_70",
      "text": "dée dans , à savoir $1.7159~(2x/3)$. Dès lors qu'un module $_W$ d'extraction de caractéristiques est mis en amont de la couche RBF, les auteurs appellent ce modèle Deep RBFDD. La fonction de coût à minimiser est: L(\\{_k\\}, W, \\{w_j, _j, _j\\}) = {2K} _k ^2+{2}||W||^2_2 Comme précédemment, un seuil automatique est calculé en sortie de l'activation $g$ par la procédure automatique liée au critère AUC-ROC (cf. ). Les anomalies seront typiquement les données pour lesquelles la sortie est inférieure à",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 70
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_71",
      "text": " ce seuil. Les couches hypersphériques sont définies dans le cadre de l'algèbre géométrique conforme, permettant de représenter des points et des hypersphères dans un espace étendu. Les méthodes D-RBFDD ou Deep SVDD et ses variantes décrites précédemment, sont adaptées pour utiliser des couches hypersphériques et effectuer une optimisation. Dans cette section sont d\\'efinis tous les algorithmes que nous proposons, en s'appuyant sur leur construction \\`a partir de probl\\`emes d'optimisation sous ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 71
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_72",
      "text": "contraintes et en d\\'etaillant le fonctionnement des fonctions de co\\^ut dirigeant l'apprentissage. Cela donne des premiers \\'el\\'ements de comparaison avec les m\\'ethodes Deep SVDD. L'analyse des propri\\'et\\'es des algorithmes propos\\'es et les exp\\'erimentations num\\'eriques seront d\\'evelopp\\'ees aux paragraphes et . L'espace des caract\\'eristiques $F$ dans lequel sont projet\\'ees les donn\\'ees est suppos\\'e de dimension $m$. Une hypersph\\`ere dans $^m$ de centre $=(c_1,,c_m)$ et de rayon $$ ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 72
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_73",
      "text": "correspond \\`a $ $, dont les $m+2$ coordonn\\'ees sont $(1,, ( ^2 - ^2)/2)$, avec $ ^2=_{i=1}^m c_i^2$, dans l'alg\\`ebre conforme $(m+1,1)$. Dans toute la suite, pour \\'eviter la confusion avec le produit scalaire, le produit interne de l'alg\\`ebre conforme est not\\'e $_i$. On rappelle en particulier que pour tout point $ =(x_0,,x_)$ de $(m+1,1)$, on a~:\\\\ $$ _i = - {2} ( - ^2 -^2 ) .$$ On note en particulier que seules les coordonn\\'ees de $$ correspondant aux coordonn\\'ees dans l'espace des car",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 73
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_74",
      "text": "act\\'eristiques sont utilis\\'ees. Cette approche s'inspire du principe que la méthode RBFDD décrite précédemment. Le réseau de neurones est constitué d'une seule couche composée de plusieurs hypersphères. La sortie du réseau correspond à une somme pondérée de sigmoïdes (fonctions $g$) appliquées au produits conformes entre les hypersph\\`eres et les points testés. (, \\{ w_j, _j \\}) = _{j=1}^{J} w_j g(_j _i ) La figure illustre deux points principaux: Elle approxime une fonction porte à bords liss",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 74
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_75",
      "text": "es sur l'intervalle $[-, ]$. Quelque soit la valeur de $$, son maximum qui est atteint pour $=$ appartient à l'intervalle $]0.5, 1[$. [H] _i $ pour $=0$} Le réseau SPH Anomaly cherche par une régression au sens des moindes carrés à ajuster les sphères pour obtenir une sortie à 1:\\\\ (\\{_k\\}, \\{w_j, _j\\}) = {2K} _k (1- (_k, \\{w_j, _j\\}) )^2 \\\\ \\'Etant donnée une hypersphère de rayon $$ \"suffisant\" grand ($>3$), la sortie $g(_j _i )$ pour les points $$ situés à une distance inférieure à $$ de $$ es",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 75
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_76",
      "text": "t proche de 1. On appellera Deep SPH Anomaly, un réseau de neurones tel que SPH Anomaly suit un extracteur de caractéristiques. Dans nos observations, nous avons constaté qu'il est préférable que ce dernier soit pré-entrainé (via un auto-encodeur). {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 76
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_77",
      "text": "{1,...,2} (K-) at (0,-+1/2) {$x_{}$}; (K-3) at (0,-3*+1/2) {$1$}; (K-4) at (0,-4*+1/2) {${2}$}; / in {1,...,3} (J-) at (0,-) {$_{}_i $}; / in {1,...,3} (K-1) edge (J-); / in {1,...,3} (K-2) edge (J-); / in {1,...,3} (K-3) edge[dashed,->,red] (J-); / in {1,...,3} (K-4) edge[dashed,->,red] (J-); (I) at (0.5cm,-2*) {$ w_{j=1}^3 g(_j _i _k)$}; in {1,...,3} (J-) edge (I); L'idée de la méthode SPH SVDD est de reformuler le problème la SVDD (cf. équation ) en utilisant une hypersphère dans l'espace géo",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 77
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_78",
      "text": "métrique conforme. En d'autres termes, utiliser une couche hypersphérique pour résoudre le problème d'optimisation en utilisant l'algèbre géométrique conforme (cf. figure ). Comme ce sont les paramètres d'une hypersphère d'encadrement qui sont recherchés, le réseau contient une couche hypersphérique à un seul neurone (une sphère) et les entrées $ ^m$ sont d'abord plongés dans l'espace conforme pour donner $ (m+1, 1)$.\\\\ [h!] {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 78
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_79",
      "text": "] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!100]; =[neuron, fill=yellow!120]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0,-+1/2) {$x_{}$}; (K-3) at (0,-3+1/2) {$1$}; (K-4) at (0,-4+1/2) {${2}$}; / in {0} (J-) at (0,-1.75) {$y_i = _i $}; / in {0} (K-1) edge (J-); / in {0} (K-2) edge (J-); / in {0} (K-3) edge[dashed,->,red] (J-); / in {0} (K-4) edge[dashed,->,red] (J-); Par conséquent, la sortie du réseau est le produi",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 79
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_80",
      "text": "t conforme entre un point $ (m+1, 1)$ et une hypersphère $ (m+1, 1)$ de centre $$ et de rayon $$. Rappelons que ce produit est\\\\ _i = -{2} \\\\ On rappelle que le problème d'optimisation pour la SVDD se formule comme\\\\ {} & ^2 + { K}_{k=1}^{K} _k\\\\ \\\\ & ||_k - ||^2-^2 _k, _k 0, k \\{1,,K\\} \\\\ peut se reformuler à l'aide du produit conforme ($||_k - ||^2-^2 = - 2~ _i _k$) comme\\\\ ,)}{} & ^2 + { K}_{k=1}^{K} _k\\\\ \\\\ & -2_i_k _k, _k 0, k \\{1,,K\\} . \\\\ En utilisant une fonction de coût charnière, on ob",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 80
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_81",
      "text": "tient une formulation sur une ligne de SPH SVDD comme\\\\ }{}\\, ^2 + { K} _{k=1}^{K} \\{0, -2 _i _k \\} \\\\ où $K$ est le nombre total de points dans l'ensemble des données. On peut préciser que $^2$ peut également s'écrire $^2$ (produit géométrique) ou encore $ _i $ car $ = 0$.\\\\ La $$-propriété de la SVDD qui permet de contrôler la proportion des points à l'intérieur de la sphère est perdue ici. L'hyper-paramètre $$ reste toutefois un moyen de contrôler la tolérance du modèle à l'erreur. En pratiqu",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 81
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_82",
      "text": "e, différents codes de la littérature montrent que la $$-propriété peut-être rétablie après optimisation en modifiant le rayon pour couvrir $100~\\ Si les données ne peuvent pas être séparées par une hypersph\\`ere, une ou plusieurs couches cachées (extraction de caractéristiques) sont ajoutées au réseau en amont (cf. figure ). Les paramètres $W$ de ce module du réseau permettent caractériser la fonction de plongement $_W: ^n ^m$ dans l'espace des caractéristiques $F$. La sortie $_W()$ de $_W$ est",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 82
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_83",
      "text": " ensuite plongée dans l'algèbre géométrique conforme $(m+1,1)$. La suite du réseau correspond à la partie SPH SVDD décrite plus haut, l'ensemble est appelé Deep SPH SVDD.\\\\ [h!] {1} [shorten >=1pt,->,draw=black!50, node distance=] =[<-,shorten <=1pt] =[circle,fill=black!25,minimum size=17pt,inner sep=0pt] =[] =[neuron, fill=red!75]; =[neuron, fill=yellow!100]; =[neuron, fill=green!100]; = [text width=4em, text centered] / in {1,...,2} (K-) at (0,--2.5) {$x_{}$}; / in {1,...,5} (J-) at (0,--1) {$",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 83
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_84",
      "text": "h_{}$}; / in {1,...,5} (K-1) edge (J-); / in {1,...,5} (K-2) edge (J-); / in {1,...,3} (L-) at (2.5,-2) {$_{W}(h)$}; / in {1,...,3} (J-1) edge (L-); / in {1,...,3} (J-2) edge (L-); / in {1,...,3} (J-3) edge (L-); / in {1,...,3} (J-4) edge (L-); / in {1,...,3} (J-5) edge (L-); (L-4) at (2.5,-7.75) {$1$}; (L-5) at (2.5,-9) {${2}$}; (I) at (2,-3.75) {$y_i = _i $}; in {1,...,3} (L-) edge (I); / in {4,...,5} (L-) edge[dashed,->,red] (I); La recherche des paramètres de l'hypersphère englobante est eff",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 84
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_85",
      "text": "ectuée dans l'espace de caractéristiques en dimension $m$. La fonction de coût est relativement similaire à celle de la SPH SVDD puisque seul un terme de régularisation est rajouté: }{}\\, ^2 + { K} _{k=1}^{K} \\{0, -2 _i (_k) \\} + {2} ||W||^2 Contrairement à la méthode Deep SVDD, tous les paramètres de l'hypersphère, y compris le centre, sont appris dans Deep SPH SVDD. Nous verrons par la suite la raison qui fait que cela est possible sans effondrement. On d\\'efinit le score d'anomalie comme () =",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 85
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_86",
      "text": " -2^* _i }(z). Ce score est si et seulement si $}(z)$ est dans la sphère $^*$. Cela indique que le réseau de la méthode Deep SPH SVDD produit directement $ ^* _i }(z)$ qui est un score de normalité. Comme nous l'avons déjà évoqué, un seuil automatique du score est calculé par la procédure automatique liée au critère AUC-ROC (cf. équation ). Cela revient à ajuster le rayon de la sphère trouvée.\\\\ Les étapes d'implémentation sont résumées à travers le pseudo-code suivant:\\\\ [ colback=gray!5, colfr",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 86
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_87",
      "text": "ame=blue!75!black, fonttitle=, title={Deep SPH SVDD} ] [H] _1, , _K\\}$\\\\ Hyperparamètres $ ]0, 1[$, $> 0$, $$, nombre maximal d'itérations $N_{}$, $$} $ et rayon $^*$ de l'hypersphère, fonction de détection d'anomalie} \\\\ \\\\ Initialiser aléatoirement les poids $W$ de $$ (méthode He)\\; { Initialiser $$ avec $ = $ et $$ leur écart-type\\; } $ avec $ (0,1)$ et $ = 1$\\; } \\\\ 1$ $N_{}$}{ Mélanger aléatoirement $X$ \\; { _k B$}{ Extraction de caractéristiques: $_k ^n _W (_k) ^m$\\; Passage en conforme: $",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 87
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_88",
      "text": "_W (_k) ^m (_k) (m+1,1)$\\; Calculer le score d'anomalie $-2 _i (_k)$; } $ = ^2 + { |B|} _{x_k B} (0, -2 _i (_k))) + {2} ||W||_2^2$\\; Mettre à jour $W$, $$ à partir de $$ avec la méthode ADAM\\; } $}{ Arrêter l'étape 2\\; } } $ = 0$\\; \\\\ {un jeu de validation $V$ est disponible}{ _k V$}{ Calculer le score d'anomalie $-2^* _i }(_k)$; } Obtenir un seuil automatique $$ via l'équation ; } $$() := -2^* _i }() > $$ \\\\ Certains des \\'el\\'ements ci-dessous seront établis dans les paragraphes et . Nous pr\\'",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 88
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_89",
      "text": "ef\\'erons les rassembler aussi ci-dessous pour le confort du lecteur. [H] {|p{0.9}|} {|c|}{} \\\\ .\\\\ L'hyper-paramètre $$ reste un coefficient de régularisation. Un taux de couverture peut être obtenu en aval par l'ajustement du rayon de la sphère sur un jeu de données de validation. \\\\ [H] {|p{0.9}|} {|c|}{} \\\\ .\\\\ Le nombre de paramètres entraînables est $m+1$ dans les deux cas. Pour cela, Deep SPH SVDD fixe le coefficient de $e_0$ à 1 pendant l'entraînement. \\\\ .\\\\ Par définition, les anomalie",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 89
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_90",
      "text": "s sont les points situés à l'extérieur de la sphère (score(z) $> 0$). En pratique, ce seuil peut être ajusté automatiquement sur un jeu de validation afin de maximiser la performance ROC. Si l'on considère que le centre de l'hypersphère est fixé par l'entraînement, cela correspond à en ajuster le rayon.\\\\ [H] {|p{0.435}|p{0.435}|} {|c|}{} & {c|}{} \\\\ Le centre $$ de l'hypersphère afin éviter la dégénerescence de celle-ci. Par exemple, on peut choisir $$ comme le barycentre des points ${(_k)}$ po",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 90
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_91",
      "text": "ur le premier batch. & Le centre $$ est libre, pas de dégénérescence observée. \\\\ Le rayon $$ est libre hormis pour la OC-Deep SVDD qui le fixe à $0$ (requiert une hypothèse de normalité). & Le rayon $$ est libre. \\\\ La sortie du réseau fournit la projection des données dans l'espace des caractéristiques \\( F \\). & La sortie du réseau donne directement le score de normalité. \\\\ Les termes de biais et les activations bornées sont interdites sous peine de dégénérescence de l'hypersphère & L'encode",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 91
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_92",
      "text": "ur $$ peut contenir des termes de biais et utiliser des fonctions d'activation bornées.\\\\ Dans cette section, nous étendons la méthode Deep SPH SVDD au cas de plusieurs sphères. L'idée première est d'envelopper efficacement des groupes spécifiques de points normaux dans l'espace des caractéristiques par un groupe de sphères. Par ce choix, nous désirons améliorer la finesse de d\\'etection et l'interprétabilité du modèle notamment dans le cas de données multimodales.\\\\ Les paramètres à optimiser s",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 92
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_93",
      "text": "ont toujours les poids $W$ de la fonction de projection $$ qui envoie les données vers l'espace des caract\\'eristiques, ainsi que les centres $$ et les rayons $_j$ des hypersphères. Afin d'adapter la formulation au cadre multisph\\'erique, deux questionnements doivent être tranchés: Comment définir une anomalie dans le cas de plusieurs hypersphères ? Comment calculer le volume des données normales couvertes par le modèle ? Pour répondre à la première question, rappelons qu'un point $$ est à l'int",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 93
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_94",
      "text": "érieur (couvert) par la sphère $_j$ si et seulement si $-2_j _i < 0$. La règle de couverture la plus naturelle est de décréter qu'un point est couvert si et seulement si il existe au moins une sphère qui le couvre. Cela revient alors à attribuer à chaque point le score d'anomalie minimal parmi toutes les sphères: () = _j \\{ -2_j _i \\} et espérer qu'il soit négatif.\\\\ Ce choix est justifié par le fait que si un point est bien inclus dans au moins une sphère, alors son score d'anomalie sera faible",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 94
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_95",
      "text": ". À l'inverse, un point qui est éloigné de toutes les sphères recevra un score élevé, reflétant son caractère atypique. Ainsi, cette définition permet d'assurer une cohérence entre la formulation du score d'anomalie et la règle de couverture adoptée pour la classification des données normales.\\\\ Dans toutes les méthodes présentées jusqu'à présent, la minimisation du volume d'une sphère qui englobe les données normales est obtenue en minimisant de manière auxiliaire le carré de son rayon. Dans le",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 95
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_96",
      "text": " cas de plusieurs hypersphères, la minimisation du volume couvert nécessite de prendre en compte les relations spatiales entre elles. Par exemple, si deux sphères se chevauchent fortement le volume de leur union est supérieur à celui d'une unique sphère optimisée couvrant les mêmes points, ce qui peut être inefficace en termes de représentation des données normales. De plus, même s'il existe une formule analytique pour le volume de l'intersection entre deux sphères, gérer plus de deux sphères se",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 96
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_97",
      "text": "mble inextricable.\\\\ En conséquence, une stratégie naturelle consiste à imposer une contrainte d'exclusion mutuelle entre les hypersphères, garantissant ainsi qu'elles ne s'intersectent pas. Cette contrainte permet de s'assurer que chaque sphère capture une région distincte de l'espace, évitant ainsi les redondances et améliorant la séparation des groupes de données. De plus, en éliminant les intersections, on simplifie considérablement la modélisation du volume couvert, qui devient simplement l",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 97
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_98",
      "text": "a somme des volumes individuels des sphères (ou de manière auxiliaire la somme des carrés des rayons). Ce choix garantit ainsi une optimisation cohérente et contrôlable, en adéquation avec l'objectif de minimisation du volume global des données normales. On notera que cette question n'\\'etait pas prise en compte dans l'approche avec la m\\'ethode SPH Anomaly. Pour garantir la distinction et la non-intersection entre les sphères, on va introduire deux termes supplémentaires dans la fonction de co\\",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 98
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_99",
      "text": "^ut. Le théorème suivant de Hestenes et al. (voir théorème 2.6.1 in ) inspire la construction (adaptation des notations).\\\\ [colframe=blue, colback=white!10, title=Critère pour l'intersection de deux sphères] Deux sphères $_1$, $_2$ se coupent, sont tangents ou parallèles, ou ne se coupent pas, si et seulement si $(_1 _2)^2$ est inférieur, égal ou supérieur à $0$, respectivement. Ainsi le d\\'eveloppement d'une expression de la forme $(_{1} _2)^2$, où $$ désigne le produit externe, permet d'obten",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 99
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_100",
      "text": "ir une relation entre les centres et les rayons des sphères, dont le signe indique si deux sphères s'intersectent.\\\\ Par exemple en dimension 2, soit $_1$ et $_2$ sont deux hypersphères caract\\'eris\\'ees par les coordonn\\'ees\\\\ {l} _1 = e_0 + x_1 e_1 + y_1 e_2 + (- {2} + {2} ) e_, \\\\ \\\\ _2 = e_0 + x_2 e_1 + y_2 e_2 + (- {2} + {2} ) e_, dont les centres sont donc $_1 = (x_1, y_1)$ et $_2 = (x_2, y_2)$ et les rayons $_1$ et $_2$. On calcule le scalaire: (_1 _2)^2 &= {4} (-_1^2 - 2_1_2 - _2^2 + x_1",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 100
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_101",
      "text": "^2 - 2x_1x_2 + x_2^2 + y_1^2 - 2y_1y_2 + y_2^2) \\\\ \\\\ & (-_1^2 + 2_1_2 - _2^2 + x_1^2 - 2x_1x_2 + x_2^2 + y_1^2 - 2y_1y_2 + y_2^2) \\\\ \\\\ &= {4} \\\\ \\\\ & \\\\ \\\\ &= {4} \\\\ Il s'avère que ce d\\'eveloppement se généralise en dimension quelconque~: \\\\ (_1 _2)^2 &= {4} (-_1^2 - 2_1_2 - _2^2 + _{i=1}^m (_{1i} - _{2i})^2) \\\\ \\\\ & (-_1^2 + 2_1_2 - _2^2 + _{i=1}^m (_{1i} - _{2i})^2) \\\\ \\\\ &= {4} \\\\ La figure illustre le signe de $(_1 _2)^2$ suivant la relation spatiale entre les deux sphères.\\\\ [htbp] {0.2}",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 101
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_102",
      "text": " [thick, scale=0.6] (0,0) circle (1.5); (1.8,0) circle (1.5); _1 _2)^2 < 0$} {0.37} [thick, scale=0.6] (0,0) circle (1.5); (2.5,0) circle (1); (3.75,-1.5) -- (3.75,1.5); (5.5,0) circle (1.5); (6,0) circle (1); _1 _2)^2 = 0$} {0.37} [thick, scale=0.6] (0,0) circle (1.5); (2.5,0) circle (0.75); (3.65,-1.5) -- (3.65,1.5); (5.5,0) circle (1.5); (6,0) circle (0.75); _1 _2)^2 > 0$} _1 _2)^2$} On remarque plusieurs éléments: L'expression est bien entendu symétrique~: $ (_1 _2)^2 =(_2 _1)^2$ Une sphère ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 102
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_103",
      "text": "est tangente à elle-même: $( )^2 = 0$ Si la sphère $_1$ est totalement incluse dans $_2$, $(_1 _2)^2 > 0$ Réduire l'intersection des sphères revient à pénaliser le terme $- (_1 _2)^2$ s'il est positif. Dans le cas de $J$ hypersphères, on est amené à considérer ce terme de pénalité pour chaque couple d'hypersphères et ainsi définir le coût:\\\\ oss_{}(_1, , _J) = _{j < j'} \\{0, - (_j _{j'})^2 + \\} \\\\ où $$ est une petite constante positive introduite pour \\'eviter que deux hypersphères soient tange",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 103
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_104",
      "text": "ntes.\\\\ Cependant, comme indiqué dans la troisième remarque et le dernier cas de la figure , le signe de $(_1 _2)^2$ n'est pas suffisant pour garantir l'exclusion mutuelle. Pour cela, un terme de non-inclusion est rajouté pour favoriser la situation où\\\\ _1 _i _2 < 0 _2 _i _1 < 0 \\\\ Cela revient à faire en sorte que le centre de chaque sphère soit perçu comme une anomalie par l'autre. On imagine bien que ce critère contribue également à la non-intersection de deux sphères. Comme précédemment, on",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 104
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_105",
      "text": " définit un terme global pour les $J$ sphères:\\\\ oss_{}(_1, , _J) = _{j, j' j} \\{0, _j _i _{j'} + \\} où $$ est une petite constante positive introduite pour éviter que les centres soient trop proches des bords des sphères. Conformément à l'objectif poursuivi, nous avons retiré le terme $j = j'$ dans la double sommation de l'équation . Toutefois, de manière amusante, on constate que\\\\ $\\{0, _j _i _{j} + \\} = _j^2/2 + $, ce qui conduit de minimiser $_j^2$ déjà présent dans la fonction de coût fina",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 105
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_106",
      "text": "le. Pour une raison de lisibilité et de lien entre les algorithmes, nous avons fait le choix de supprimer le terme $j=j'$ de la double sommation.\\\\ Finalement, en regroupant l'ensemble des consid\\'erations pr\\'ec\\'edentes, la formulation du problème de la Deep M-SPH SVDD s'écrit comme\\\\ ,, , W}{} \\{ & _{j} ^2 + { K} _{k} \\{ 0, {}\\{ -2_j _i _W(_k) \\} \\} \\\\ \\\\ & + oss_{}(_1, , _J) + oss_{}(_1, , _J) + {2} ||W||^2 \\} \\\\ Pour mieux illustrer les étapes d'implémentation de la méthode Deep SPH SVDD, l",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 106
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_107",
      "text": "a représentation algorithmique ci-dessous, sous forme de pseudo-code, synthétise les étapes essentielles de sa mise en œuvre.\\\\ [ colback=gray!5, colframe=blue!75!black, fonttitle=, title={Deep M-SPH SVDD} ] [H] _1, , _K\\}$, nb. hypersphères $J$\\\\ Hyperparamètres: $ ]0, 1[$, $> 0$, $$, nombre max. d'itérations $N_{}$, $$} $ et rayons $_j^*$ des hypersphères, fonction de détection d'anomalie} \\\\ \\\\ Initialiser aléatoirement les poids $W$ de $$ (méthode He)\\; { Clustering des $\\{_W(x) | x X\\}$ en ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 107
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_108",
      "text": "$J$ groupes\\; Initialiser les $_j$ à partir des centres $_j$ et des écart-types $_j$ des clusters\\; } _j$ avec $_j (0,1)$; $_j$ = 1\\; } \\\\ 1$ $N_{}$}{ Mélanger aléatoirement $X$ \\; { _k B$}{ Extraction de caractéristiques: $_k ^n _W (_k) ^m$\\; Passage en conforme: $_W (_k) ^m (_k) (m+1,1)$\\; _j$}{Calculer les scores d'anomalie $-2_j _i (_k)$;} } $_{} = 0$; $_{} = 0$ \\; ^2, j < j'$} { $_{} = _{} + \\{0, - (_j _{j'})^2 + \\}$\\; $_{} = _{} + \\{0, _j _i _{j'} + \\} + \\{0, _{j'} _i _{j} + \\}$\\; } $ = _j",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 108
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_109",
      "text": " _j^2 + { |B|} _{x_k B} _j \\{ -2_j _i _{W}(_k)\\} + _{} + _{} + {2} ||W||_2^2$\\; Mettre à jour $W, _1, , _J$ à partir de $$ avec la méthode ADAM\\; } $}{ Arrêter l'étape 2\\; } } $ = 0$\\; \\\\ {un jeu de validation $V$ est disponible}{ _k V$} { Calculer le score d'anomalie $_j \\{ -2^*_j _i _{W^*}(_k)\\}$ } Obtenir un seuil automatique $$ via l'équation ; } $$() := _j \\{ -2^*_j _i _{W^*}(z) \\} > $$ } La méthode Deep M-SPH SVDD généralise Deep SPH SVDD, car lorsque le nombre de sphères est fixé à $J=1$,",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 109
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_110",
      "text": " les deux algorithmes deviennent équivalents. L'algorithme le plus proche de Deep M-SPH SVDD est la variante Deep Multi-Sphere SVDD (DMSVDD) développée par Ghafoori et Leckie dans . Avant de comparer point par point les deux algorithmes, donnons la fonction de coût suivi par DMSVDD: _{W, } {J} _j ^2_j + { K} _k (0, ||_W(_k) - c(_W(_k))||^2 - (_W(_k))^2) + {2} ||W||^2_2 où $c(_W(_k))$ (resp. $(_W(_k))$) désigne le centre (resp. le rayon) du centre le plus proche de $(_W(_k)$ dans $F$. Le score d'",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 110
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_111",
      "text": "anomalie ainsi construit est clairement identique au nôtre dans l'équation (). Pour faciliter la comparaison, diff\\'erents \\'el\\'ements sont list\\'es dans l'encadr\\'e ci-dessous. [H] {|p{0.48}|p{0.48}|} & } \\\\ $k$-means dans l’espace des caractéristiques pour initialiser les centres et les rayons des sphères. & $k$-means dans l’espace des caractéristiques pour fixer les centres.\\\\ Rétropropagation conjointe des paramètres de l’encodeur et des sphères via Adam. & Optimisation alternée : mise à jo",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 111
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_112",
      "text": "ur séparée des rayons et des paramètres de l’encodeur. \\\\ Centres ajustés dynamiquement pendant l’entraînement via rétropropagation. & Fixés après l’initialisation, restent constants tout au long de l’optimisation. \\\\ Rayons ajustés dynamiquement pendant l’entraînement via rétropropagation. & Déterminés empiriquement comme les $(1-)$-quantiles des distances aux centres. \\\\ La $$-propriété n'est pas respectée lors de l'optimisation. & La $$-propriété forcée par le choix empirique des rayons. \\\\ I",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 112
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_113",
      "text": "ntroduit un coût d’intersection et de non-inclusion pour éviter le recouvrement excessif des sphères. & Les sphères peuvent s'intersecter; la somme des carrés des rayons ne mesure donc pas le volume de données couvert.\\\\ Le nombre de sphères $J$ est fixé. Les sphères peuvent avoir un rayon nul lors de l'optimisation & Ajusté dynamiquement en supprimant les sphères avec un effectif trop faible. \\\\ Le score d'anomalie est celui de la distance à la plus proche sphère. & Le score d'anomalie est celu",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 113
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_114",
      "text": "i de la distance à la plus proche sphère. \\\\ Au final, on voit que Deep M-SPH SVDD adopte une approche plus flexible grâce à une optimisation conjointe qui permet une adaptation fine des sphères aux données et qui modélise explicitement les relations géométriques entre les sphères. L'approche de DMSVDD est plus rigide avec des centres fixés et des rayons \"optimisés\" pour la $$-propriété. Cela favorise sans doute une répartition stable des sphères, mais la répartition des données dans les sphères",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 114
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_115",
      "text": " dépend largement de la régularité de $$. Dans les modèles classiques de Deep SVDD (ex. One-Class Deep SVDD), l'encodeur est associé à un centre $c$ et un rayon $$ fixés. Dans les modèles Deep SPH SVDD proposés, \\`a une ou plusieurs sph\\`eres, les paramètres caract\\'eristiques des sphères, rayon et centre, sont libres \\`a l'initialisation puis appris. C'est un int\\'er\\^et de ces nouvelles m\\'ethodes. C'est \\'egalement surprenant au regard du contexte classique. En effet, il est connu (voir ) que",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 115
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_116",
      "text": " si les centre et rayon des sph\\`eres sont laiss\\'es libres dans les algorithmes classiques de Deep SVD, alors ceux-ci convergent vers la solution triviale $(W_0, , 0)$, o\\`u $W_0$ d\\'esigne le r\\'eseau dont tous les poids sont nuls. En particulier, le rayon de la sph\\`ere limite est nul, elle est d\\'eg\\'en\\'er\\'ee, r\\'eduite \\`a un point not\\'e $c_0$ : c'est le phénomène de collapse. Les algorithmes Deep SPH SVDD propos\\'es ont pu appara\\^ tre jusqu'ici comme des r\\'e-\\'ecritures d'algorithmes ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 116
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_117",
      "text": "classiques profitant du formalisme de l'alg\\`ebre conforme. Il s'av\\`ere qu'ils ont un comportement tr\\`es diff\\'erent vis \\`a vis du risque de collapse. On a constat\\'e apr\\`es de nombreuses exp\\'erimentations que, bien que les centres et les rayons soient laiss\\'es libres, les rayons ne tendent pas toujours vers z\\'ero. Le pr\\'esent paragraphe est donc consacr\\'e \\`a ce comportement { a priori} \\'etonnant. On introduit quelques notations pour uniformiser le raisonnement entre les diff\\'erentes",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 117
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_118",
      "text": " m\\'ethodes pr\\'esent\\'ees dans ce manuscrit combinant r\\'eseau de neurones et SVDD. Soit d'abord $$ _W \\, : \\, ^n F ^m.$$ la fonction associ\\'ee au r\\'eseau $W$ qui effectue le plongement des donn\\'ees de $^{n}$ à un espace de caractéristiques $F$. Soit $W_0$ le r\\'eseau dont tous les poids sont nuls (avec les notations du manuscrit, $^l = 0$ pour tout $^l W_0$). La fonction $_{W_0}$ associ\\'ee est donc constante. Soit $ ^m$ cette constante, $\\{ \\} = _{W_0}(^n)$. On suppose qu'on dispose de $K$",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 118
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_119",
      "text": " donn\\'ees not\\'ees $_k$ pour $1 k K$. Toutes les m\\'ethodes m\\^elant apprentissage et SVDD pr\\'esent\\'ees ont une structure commune dans la fonction de perte associ\\'ee. On peut les \\'ecrire sous la forme d'une somme de fonctions de la forme J (W,, ) = ^2 + { K} _{ k}^{} ||_W(_k) - ||^2 _{F} + (_W) + (,) avec && 0, \\ 0,\\ 0, \\ 0, \\\\ && \\, : \\, ^{m+1} _+ (,0) = 0 \\ ^m. Dans la suite, on va supposer pour all\\'eger les notations qu'on ne cherche qu'une sph\\`ere englobant les donn\\'ees, si bien que ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 119
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_120",
      "text": "la fonction de perte est exactement donn\\'ee par . Si les param\\`etres \\`a apprendre sont les param\\`etres du r\\'eseau, le rayon et le centre de la sph\\`ere, le but de l'algorithme est donc de d\\'eterminer (W^*,^*,^*) = _{(W,,)} J (W,, ) , que le centre et le rayon soient appris directement ou appris { via} l'hypersph\\`ere conforme $^* = (1, c_1^*,,c_m^*, ( ^* ^2 - (^*)^2)/2)$. On \\'enonce le r\\'esultat suivant. ^*,^*)$ du probl\\`eme est } $$ W^*= W_0, ^* = _0, ^* = 0.$$ On v\\'erifie facilement ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 120
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_121",
      "text": "que $J(W_0,_0,0)=0$. Comme la fonction $J$ est positive, $(W_0,_0,0)$ r\\'ealise n\\'ecessairement le minimum de $J$ est est une solution du probl\\`eme . S'il existe une autre solution $(W^*,^*,^*)$ au probl\\`eme , elle v\\'erifie n\\'ecessairement aussi $J(W^*,^*,^*)=0$. Chaque terme dans la somme d\\'efinissant $J$ \\'etant une fonction \\`a valeur positive, cela implique que $$ (^*)^2 =0 ,\\ (_{W^*}) =0,\\ (^*,^*) =0 , \\ ||_{W^*}(_k) - ^*||^2 _{F} =0 \\ 1 k K.$$ La premi\\`ere relation implique $^*=0$, ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 121
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_122",
      "text": "la seconde $W^*=W_0$, si bien que la quatri\\`eme implique alors $^*=_{W^*}(_k) = _{W_0}(_k) =_0$. $$ Un tel r\\'esultat a d\\'ej\\`a \\'et\\'e d\\'emontr\\'e par les auteurs de pour le cadre Deep SVDD. Ils en concluent que l'algorithme en question risque de converger vers cette solution d\\'eg\\'en\\'er\\'ee. Pour l'\\'eviter ils pr\\'econisent de fixer rayon et centre et de ne plus chercher qu'\\`a apprendre les poids du r\\'eseau $W$ : pr\\'ecis\\'ement, pour $ 0$ donn\\'e, pour $ _0$ donn\\'e, r\\'esoudre $$W^* ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 122
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_123",
      "text": "= _{W} J (W,, ).$$ C'est un appauvrissement du mod\\`ele dont il est cependant bien connu qu'il est n\\'ecessaire~: l'algorithme Deep SVDD a tendance \\`a collapser lorsque le centre et le rayon ne sont pas fix\\'es. Pour autant, comme on va l'illustrer dans le paragraphe suivant, les algorithmes Deep SPH SVDD, bien que soumis aussi au r\\'esultat de la Proposition ci-dessus, ne conduisent pas au collapse. Le fait que les Deep SPH SVDD ne soient pas sensibles au collapse va entra\\^iner une cascade de",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 123
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_124",
      "text": " bonnes propri\\'et\\'es. L'article mentionne aussi le fait que le r\\'eseau associ\\'e \\`a la m\\'ethode Deep SVDD ne doit pas contenir de biais. D'abord, en toute rigueur, ce n'est vrai que si la fonction de perte ne contient pas le terme de régularisation $(_W)$ (voir la d\\'emonstration de la Prop. 2 dans ). Ensuite, et surtout, c'est une cons\\'equence du fait que le centre est fix\\'e pour la Deep SVDD. Les Deep SPH SVDD qui ne collapsent pas quand le centre est laiss\\'e libre peuvent donc s'appuy",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 124
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_125",
      "text": "er sur un r\\'eseau dont l'architecture est enrichie par des biais. D\\`es lors (voir Prop. 3 dans ), les Deep SPH SVDD ne sont pas limit\\'es \\`a l'utilisation de fonctions d'activation non born\\'ees. Le but de ce paragraphe est de comprendre pourquoi comment l'approche Deep SPH SVDD se comporte par rapport au phénomène de collapse ({ i.e.} ne produit pas une sph\\`ere d\\'eg\\'en\\'er\\'ee en un point) et pourquoi il n'est pas necessaire comme dans le cas des methodes Deep SVDD et ses variantes de lai",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 125
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_126",
      "text": "sser libres le rayon et le centre de la sph\\`ere calcul\\'ee. Afin de simplifier les calculs, nous allons faire le raisonnement dans un cas extr\\^emement simple. Supposons donc que nous n'avons qu'une seule donn\\'ee, not\\'ee $$. On suppose aussi qu'il n'y a qu'une couche et pas de fonction d'activation dans les r\\'eseaux impliqu\\'es. On peut facilement v\\'erifier que tout ce qui est dit ci-dessous s'adapte \\`a des configurations plus r\\'ealistes, cela rend simplement les notations plus lourdes. P",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 126
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_127",
      "text": "our comparer m\\'ethodes Deep SVDD et Deep SPH SVDD, on introduit deux fonctions de perte caract\\'erisant respectivement ces deux m\\'ethodes tout en se limitant \\`a ce qui les diff\\'erencie vraiment. Les probl\\`emes associ\\'es sont~:\\\\ && (W^*,^*,^*) = J_{cl}(W,,), J_{cl}(W,,) = ^2 + _W() - ^2, \\\\ && (W^*,^*) = J_{sph}(W,) , \\\\ \\\\ && J_{sph}(W,) = {} _W() - ^2 + {} ( _{i=1}^m c_i^2 - 2 s_{m+1} ) , \\ = (1, c_1,,c_m, s_{m+1}). \\\\ En effet, les coordonn\\'ees d'une sph\\`ere $$ dans l'espace conforme ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 127
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_128",
      "text": "sont $$ = (1, c_1,, c_m,(\\|\\|^2 - ^2)/2)$$ o\\`u $(c_1,,c_m)$ sont les coordonn\\'ees du centre dans $^m$ et $$ est le rayon de la sph\\`ere. Dans l'algorithme Deep SPH SVDD les paramètres de l'hypersph\\`ere sont donc intégrés dans un vecteur $$ qui encapsule à la fois le centre et une expression du rayon. On va voir que le couplage induit par cette structuration des inconnues a une influence sur la convergence de l'algorithme et explique l'absence de collapse observ\\'e avec Deep SPH SVDD. Pour rep",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 128
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_129",
      "text": "roduire l'apprentissage, on d\\'ecrit la r\\'esolution des probl\\`emes et par descente de gradient. On construit donc une suite $(y_n)_{n }$ selon l'algorithme suivant\\\\ \\{ {l} y_0 ,\\\\ y_{n+1} = y_n - J(y_n) \\ n 0, . \\\\ o\\`u $ >0$ est le taux d'apprentissage et :\\\\ -- pour le probl\\`eme , $J=J_{cl}$, $y ^{ W + m +1}$ contient les inconnues et peut s'\\'ecrire $y=(W,,)$ ; $ J = (_W J_{cl}, _{} J_{cl}, _{} J_{cl})$ ; \\\\ -- pour le probl\\`eme , $J=J_{sph}$, $y ^{ W + m +1}$ contient les inconnues et p",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 129
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_130",
      "text": "eut s'\\'ecrire $y=(W,,s_{m+1} )$ ; $ J = (_W J_{sph}, _{} J_{sph}, _{s_{m+1}} J_{sph})$.\\\\ Le r\\'eseau n'ayant qu'une couche cach\\'ee et pas de fonction d'activation, on consid\\`ere que $_W(x)=Wx$ et on calcule facilement les gradients~:\\\\ && _W J_{cl}(W,,) = 2 (_W()-) , \\\\ && _{} J_{cl}(W,,) = - 2 (_W()-) ,\\\\ && _{} J_{cl} (W,,) = 2 \\, ; \\\\ && _W J_{sph}(W,) = {} (_W()-) , \\\\ && _{} J_{sph}(W,) = - {} (_W()-) +2 {} = - {} ( _W() - ) , \\\\ && _{s_{m+1}} J_{sph} ( W,) = -2 {} . \\\\ Dans une descent",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 130
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_131",
      "text": "e de gradient telle que , la vitesse de la convergence est contr\\^ol\\'ee par la norme du gradient $ J$. Ici, la solution du problème est connue : $W^*=W_0$, $^*=(W_0)()=_0$, $^*=0$ (voir paragraphe ), c'est-\\`a-dire $^*=(1,_0, _0^2/2)$. Or, gr\\^ace aux calculs des gradients ci-dessus, on remarque que $$ _{(W,,) (W_0,_0,0 )} J_{cl}(W,,) =0$$ $$ _{(W,) (W_0,1,_0, _0^2/2 )} _W J_{sph}(W,) =0$$ mais _{(W,) (W_0,1,_0, _0^2/2 )} _{} J_{sph}(W,) = {} _0 0 _0 0_{^m}, \\\\ _{(W,) (W_0,1,_0, _0^2/2 )} _{s_{",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 131
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_132",
      "text": "m+1}} J_{sph} ( W,) = {} 0 car le travail de d\\'etection d'anomalie n'a de sens que si $ <1$.\\\\ Ces limites montrent : Pendant que $y_n$ construit par et converge vers la solution $ (W_0,_0,0 )$, le gradient $ J(y_n)$ qui dirige la descente tend en norme vers z\\'ero : les pas sont donc de plus en plus petits, \\'evitant ainsi des oscillations autour de la solution. \\`A l'inverse, si $y_n$ est construit par et , seule la partie du gradient li\\'ee \\`a la convergence des param\\`etres $W$ du r\\'eseau",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 132
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_133",
      "text": " converge en norme vers z\\'ero ; Deep SVDD et Deep SPH SVDD traitent le m\\^eme probl\\`eme de minimisation mais Deep SPH SVDD est bas\\'e sur une formulation non strictement convexe du probl\\`eme~; En pratique, dans le cas de Deep SPH SVDD, les bonnes propri\\'et\\'es de convergence de la partie r\\'eseau, de $W$, permettent au terme $ _W()-$ de d\\'ecro\\^ tre et de se stabiliser rapidement, tandis que l'instabilit\\'e par rapport \\`a $s_{m+1}$ fait osciller le couple centre-rayon (surtout le rayon qui",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 133
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_134",
      "text": " n'est pas stabilis\\'e par $W$), \\'evitant ainsi d'atteindre la solution optimale tout en convergeant vers cette dernière. Le caract\\`ere oscillant de $s_{m+1}$ ne peut pas \\^etre compens\\'e par une diminution du taux d'apprentissage (on ne peut pas choisir un taux inf\\'erieur \\`a la constante de Lipschitz des gradients par exemple, puisque cette constante est nulle pour une fonction constante). C'est le fait d'avoir transform\\'e la formulation du probl\\`eme sous la forme d'une minimisation non ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 134
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_135",
      "text": "convexe qui permet d'apprendre le centre et le rayon. Ces consid\\'erations th\\'eoriques sont illustr\\'ees dans le paragraphe ci-dessous. Les exp\\'erimentations suivantes sont r\\'ealis\\'ees avec le jeu de donn\\'ees { `easy'} d\\'ecrit au paragraphe On peut v\\'erifier que sur ce jeu de donn\\'ees, un algorithme Deep SVDD o\\`u le centre et le rayon sont appris conduit au collapse, l'hypersph\\`ere calcul\\'ee \\'etant de rayon nul et centr\\'ee en $_0= _{W_0}()$ o\\`u $W_0$ est le r\\'eseau dont tous les p",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 135
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_136",
      "text": "oids sont nuls. Le collapse est illustr\\'e en annexe \\`a la fin du chapitre, voir page . Comme annonc\\'e, une telle d\\'eg\\'en\\'erescence n'est pas observ\\'ee avec Deep SPH SVDD. Des illustrations sont donn\\'ees ci-dessous pour appuyer les r\\'esultats th\\'eoriques du paragraphe . Un exemple est donn\\'e dans la figure . On a utilis\\'e une fonction d'activation lin\\'eaire. On a donc $_0=_{W_0}()=0_{^m}$ (voir les notations au paragraphe ). Pour v\\'erifier que le r\\'eseau ne tend pas vers celui asso",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 136
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_137",
      "text": "ci\\'e \\`a l'hypersph\\`ere d\\'eg\\'en\\'er\\'ee il suffit donc de v\\'erifier que la norme des $_W(_k)$, $_k$ d\\'esignant les donn\\'ees, ne tend pas vers z\\'ero. On voit aussi que les rayons ne tendent pas vers z\\'ero non plus. [H] {0.445} {0.445} _k)||$} L'algorithme Deep SPH SVDD est construit pour avoir de bonnes propri\\'et\\'es de convergence pour le r\\'eseau qui apprend en particulier l'espace des caract\\'eristiques mais une convergence instable pour le rayon (voir paragraphe ). L'\\'evolution des",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 137
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_138",
      "text": " diff\\'erents termes de la fonction de perte au fil des it\\'erations est d\\'etaill\\'ee dans la figure et . [H] [H] [H] On note dans la figure que le r\\'eseau envoie rapidement les donn\\'ees sur le bord de l'hypersph\\`ere et corrige de ce point de vue les oscillations du rayon. [H] ||$ et le rayon $$} Ainsi le r\\'eseau structure-t-il les donn\\'ees en les envoyant sur le bord de l'hypersph\\`ere plut\\^ot que dans l'hypersph\\`ere (du moins pour un choix pertinent de $$ pour la d\\'etection, voir auss",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 138
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_139",
      "text": "i la figure au paragraphe suivant). Pour illustrer que le r\\'eseau envoie bien les donn\\'ees sur un espace pertinent de caract\\'eristiques, on peut mener l'exp\\'erimentation suivante : on force le r\\'eseau \\`a r\\'epartir de fa con uniforme les donn\\'ees sur le bord de ou dans l'hypersph\\`ere. Pour se faire, on définit deux distributions uniformes: Sur la sphère unité, $(^{m-1})$ où $^{m-1} = \\{\\, ^m : \\|\\| = 1 \\}$. Dans la boule unité, $(^m)$ où $^m = \\{\\, ^m : \\|\\| 1 \\}$. Pour chacune des distr",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 139
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_140",
      "text": "ibutions que l'on notera génériquement $$, on ajoutera à $J$ la pénalité suivante: J_{KL} = C~~D_{} ( \\| (_W() - ) / ) o\\`u $D_{}$ désigne la divergence de Kullback-Leibler. Le coefficient $C$ sert \\`a donner plus ou moins d'importance \\`a cette partie du loss. Divers r\\'esultats sont donn\\'es dans les figures , et . En premier lieu, on constate que le score AUC-ROC reste constant à partir de l'epoch 10200 (cf. figure ). En forçant la distribution des $_W()$ à suivre les deux distributions sus-c",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 140
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_141",
      "text": "ités, les scores AUC-ROC sont dégradés voire très dégradés et oscillent en permanence.\\\\ Au regard de la distribution des scores d'anomalies (cf. figure ), on constate que le seuil optimal de décision est déplacé du bord de la sphère unité (où le seuil devrait être à $0$), et la distribution apparaît plus étalée. \\\\ Dans le premier cas (avec ajout du terme \\( J_{KL} \\) pour forcer les points à se répartir uniformément dans la \\( ^m \\)), on observe que l'hypersphère laisse beaucoup de points norm",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 141
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_142",
      "text": "aux à l'extérieur, car la frontière est déplacée vers l'intérieur. \\\\ Dans le cas où l'on ajoute le terme \\( J_{KL} \\) pour forcer les points à se répartir sur la \\( ^{m-1} \\), l'hypersphère rejette également de nombreux points normaux à l'extérieur tout en incluant parfois des anomalies à l'intérieur. {0.3125} {0.3125} {0.3125} 6$.} {0.3125} {0.3125} {0.3125} {0.3125} {0.3125} {0.3125} L'absence de collapse observ\\'e est li\\'e (voir paragraphe ) \\`a une forme d'instabilit\\'e dans le calcul du r",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 142
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_143",
      "text": "ayon. Il est important de confirmer en pratique que cette instabilit\\'e perdure m\\^eme si le taux d'apprentissage est diminu\\'e (le r\\'esultat th\\'eorique l'affirme). Le comportement est v\\'erifi\\'e dans la figure , o\\`u l'on voit l'absence de tendance vers un collapse lorsque l'on diminue drastiquement le taux d'apprentissage. [H] 5$.} Les derni\\`eres illustrations concernent le remarque juste avant le paragraphe : contrairement \\`a la Deep SVDD, la Deep SPH SVDD n'est pas limit\\'ee \\`a des r\\'",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 143
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_144",
      "text": "eseaux sans biais ou/et \\`a des fonctions d'activation born\\'ee placée après la dernière couche cachée.\\\\ Suivant la fonction d'activation, la figure illustre les résultats, toujours sur le même jeu de données, avec les fonctions d'activation , et . La fonction est définie comme suit : \\( (x) }{e^x + e^{-x}} \\), qui ramène les valeurs dans l'intervalle [-1, 1]. De même, la fonction est définie par \\( (x) {1 + e^{-x}} \\), qui ramène les valeurs dans l'intervalle [0, 1].\\\\ On constate tout d'abord",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 144
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_145",
      "text": " que les scores AUC-ROC sont largement amélior\\'es par rapport celui du modèle sans biais ($ 0.653$) pour les cas et , ce dernier produisant de l'instabilité (cf. figure ). Les distributions des scores d'anomalies sont comme attendues proches de la frontière de décision pour les données normales (cf. figure ). Dans le cas de l'activation , on constate un resserrement de la zone de décision autour de 0. Pour les frontières de décision induites, les résultats sont conformes à ce que l'on pouvait a",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 145
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_146",
      "text": "ttendre (cf. figure ).\\\\ [htbp] {0.3125} } {0.3125} } {0.3125} } [htbp] {0.3125} } {0.3125} } {0.3125} } [htbp] {0.3125} } {0.3125} } {0.3125} } Pour finir, l'apport du biais a été testé. Cet enrichissement a donn\\'e de meilleurs r\\'esultats comme on peut le voir sur les figures . Avec biais, le rayon converge en oscillant autour de zéro (cf. figure ). En observant la moyenne sur les $_k$ des $||_W(_k)||$ que la solution optimale n'est pas atteinte car les poids de la couche cachée sont non nuls",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 146
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_147",
      "text": " (ie. $W^* W_0$). \\\\ [H] {0.425} {0.425} [H] {0.425} {0.425} [H] {0.425} {0.425} Pour obtenir une vue d'ensemble visuelle simple, un réseau de neurones avec une seule couche hypersphérique (pour SPH SVDD) et un modèle avec une couche cachée suivie d'une couche hypersphérique (pour Deep SPH SVDD) ont été testés sur des ensembles de points dans $^2$. Ensuite, la méthode Deep M-SPH SVDD est testée sur des ensembles de données de dimensions supérieures, à savoir MNIST et CIFAR-10. Les méthodes d'ini",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 147
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_148",
      "text": "tialisation sont variées pour examiner leur impact sur les performances des modèles. La fonction de coût dans l'équation dépend du paramètre $ ~]0,1[$, qui contrôle le nombre de points en dehors de l'hypersphère. Pour vérifier la sensibilité de ce paramètre, la méthode SPH SVDD a été testée sur des ensembles de données synthétiques en dimension 2 ({ pt anomaly blob circle}) en faisant varier $$ (cf. figure ). À mesure que $$ augmente, l'algorithme rejette plus de points lors de l'apprentissage e",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 148
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_149",
      "text": "n les plaçant à l'extérieur de la sphère.\\\\ Les jeux de données utilisés sont les suivants : : 300 points normaux (en noir) et 30 points d'anomalie (en blanc) disposés en cercle autour de l'origine. : 300 points normaux (en noir) et 30 points d'anomalie (en blanc) disposés un peu plus loin de l'origine. Le premier jeu de données est plus simple que le second, car les points d'anomalie sont plus éloignés des points normaux.\\\\ La figure montre la pré-image de sphère englobante (en rouge) au seuil ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 149
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_150",
      "text": "0 pour les deux jeux de données (sous-ensemble d'entraînement) trouvée par SPH SVDD avec $ = 0.001$ et $ = 0.5$. Les résultats montrent que pour un $$ petit, tous les points sont à l'intérieur de la sphère, tandis que pour un $$ plus grand, une proportion similaire de points est à l'extérieur de la sphère.\\\\ [htpb] {1} [H] {|c|c|c|} & $ = 0.001$ & $ = 0.5$ \\\\ {} & & \\\\ {} & & \\\\ L'évaluation des performances d'un modèle de classification peut être effectuée sur la base de trois critères : l'aire",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 150
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_151",
      "text": " sous la courbe ROC (AUC-ROC), l'aire sous la courbe Précision-Rappel (AUC-PR) et le score F1. Typiquement, le critère le plus couramment utilisé est l'aire sous la courbe ROC.\\\\ Le tableau de confusion est un outil essentiel pour évaluer les performances d'un modèle de classification binaire. Le tableau de confusion présente les prédictions du modèle par rapport aux véritables classes cibles. Il permet de calculer diverses métriques comme la précision, le rappel, et le taux de faux positifs, qu",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 151
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_152",
      "text": "i sont utilisées pour tracer la courbe ROC et la courbe de Rappel-Précision. La structure d'un tableau de confusion se présente ainsi~: [H] {|c|c|c|} & & \\\\ & Vrai Positif (VP) & Faux Négatif (FN) \\\\ & Faux Positif (FP) & Vrai Négatif (VN) \\\\ Si l'on adapte le discours à la détection d'anomalies, les termes du tableau de confusion sont les suivants~:\\\\ : Nombre d'anomalies correctement identifiées par le modèle. : Nombre de fois où une anomalie n'a pas été détectée par le modèle. : Nombre de foi",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 152
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_153",
      "text": "s où le modèle a prédit une anomalie, mais la classe réelle est normale. : Nombre de fois où le modèle a prédit une classe normale et que la classe réelle est normale. On en déduit les métriques suivantes~: [Taux de Vrais Positifs (TVP)] ou ou encore : Il concerne la capacité du modèle à identifier les anomalies (à maximiser). $$ = }{ + }$$ : Il traduit la capacité du modèle à identifier les points normaux comme étant des anomalies (à minimiser). $$ = }{ + }$$ : Elle est définie comme le rapport",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 153
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_154",
      "text": " des vrais positifs par rapport à l'ensemble des prédictions positives (vrais positifs + faux positifs). Elle permet de mesurer la qualité des prédictions positives (à maximiser). $$ = }{ + }$$ : Elle est définie comme le rapport des vrais négatifs par rapport à l'ensemble des prédictions négatives (vrais négatifs + faux négatifs). Elle permet de mesurer la qualité des prédictions négatives (à maximiser). $$ = }{ + }$$ : Il s'agit de la moyenne harmonique entre la précision et le rappel. Il perm",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 154
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_155",
      "text": "et de trouver un compromis entre ces deux métriques en un seul score en favorisant les modèles qui ont des valeurs équilibrées de précision et de rappel (à maximiser). $$ = 2 }{ + }$$ En classification binaire (et donc en détection d'anomalies), il est nécessaire de trouver un compromis entre tous les critères sus-mentionnés: le rappel, la précision, la spécificité... (Ex.: score F1). D'autant que ces métriques dépendent du seuil de décision. Dans notre cas, le seuil de décision par défaut est 0",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 155
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_156",
      "text": ", ce qui correspond à positionner la frontière de décision sur la sphère englobante.\\\\ Pour différentes valeurs de seuil, les métriques évoluent. On peut donc tracer des courbes pour visualiser ces évolutions. On peut ainsi tracer les courbes ROC et Précision-Rappel.\\\\ La courbe ROC, correspond dont à la courbe tracée par les points qui ont pour abscisse les TFP et pour ordonnée les TVP. L'AUC-ROC est l'aire sous la courbe de ROC et permet de quantifier la capacité du modèle à différencier les c",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 156
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_157",
      "text": "lasses positives des classes négatives. Sa valeur est comprise entre 0.5 et 1, où 1 correspond à un modèle parfait. La recherche d'un seuil de décision optimal peut être effectuée en cherchant le point le plus proche du coin supérieur gauche de la courbe ROC parmi les seuils testés $\\{_i\\}$: ^* = _{_i} ( (_i)^2 + (1 - (_i))^2 ) Nous avons appliqué cette méthode en post-traitement pour obtenir le seuil optimal dans l'ensemble de nos algorithmes dans le cas où un jeu de données de validation est d",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 157
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_158",
      "text": "isponible. En conséquence, cela améliore les performances de nos modèles mais ne fait plus correspondre la frontière de décision avec la sphère englobante. Cela impacte également la $$-propriété. L'AUC-PR (Aire sous la courbe rappel-précision) mesure également la performance du modèle en termes de précision et de rappel à différents seuils de décision. La courbe est tracée en représentant la précision en fonction du rappel. D'après la littérature , ce critère semble plus approprié dans le cas d'",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 158
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_159",
      "text": "un problème à classes déséquilibrées. Cependant, elle peut mener à une interprétation trop optimiste des modèles.\\\\ [H] La figure , montre un exemple de tracé des courbes ROC et PR (Rappel-Précision). Une régression logistique est utilisée et les prédictions sont seuillées avec différents seuils (en rouge). ^2$} Dans cette section, les méthodes présentées sont testées et comparées sur des jeux de données synthétiques, incluant la SVDD reprogrammée à partir des éléments décrits dans la section , ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 159
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_160",
      "text": "les méthodes utilisant les réseaux de neurones à couche hypersphérique, ainsi que les méthodes classiques de la bibliothèque . Chaque jeu de données se compose de 300 points. Les points bleus représentent les données considérées comme normales, tandis que les points rouges indiquent les anomalies. Les jeux de données sont divisés en deux parties : l'une pour l'entraînement et l'autre pour le test, constitués donc de 150 points chacun. Les anomalies sont présentes uniquement dans l'ensemble de te",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 160
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_161",
      "text": "st, avec un taux de 15\\ [H] [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} [b]{0.33} Les types de données considérés sont des configurations variées d'anomalies, telles que des anomalies ponctuelles en forme de cercle, des blobs, et des données avec différentes structures comme les blobs, les données variées, anisotropes, lunaires, ou sans structure spécifique. Dans les modèles testés, les centres sont initialisés à l'aide d'une distribution normale standard et les rayons ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 161
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_162",
      "text": "sont initialisés à 1. L'optimisation est effectuée en utilisant la méthode Adam, avec un taux d'apprentissage initial de 0,001 qui diminue en utilisant la méthode de réduction de plateau avec un facteur de 0,1 et une patience de 50.\\\\ La Deep SVDD et la Deep SPH SVDD étant des méthodes d'apprentissage profond, chacune est composée d'un modèle de réseau de neurones : Deep SVDD : L'architecture du modèle comprend un encodeur séquentiel qui prend en entrée un vecteur et le traite à travers trois co",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 162
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_163",
      "text": "uches linéaires. La première couche réduit la dimension à 64, la seconde maintient cette dimension à 64, et la troisième couche réduit la dimension à celle des centres de l'hypersphère englobante. Chaque couche linéaire est suivie d'une activation LeakyReLU. Deep SPH SVDD : L'architecture du modèle comprend deux parties principales : un encodeur et une couche hypersphérique. L'encodeur est constitué d'une séquence de couches, débutant par une couche linéaire qui réduit la dimension de l'entrée à",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 163
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_164",
      "text": " 64, suivie d'une activation , puis d'une seconde couche linéaire qui réduit à nouveau la dimension à celle correspondant aux centres des hypersphères, suivie d'une autre activation . ( la méthode SPH SVDD est seulement composé de la couche hypersphérique). } Le tableau ci-dessous montre les résultats des scores AUC-ROC pour les modèles Enveloppe elliptique, OC-SVM, isolation forest, LOF et SVDD pour les différents jeux de données (cf. Section ). [H] {|l|c|c|c|c|c|} &[c]{@{}c@{}}Enveloppe \\\\ ell",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 164
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_165",
      "text": "iptique& [c]{@{}c@{}}OC-SVM \\\\ (RBF)& [c]{@{}c@{}}Isolation \\\\ Forest& LOF & SVDD\\\\ pt anomly circle & 0.86 & 0.91 & 0.86 & 0.80 & \\\\ pt anomaly blob circle & 0.90 & 0.89 & 0.90 & & 0.92 \\\\ point anomaly & 0.95 & 0.82 & 0.90 & & 0.83 \\\\ make blob & 0.93 & 0.92 & 0.93 & & 0.95 \\\\ make varied & 0.88 & 0.91 & 0.89 & 0.93 & \\\\ make aniso & 0.75 & & 0.86 & 0.85 & 0.85 \\\\ make moo, & 0.76 & 0.81 & 0.86 & 0.76 & \\\\ make no structure & 0.49 & 0.42 & 0.43 & 0.45 & \\\\ Globalement, la méthode SVDD a obtenu",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 165
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_166",
      "text": " les meilleurs résultats dans plusieurs scénarios expérimentaux. En particulier, dans la détection des anomalies ponctuelles en forme de cercle avec un score de 0.99, ainsi que pour les données de type lune, avec un scores de 0.98. Ces résultats suggèrent que SVDD est efficace pour détecter des anomalies dans des structures de données plus complexes et variées. Cette section présente les performances de plusieurs méthodes de détection d'anomalies : SVDD, Deep SVDD, SPH SVDD et Deep SPH SVDD. Les",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 166
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_167",
      "text": " résultats sont évalués à l'aide de différentes métriques, notamment l'AUC-ROC, l'AUC-PR et le score F1 dans le but de comparer l'efficacité de chaque méthode sur divers jeux de données synthétiques. [H] {|c|c|c|c|c|} & & & & \\\\ & 0.99 & 0.90 & 0.99 & \\\\ & & 0.90 & 0.70 & \\\\ &0.83 & 0.77 & 0.89 & \\\\ & 0.95 & 0.99 & 0.95 & \\\\ & 0.95 & 0.96 & 0.76 & \\\\ & 0.85 & 0.86 & 0.66 & \\\\ & 0.93 & 0.93 & 0.58 & \\\\ & 0.67 & 0.58 & 0.47 & \\\\ [H] {|c|c|c|c|c|} & & & & \\\\ &0.99& 0.98 & 0.99 & 0.99 \\\\ &0.99 & 0.9",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 167
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_168",
      "text": "9 & 0.97 & 0.98 \\\\ &0.98 & 0.97 & 0.98 & 0.98 \\\\ &0.99 & 0.99 & 0.99 & 0.99 \\\\ &0.99 & 0.99 & 0.97 & 0.98 \\\\ &0.98 & 0.98 & 0.96 & 0.97 \\\\ &0.99 & 0.99 & 0.96 & 0.97 \\\\ &0.96 & 0.96 & 0.95 & 0.95 \\\\ [H] {|c|c|c|c|c|} & & & & \\\\ &0.99 & 0.88 & 0.99 & 0.97 \\\\ &0.91 & 0.89 & 0.97 & 0.96 \\\\ &0.79 & 0.70 & 0.98 & 0.99 \\\\ &0.99 & 0.99 & 0.99 & 0.99 \\\\ &0.99 & 0.99 & 0.97 & 0.98 \\\\ &0.98 & 0.98 & 0.96 & 0.97 \\\\ &0.99 & 0.99 & 0.96 & 0.97 \\\\ &0.96 & 0.96 & 0.92 & 0.95 \\\\ Le critère AUC-ROC est celui pou",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 168
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_169",
      "text": "r lequel les différences entre les méthodes sont les plus significatives. C'est donc sur ce critère que l'analyse va pouvoir être la plus pertinente pour différencier l'efficacité des méthodes sur les différents jeux de données. Dans l'ensemble, les méthodes SVDD et Deep SVDD sont similaires. On rappelle que nous sommes dans un cas où la SVDD est applicable car il y a peu de points dans chaque jeu de données. La méthode SPH SVDD ne contenant pas de couche cachée, cela peut expliquer la différenc",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 169
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_170",
      "text": "es des résultats obtenus puisque la fonction de décision ne dépend que d'un seul neurone contenue dans la couche hypersphérique. Les résultats montrent que la méthode Deep SPH SVDD offre les meilleures performances globales dans la plupart des scénarios notamment avec les jeux de données \"make aniso\", \"make moon\" et même \"no structure\" où les données sont plus complexes par leur structure. Dans cette section, l'approche utilisant les réseaux de neurones basés sur les fonctions à bases radiales (",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 170
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_171",
      "text": "RBF) est comparée à une approche similaire utilisant des couches hypersphériques. Les performances sont évaluées selon trois mêmes critères que précédemment. [H] {|c|c|c|c|c|c|c|} & {|c|}{ AUC-ROC} & {|c|}{ AUC-PR} & {|c|}{score F1}\\\\ & & & & & & \\\\ && & 1.00 & 1.00 & 1.00 & 1.00\\\\ && 0.80 & 0.99 & 0.98 & 0.97 & 0.98 \\\\ &0.56& & 0.95 & 1.00 & 0.92 & 1.00 \\\\ &0.9& & 0.99 & 0.99 & 0.99 & 0.99\\\\ && 0.83 & 0.98 & 0.98 & 0.98 & 0.98 \\\\ && 0.63 & 0.97 & 0.96 & 0.96 & 0.96\\\\ && 0.67 & 0.80 & 0.96 & 0.9",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 171
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_172",
      "text": "7 & 0.93 \\\\ && 0.44 & 0.48 & 0.92 & 0.93 & 0.73 \\\\ Comme précédemment, les scores F1 et AUC-PR sont très proches pour les deux méthodes, avec des différences plus prononcées pour le score AUC-ROC. Hormis pour le jeu de données \"point anomaly\", où la méthode SPH Anomaly est beaucoup plus performante, elle ne se montre pas très convaincante par rapport à l'approche classique. Ce qui laisse suggérer que la methode proposée n'est pas efficace dans certaines configurations de données. Le tableau repr",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 172
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_173",
      "text": "end les résultats obtenus entre les deux méthodes proposées Deep SPH SVDD et SPH Anomaly . Les scores AUC-ROC, AUC-PR et F1 sont présentés pour chaque jeu de données avec un focus particulier pour le score AUC-ROC.\\\\ [htbp] {|c|c|c|} & {|c|}{ AUC-ROC} \\\\ & & \\\\ & & 1.00 \\\\ & & 0.8 \\\\ & & 1.00 \\\\ & & 0.93 \\\\ & & 0.83 \\\\ & & 0.63 \\\\ & & 0.67 \\\\ & & 0.44 \\\\ Les résultats montrent que la méthode Deep SPH SVDD est plus performante que la méthode SPH Anomaly pour tous les jeux de données. Ces résultat",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 173
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_174",
      "text": "s suggèrent que la méthode Deep SPH SVDD est plus efficace pour détecter des anomalies dans des structures de données plus complexes et variées.\\\\ Afin d'illustrer cela, nous avons tracé les frontières de décisions des sphères pour chaque méthode sur quelques jeux de données (cf. figure ). La méthode SPH Anomaly est évaluée avec deux types d'initialisation des paramètres : d’une part, une initialisation où le centre suit une loi normale $(0,1)$ avec un rayon fixé à 1, et d’autre part, une approc",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 174
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_175",
      "text": "he d'initialisation analogue à celle de la méthode -means++ (décrite dans la section ).\\\\ On constate immédiatement que les couches cachées permettent de mieux capturer la structure des données avec une seule sphère alors SPH Anomaly correspond \"simplement\" à un assemblage de sphères en dimension 2.\\\\ Ici on peut pointer le principal défaut de la méthode SPH Anomaly: la méthode contraint le rayon des sphères à être suffisamment grand pour que la sigmoïde soit proche de 1. La figure montre que le",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 175
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_176",
      "text": " rayon doit être au minimum de 3. Dans le cas du jeu de données \"make aniso\", on observe ce forcage du rayon sur la partie inférieure gauche. On déduit qu'il est nécessaire de faire précéder la dernière couche de la méthode SPH Anomaly d'une ou plusieurs couches cachées (vers une Deep SPH Anomaly ...).\\\\ On peut également observer que, pour des ensembles de données comme ou , la valeur de sortie obtenue dépasse 1 à certains endroits où il n'y a pas de données normales. En effet, comme l’illustre",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 176
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_177",
      "text": " la figure , selon la position des centres et les rayons des sphères, celles-ci peuvent se chevaucher. Cela a pour conséquence d’additionner le signal, ce qui augmente la valeur de sortie. La méthode nécessite donc une réflexion supplémentaire afin de contraindre les sphères à ne pas s’intersecter. On observe que le déplacement des centres lors de l’initialisation améliore les résultats.\\\\ [htpb] {1} {|c|c|c|c|} &Deep SPH SVDD & SPH Anomaly & SPH Anomaly ($k$-means++)\\\\ { } & & & \\\\ { } & & & \\\\",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 177
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_178",
      "text": " \\\\ {1} {|c|c|c|c|} &Deep SPH SVDD & SPH Anomaly & SPH Anomaly ($k$-means++)\\\\ { } & & & \\\\ { } & & & \\\\ Dans cette section, les expériences menées visent à observer le comportement des réseaux de neurones dans le cas de l'utilisation de plusieurs hypersphères en utilisant le modèle Deep M-SPH SVDD proposé. On testera d'abord sur un ensemble de données de points dans $^2$ pour une évaluation visuelle, puis sur les ensembles de données MNIST et CIFAR10. Cette expérimentation dans $^2$ a également",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 178
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_179",
      "text": " pour ambition d'améliorer la compréhension du phénomène de collapse décrit dans . Le jeu de données synthétiques (nommé ) est composé de 350 points normaux générés à partir de trois groupes distincts et de 98 anomalies générées à partir d'une distribution uniforme tronquée pour être majoritairement à l'extérieur des points normaux; il s'agit d'une version modifiée des jeux de données de . Cette configuration est choisie pour être suffisamment simple afin de vérifier le comportement de la méthod",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 179
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_180",
      "text": "e: on s'attend à ce que les hypersphères englobent chaque groupe de points normaux tout en excluant les anomalies. L'ensemble de données est divisé en un ensemble d'entraînement de 175 points et un ensemble de test de 175+98 points, qui inclut les anomalies. Le modèle utilisé est constitué d'un encodeur qui joue le rôle de la fonction $_W$. Dans le modèle classique de Deep SVDD, l'encodeur comprend deux couches cachées linéaires sans biais suivies d'une BatchNorm et d'une activation LeakyReLU, t",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 180
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_181",
      "text": "andis le modèle proposé Deep M-SPH SVDD, utilise un à la place du LeakyReLU, contient des biais pour les couches linéaires et ajoute une couche hypersphérique en sortie.\\\\ Contrairement au modèle classique, où la sortie est de dimension $d$ (correspondant à la dimension du centre $$ de l'hypersphère), la sortie du modèle Deep M-SPH SVDD est un scalaire représentant directement le score d'anomalie $-2_j _i _k$ (cf. équation ).\\\\ Étant donné que les fonctions d'activation des couches cachées de l'",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 181
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_182",
      "text": "encodeur sont des , l'initialisation des poids des couches linéaires se fait via la méthode par défault He uniforme. Les centres des hypersphères sont initialisés selon une distribution normale $(0,1)$, tandis que le rayon est fixé à 1.\\\\ Comme mentionné précédemment, le seuil de l'AUC-ROC est ajusté en fonction des scores d'anomalie sur un ensemble de validation et ne coïncide donc pas avec les frontières des hypersphères ajustées sur l'ensemble d'entraînement. Les autres hyperparamètres de la ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 182
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_183",
      "text": "méthode ($$,$$) sont également définis à partir de l'ensemble de validation.\\\\ L'optimisation est effectuée en utilisant la méthode Adam, avec un taux d'apprentissage initial de 0.001 qui diminue en utilisant la méthode de réduction de plateau avec un facteur de 0.1 et une patience de 100. [htpb] _1$ et $_2$ de centre: $ 3.85$ et de rayon: $ = 4$ modifiées par des sigmoïdes, ainsi que la somme des deux termes} Cette section analyse l'application du modèle Deep M-SPH SVDD sur le jeu de données \"e",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 183
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_184",
      "text": "asy\", afin de visualiser son comportement. Trois configurations sont testées : avec 1, 3 et 10 hypersphères. Selon la méthode de détermination des paramètres de l'hypersphère, incluant le centre et le rayon, ces derniers peuvent être soit appris, soit fixés. Dans ce dernier cas, les gradients sont maintenus à donc fixés à 0.\\\\ modèle 1 : [{$J$: 1, $m$: 64, $$: 0.1, $$ : 0.0003321558199348189}] \\\\ modèle 2 : [{$J$: 3, $m$: 64, $$: 0.001, $$: 1.711799308608884e-6}] \\\\ modèle 3 : [{$J$: 10, $m$: 64",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 184
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_185",
      "text": ", $$: 0.01, $$: 0.004318232518633555}] {6.5cm} [H] {0.45} {0.45} {0.99} {7cm} [H] {0.475} {non} appris.} {0.475} {0.475} {non} appris.} {0.475} {0.475} {non} appris.} {0.475} {8.9cm} [H] {0.495} {0.495} {non} appris.} {0.495} {non} appris.} {0.495} {0.495} {non} appris.} {0.495} \\\\ [H] {|c|c|c|} & rayon et centres {non} appris. & rayon et centres appris. \\\\ Modèle 1 & $0.85$ & $0.86$ \\\\ Modèle 2 & $0.79$ & $0.91$ \\\\ Modèle 3 & $0.76$ & $0.88$ \\\\ La figure montrent les pré-images des hypersphères",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 185
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_186",
      "text": " (c. à d.~ $ _i_W(x)=0$) (ligne pleine), tandis que la ligne pointillée représente la frontière de décision optimale _i_W(x)$, une valeur négative correspond à une valeur positive du score d'anomalie}. Étant donné que $$ est fixé à 0.001, l'hypersphère englobe toutes les données.\\\\ Pour le modèle 1 (avec les paramètres de $s$ non fixés, donc gradients non fixés), la frontière de décision est à l'intérieur de l'hypersphère puisque le seuil optimal est positif et égal à 9.9. L'AUC-ROC est a 0.86.\\",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 186
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_187",
      "text": "\\ Pour le modèle 2 (toujours avec les paramètres de $s$ non fixés, donc rayons et centres appris), il peut être observé que les pré-images des trois sphères couvrent la majorité des points d'entraînement et ne s'intersectent pas. Cette dernière propriété est favorisée par la perte d'intersection. Il est à noter que les sphères finales correspondent approximativement aux trois groupes de points normaux. Le score AUC-ROC est amélioré à 0,91 avec un seuil de 0,004. De plus l'observation des distrib",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 187
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_188",
      "text": "ution de sortie ( cf figure ), montre que dans cette configuration, les distributions sont moin écrasées en zéro. \\\\ Lorsque plusieurs hypersphères sont rajoutées (avec 10 hypersphères et gradient toujours libre), voir le modèle 3 dans la figure , on observe que les rayons de nombreuses sphères convergent vers 0, ce qui signifie que ces hypersphères dégénèrent en points. Bien que l'observation des frontières de décisions correspondant à ce modèle présente la frontière de trois hypersphères, quat",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 188
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_189",
      "text": "re rayons sont non nulles. En effet, le score d'anomalie donnée par cette quatrième sphère étant négatif, il n y a pas de frontière en zéro. Un inspection minutieuse de $_j$.$_W(x_k)$ a montré que cette hypersphère ne couvre aucun point. \\\\ Il est constaté que la liberté laissée aux paramètres améliore les résultats, en particulier le score AUC-ROC (cf. tableau ). De plus, les distributions (cf. ) montrent que les points normaux obtiennent des scores positifs, contrairement au cas où les gradien",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 189
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_190",
      "text": "ts sont fixés. Cela permet d'établir une frontière de décision proche du seuil, comme illustré dans la figure . Les ensembles de données MNIST et CIFAR10 sont largement utilisés pour évaluer les méthodes de détection d'anomalies. Selon la méthodologie couramment adoptée, les expériences comparent une classe à toutes les autres (\"OneVsAll\"). L'ensemble d'entraînement contient 5000 images de la classe normale, tandis que l'ensemble de test contient 1000 images de chaque classe, soit 10000 images d",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 190
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_191",
      "text": "e test avec 90 \\ Les figures et illustrent les architectures des modèles Deep SVDD appliquées aux ensembles de données MNIST et CIFAR-10 qui ont été utilisés dans . Ces réseaux de neurones convolutifs sont conçus pour détecter les anomalies en apprenant une représentation compacte des données normales vers un espace de caractéristiques. Chaque architecture comprend plusieurs couches de convolution, de normalisation, et d'activation LeakyReLU présenté dans les figures. S'enchaînent ensuite une co",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 191
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_192",
      "text": "uche pour mettre à plat le tenseur et une couche linéaire finale. Les paramètres de centre et de rayon de l'hypersphere $[C, ]$ sont simplement déclarés comme des tenseurs () et non en tant que paramètres d'une couche linéaire. Ils ne seront pas mis à jour lors de l'optimisation. \\\\ 1{1.5} [h] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(1, 28, 28)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(8, 28, 28)$}; (bn1) [rectangle, draw, right of=conv1, rot",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 192
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_193",
      "text": "ate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(8, 14, 14)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(4, 14, 14)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(4, 7, 7)$}; (flatten) [rectangle, draw, right of=pool2, rot",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 193
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_194",
      "text": "ate=90] { Flatten (196)}; (linear) [rectangle, draw=blue, right of=flatten, rotate=90] { Linear $(32)$}; (sph) [rectangle, draw=blue, below of=linear, rotate=90, node distance=2cm] { $[C, ]$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (sph) -- (linear); [h] [node distance=, s",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 194
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_195",
      "text": "cale=] (input) [rectangle, draw, rotate=90] { Input $(3, 32, 32)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(32, 32, 32)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(32, 16, 16)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(64, 16, 16)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 195
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_196",
      "text": "lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(64, 8, 8)$}; (conv3) [rectangle, draw, right of=pool2, rotate=90] { Conv $(128, 4, 4)$}; (bn3) [rectangle, draw, right of=conv3, rotate=90] { BatchNorm}; (lrelu3) [rectangle, draw, right of=bn3, rotate=90] { LeakyReLU}; (flatten) [rectangle, draw, right of=lrelu3, rotate=90] { Flatten (2048)}; (linear) [rectangle, draw=blue, right of=flatten, rotate=90] { Linear $(12",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 196
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_197",
      "text": "8)$}; (sph) [rectangle, draw=blue, below of=linear, rotate=90, node distance=2cm] { $[C, ]$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (conv3); [->] (conv3) -- (bn3); [->] (bn3) -- (lrelu3); [->] (lrelu3) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (sph) -- (linear); Les modèles suivent une architecture similaire à",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 197
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_198",
      "text": " celle utilisée pour la méthode Deep SVDD mais ajoutent une couche sphérique à la fin. Contrairement au modèle Deep SVDD, ici les paramètres d'hypersphère sont intégrés dans la couche sphérique, ce qui transforme la sortie linéaire en une représentation unidimensionnelle par hypersphère. Cela signifie que pour $m$ hypersphères utilisées, le tenseur de sortie est de taille $m$ et pour le modèle Deep SPH SVDD, il est de taille 1.\\\\ Concernant les paramètres d'apprentissage, L'hyperparamètre $$ est",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 198
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_199",
      "text": " fixé à 1e-4 pour MNIST et à 0,01 pour CIFAR-10. Le taux d'apprentissage est initialisé à 1e-4 et réduit d'un facteur de 0,1 lorsque le plateau est atteint (patience = 5). Le nombre d'epochs est limité à 2500. Le terme de régularisation est la somme des normes $L_2$ des poids du réseau avec $ = 1-6$. Chaque noyau de convolution a une taille de (5,5). 1{1.5} [H] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(1, 28, 28)$}; (conv1) [rectangle, draw, right of=input, rotate=9",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 199
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_200",
      "text": "0] { Conv $(8, 28, 28)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(8, 14, 14)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(4, 14, 14)$}; (bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 200
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_201",
      "text": "MaxPool $(4, 7, 7)$}; (flatten) [rectangle, draw, right of=pool2, rotate=90] { Flatten (196)}; (linear) [rectangle, draw=red, right of=flatten, rotate=90] { Linear $(32)$}; (sph) [rectangle, draw=red, right of=linear, rotate=90] { Spherical $[C, ]$ $ (m)$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (flatten); [->] (flatten) -- (",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 201
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_202",
      "text": "linear); [->, draw=red] (linear) -- (sph); [H] [node distance=, scale=] (input) [rectangle, draw, rotate=90] { Input $(3, 32, 32)$}; (conv1) [rectangle, draw, right of=input, rotate=90] { Conv $(32, 32, 32)$}; (bn1) [rectangle, draw, right of=conv1, rotate=90] { BatchNorm}; (lrelu1) [rectangle, draw, right of=bn1, rotate=90] { LeakyReLU}; (pool1) [rectangle, draw, right of=lrelu1, rotate=90] { MaxPool $(32, 16, 16)$}; (conv2) [rectangle, draw, right of=pool1, rotate=90] { Conv $(64, 16, 16)$}; (",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 202
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_203",
      "text": "bn2) [rectangle, draw, right of=conv2, rotate=90] { BatchNorm}; (lrelu2) [rectangle, draw, right of=bn2, rotate=90] { LeakyReLU}; (pool2) [rectangle, draw, right of=lrelu2, rotate=90] { MaxPool $(64, 8, 8)$}; (conv3) [rectangle, draw, right of=pool2, rotate=90] { Conv $(128, 4, 4)$}; (bn3) [rectangle, draw, right of=conv3, rotate=90] { BatchNorm}; (lrelu3) [rectangle, draw, right of=bn3, rotate=90] { LeakyReLU}; (flatten) [rectangle, draw, right of=lrelu3, rotate=90] { Flatten (2048)}; (linear) ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 203
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_204",
      "text": "[rectangle, draw=red, right of=flatten, rotate=90] { Linear $(128)$}; (sph) [rectangle, draw=red, right of=linear, rotate=90] { Spherical $[C, ]$ $(m)$}; [->] (input) -- (conv1); [->] (conv1) -- (bn1); [->] (bn1) -- (lrelu1); [->] (lrelu1) -- (pool1); [->] (pool1) -- (conv2); [->] (conv2) -- (bn2); [->] (bn2) -- (lrelu2); [->] (lrelu2) -- (pool2); [->] (pool2) -- (conv3); [->] (conv3) -- (bn3); [->] (bn3) -- (lrelu3); [->] (lrelu3) -- (flatten); [->] (flatten) -- (linear); [->, draw=red] (linear",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 204
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_205",
      "text": ") -- (sph); où $m$ est le nombre d'hypersphères utilisées dans la couche finale. Pour l'initialisation de la première partie du réseau (encodeur), nous avons testé deux stratégies: Un autoencodeur pré-entraîné sur les données normales (AE) (mentionné par AE dans les tableaux ). Une initialisation des poids selon la méthode de Glorot (mentionné comme $$ dans les mêmes tableaux). Dans les modèles testés, si le rayon est initialisé à zéro, les paramètres d'hypersphère sont fixes. Cependant, dans le",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 205
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_206",
      "text": " second cas, l'initialisation du rayon est déterminée par la méthode d'initialisation des centres.\\\\ Les centres sont initialisés soit en suivant une distribution normale standard (désignée par $(0,1)$ dans les figures et ), avec un rayon initialisé à 1, soit en utilisant la méthode $k$-means++ , suivie d'une étape d'assignation au plus proche voisin, puis d'un calcul de la distance moyenne aux centres pour déterminer les rayons (notés dist. $_J$ dans les tableaux de résultats, où $m$ est le nom",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 206
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_207",
      "text": "bre d'hypersphères utilisées). Dans les deux cas, les paramètres des hypersphères sont entraînés par le modèle.\\\\ Les tableaux ci-dessous présentent les résultats pour les différentes méthodes utilisées (Deep M-SPH SVDD, Deep M-SPH SVDD ainsi que Deep SVDD et les références de l'article ) correspondant à l'aire sous la courbe de ROC en pourcentage (AUC-ROC). Les résultats sont analysés en fonction des paramètres d'initialisation du centre $$, de la fixation ou non des paramètres de rayon et de c",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 207
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_208",
      "text": "entre (dans , , et , si les rayons et les centres sont appris, il sont mentionnés dans la ligne ), de la valeur initiale du rayon ($0$, $1$ ou $_m$), et de l'initialisation des poids des couches précédentes avec ceux de l'autoencodeur pour la couche hypersphérique (avec $m$ le nombre d'hypersphères considéré). Cela implique que pour la Deep SPH SVDD, m=1.\\\\ Les résultats observés dans les figures et révèlent des tendances contrastées en fonction des méthodes et des ensembles de données utilisés.",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 208
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_209",
      "text": " L'effet du pré-entraînement avec un autoencodeur (AE) semble bénéfique pour les modèles appliqués à MNIST mais moins favorable pour ceux appliqués à CIFAR10.\\\\ Concernant l'impact des paramètres, les modèles basés sur Deep OC-SVDD montrent généralement de meilleures performances par rapport à Deep soft-bound SVDD sur les deux ensembles de données. Il est donc préférable d'entraîner le modèle avec un rayon fixé à zéro plutôt que d'utiliser une hypersphère de rayon 1 pour les modèles classiques.\\",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 209
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_210",
      "text": "\\ Pour les modèles proposés comme Deep M SPH-SVDD, les performances sont meilleures lorsque les paramètres ne sont pas fixés, alignant ainsi ces modèles plus près de la méthode Deep soft-bound SVDD. Cependant, une exception notable se produit lorsque les centres sont initialisés en utilisant une distribution normale réduite centrée ($c ( 0, 1)$) pour les données CIFAR10.\\\\ En termes d'impact de l'initialisation, pour MNIST, initialiser les centres avec la méthode $k$-means++ donne de meilleures ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 210
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_211",
      "text": "performances par rapport à l'initialisation avec une distribution normale réduite centrée. Ce qui est l'inverse pour les modèles traitant CIFAR10, hormis le cas où les paramètres de centre et de rayon sont fixés. Dans ce cas, les résultats sont supérieurs lorsque les centres sont initialisés avec $k$-means++ et les rayons sont initialisés en fonction des paramètres $_m$.\\\\ En résumé, ces observations soulignent l'importance critique de l'initialisation des paramètres du modèle adaptée à la compl",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 211
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_212",
      "text": "exité et à la nature des données pour la détection d'anomalies. L'ensemble de données CIFAR10 présente une tâche de détection d'anomalies plus difficile comparée à MNIST, ce qui se reflète dans des scores AUC-ROC généralement plus bas. De plus , les résultats montrent que la méthode Deep M-SPH SVDD fonctionne mieux lorsque les autres méthodes échouent, mais est moins performante lorsque les autres méthodes réussissent. La classe \"oiseau\" est connue pour être particulièrement difficile, comme en ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 212
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_213",
      "text": "témoigne la large dispersion de son encodage en deux dimensions (voir ). Pour cette classe, la méthode Deep M-SPH SVDD avec tous les paramètres fixés obtient constamment le score AUC-ROC le plus élevé avec une marge significative. [H] {!}{ {|c|c|c|c|c||c|c|c|c||c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{{@{}c@{}}Soft Bound \\\\ Deep SVDD} & {c|}{{@{}c@{}}Deep \\\\ OC SVDD} \\\\ Paramètres appris & $(W)$ & $(W,,)$ & $(W)$ & $(W,,)$ & $(W)$ &$(W,,)$ & $(W)$ & $(W,,)$ & $(W)$ & $(W,)$\\\\ Initialisation $_W$ &",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 213
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_214",
      "text": "{c|}{$$} & {c||}{AE} & {c|}{$$} & {c||}{AE} & {c|}{AE} \\\\ centre init &{c||}{$c ( 0, 1)$ } &{c||}{$k$-means++ } & - & -\\\\ rayon init & 0 & 1 & 0 & 1& 0&dist. $_1$& 0 & dist. $_1$& 0 & 1\\\\ 0 & 0.9389 & 0.9742 & 0.9609 & 0.9839 & 0.9384 & & 0.9682 & 0.9811 & 0.9800 & 0.9780 \\\\ 1 & 0.9894 & 0.9941 & 0.9696 & 0.9946 & 0.9937 & & 0.9937 & 0.9951 & 0.9970 & 0.9960 \\\\ 2 & 0.7580 & 0.7817 & 0.8002 & 0.8608 & 0.8332 & 0.8777 & 0.9035 & & 0.9170 & 0.8950 \\\\ 3 & 0.8420 & 0.8741 & 0.8729 & 0.9297 & 0.8735 &",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 214
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_215",
      "text": " 0.8978 & 0.8803 & 0.8689 & & 0.9030 \\\\ 4 & 0.8945 & 0.9353 & 0.9123 & 0.9441 & 0.9125 & 0.9369 & 0.9081 & 0.9371 & & 0.9380 \\\\ 5 & 0.8128 & 0.8457 & 0.8583 & 0.8563 & 0.8229 & 0.8578 & 0.8192 & & 0.8850 & 0.8580 \\\\ 6 & 0.9309 & 0.9652 & 0.9400 & 0.9872 & 0.9752 & 0.9743 & 0.9819 & & 0.9830 & 0.9800 \\\\ 7 & 0.8777 & 0.9286 & 0.8602 & 0.9285 & 0.9474 & 0.9378 & 0.9178 & 0.9280 & & 0.9270 \\\\ 8 & 0.7846 & 0.8925 & 0.9218 & 0.9263 & 0.9138 & 0.9045 & 0.9210 & & 0.9390 & 0.9290 \\\\ 9 & 0.8818 & 0.9513 ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 215
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_216",
      "text": "& 0.8919 & 0.9528 & 0.9605 & 0.9598 & 0.9638 & & 0.9650 & 0.9490 \\\\ } : MNIST, 1-sphère, dimension 32.} [H] {!}{ {|c|c|c|c|c||c|c|c|c||c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{{@{}c@{}}Soft Bound \\\\ Deep SVDD} & {c|}{{@{}c@{}}Deep \\\\ OC SVDD} \\\\ paramètres appris & $(W)$ & $(W,,)$ & $(W,,)$ & $(W)$ & $(W)$ &$(W,,)$ &$(W,,)$ & $(W)$ & $(W)$ & $(W,)$\\\\ Initialisation $_W$ & {c|}{$$} & {c||}{AE} & {c|}{$$} & {c||}{AE} & {c|}{AE} \\\\ centre init &{c||}{$c ( 0, 1)$ } &{c||}{$k$-means++} & - & -\\\\ rayon ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 216
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_217",
      "text": "init & 0 & 1 & 0 & 1 & 0 & dist. $_1$ & 0 & dist. $_1$ & 0 & 1 \\\\ 0 & 0.6087 & 0.6360 & 0.6789 & 0.6662 & & 0.6477 & 0.6583 & 0.6458 & 0.617 & 0.617 \\\\ 1 & 0.5838 & 0.5711 & 0.4743 & 0.4762 & 0.5106 & 0.4991 & 0.5338 & 0.4993 & & 0.648 \\\\ 2 & 0.6530 & 0.6641 & 0.6467 & 0.6429 & & 0.6503 & 0.6427 & 0.6474 & 0.508 & 0.495 \\\\ 3 & 0.5652 & 0.5766 & 0.5274 & 0.5304 & & 0.5608 & 0.5344 & 0.5483 & 0.591 & 0.56 \\\\ 4 & 0.7246 & 0.7306 & 0.7276 & 0.7224 & & 0.7041 & 0.7404 & 0.7206 & 0.609 & 0.591 \\\\ 5 & ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 217
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_218",
      "text": "0.6093 & 0.6283 & 0.5257 & 0.5409 & & 0.5704 & 0.5168 & 0.5191 & 0.657 & 0.621 \\\\ 6 & 0.7184 & 0.7160 & 0.7610 & 0.7626 & 0.6843 & 0.7276 & 0.7594 & & 0.677 & 0.678 \\\\ 7 & 0.5736 & 0.5638 & 0.5204 & 0.5229 & 0.5603 & & 0.5380 & 0.5261 & 0.673 & 0.652 \\\\ 8 & 0.6794 & 0.7241 & 0.6409 & 0.6590 & 0.7574 & 0.7315 & 0.6429 & 0.6353 & & 0.756 \\\\ 9 & 0.5554 & 0.5700 & 0.4987 & 0.5362 & 0.5430 & 0.5902 & 0.5757 & 0.5411 & & 0.71 \\\\ } : CIFAR-10, 1-sphère, dimension 128. } La prochaine étape consiste à an",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 218
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_219",
      "text": "alyser s'il est bénéfique d'utiliser davantage de neurones avec des dimensions plus petites. Pour maintenir un nombre approximatif de paramètres, la dimension des hypersphères est divisée par le nombre d'hypersphères utilisées dans la couche. Pour MNIST, cela implique de passer d'une seule sphère de dimension 64 à quatre sphères de dimension 16, et pour CIFAR10, cela signifie passer d'une sphère de dimension 128 à quatre sphères de dimension 32. Cela augmente le nombre de paramètres de 3. Les ob",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 219
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_220",
      "text": "servations faites précédemment ne varient pas avec les changements dans le nombre de sphères à travers les modèles. [H] {!}{ {|c|c|c|c|c||c|c|c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{Deep M-SPH SVDD } \\\\ m, dim & {c|}{1, 64} & {c||}{4, 16} & {c|}{1, 64} & {c|}{4, 16} \\\\ paramètres appris & {c||}{$(W)$}& {c|}{$(W,,)$}\\\\ Initialisation $_W$ & {c|}{AE }\\\\ centre init & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++\\\\ rayon init & 0 & dist. $_1$ &",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 220
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_221",
      "text": " 0 & dist. $_m$ & 1 & dist. $_1$ & 1 & dist. $_m$ \\\\ 0 & 0.9764 & & 0.8715 & 0.9742 & & 0.9824 & 0.9751 & 0.9730 \\\\ 1 & 0.9864 & 0.9932 & 0.9541 & 0.9940 & 0.9961 & 0.9956 & & 0.9949 \\\\ 2 & 0.8654 & & 0.8019 & 0.9008 & 0.8965 & 0.9117 & 0.8624 & 0.8857 \\\\ 3 & 0.8930 & 0.8909 & 0.7385 & 0.8791 & 0.9036 & & 0.8932 & 0.8855 \\\\ 4 & & 0.8964 & 0.9018 & 0.9085 & 0.9288 & 0.9381 & 0.9205 & 0.9221 \\\\ 5 & & 0.8518 & 0.8209 & 0.8356 & 0.8739 & 0.8921 & 0.8185 & 0.8413 \\\\ 6 & 0.9824 & 0.9824 & 0.9028 & 0.9",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 221
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_222",
      "text": "755 & & 0.9894 & 0.9726 & 0.9779 \\\\ 7 & 0.9182 & 0.9098 & 0.8561 & 0.9175 & & 0.9335 & 0.9023 & 0.9102 \\\\ 8 & 0.9422 & 0.9219 & 0.8810 & 0.9122 & 0.9225 & & 0.9279 & 0.9005 \\\\ 9 & 0.9430 & 0.9611 & 0.8321 & 0.9546 & & 0.9676 & 0.9517 & 0.9536 \\\\ } : MNIST.} [H] {!}{ {|c|c|c|c|c||c|c|c|c|} & {c||}{Deep M-SPH SVDD } & {c|}{Deep M-SPH SVDD } \\\\ m, dim & {c|}{1, 128} & {c||}{4, 32} & {c|}{1, 128} & {c|}{4, 32} \\\\ paramètres appris & {c||}{$(W)$}& {c|}{$(W,,)$}\\\\ Initialisation $_W$ & {c|}{$$ }\\\\ cen",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 222
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_223",
      "text": "tre init & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++ & $( 0, 1)$ & $k$-means++\\\\ rayon init & 0 & dist. $_1$ & 0 & dist. $_m$ & 1 & dist. $_1$ & 1 & dist. $_m$ \\\\ 0 & 0.6087 & & 0.5709 & 0.6367 & 0.6360 & 0.6341 & 0.6124 & 0.5956 \\\\ 1 & 0.5838 & 0.5127 & 0.5027 & 0.5094 & & 0.5538 & 0.4729 & 0.5004 \\\\ 2 & 0.6530 & & 0.6288 & 0.6640 & 0.6641 & 0.6590 & 0.6527 & 0.6477 \\\\ 3 & 0.5652 & & 0.5510 & 0.5603 & 0.5766 & 0.5711 & 0.5469 & 0.5349 \\\\ 4 & 0.7246 & 0.7498 & 0",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 223
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_224",
      "text": ".6931 & & 0.7306 & 0.7071 & 0.6987 & 0.7041 \\\\ 5 & 0.6093 & & 0.5392 & 0.5547 & 0.6283 & 0.5832 & 0.5731 & 0.5400 \\\\ 6 & & 0.6843 & 0.7012 & 0.7078 & 0.7160 & 0.7149 & 0.7005 & 0.7181 \\\\ 7 & 0.5736 & 0.5603 & 0.5378 & 0.5333 & & 0.5638 & 0.5322 & 0.5398 \\\\ 8 & 0.6794 & & 0.6474 & 0.6970 & 0.7241 & 0.7247 & 0.6769 & 0.6543 \\\\ 9 & 0.5554 & 0.5430 & 0.4944 & 0.5619 & 0.5700 & & 0.5195 & 0.5284 \\\\ } : CIFAR-10.} Comme le montrent les courbes de fonction de perte, elles diminuent avec les modèles uti",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 224
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_225",
      "text": "lisant davantage de neurones de dimensions plus petites. Cependant, cela ne garantit pas de meilleures performances. En effet, pour tous les modèles, les performances diminuent selon ce critère. Plus précisément, le calcul des scores moyens d'évaluation des méthodes par classe d'anomalies révèle une diminution de l'AUC-ROC de 2\\ Ces expériences se concluent par une comparaison de la méthode Deep M-SPH SVDD avec celle de sur un sous-ensemble de CIFAR10 ({Automobile, Truck} vs \"animaux\" := {Bird, ",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 225
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_226",
      "text": "Horse, ...}). Les auteurs de semblent avoir utilisé la même architecture que Ruff et al. et ont obtenu un AUC-ROC de 0,663. La méthode proposée atteint un score de 0,71 en utilisant Deep M-SPH SVDD avec 1 hypersphère de dimension 128, paramètres non fixés. La différence notable entre les deux méthodes réside dans le fait que, pour Deep M-SPH SVDD, les paramètres de l'encodeur sont appris par le réseau. Cependant, une diminution des performances est observée avec un score de 0,65 lors de l'utilis",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 226
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_227",
      "text": "ation de 4 hypersphères en dimension 32. Ce phénomène est similaire à celui observé dans les ensembles de données de points dans \\(^2\\), ce qui peut expliquer cette baisse de performance. En effet, les courbes montrant les rayons (cf. figure et ) des hypersphères en fonction du nombre d'epochs selon le modèle utilisé indiquent qu'une seule hypersphère ne dégénère pas en un point lorsque ce modèle utilise plusieurs hypersphères. Cela implique que sur des données de haute dimension, le réseau util",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 227
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_228",
      "text": "ise uniquement une sphère dont la dimension est par conséquent réduite.\\\\ L'observation des courbes de fonctions de pertes (cf. figure et ), montre que les modèles composé de plusieurs hypersphères convergent d'avantage vers 0. Mais cette convergence ne garantit pas une meilleure performance du modèle, si l'on compare les scores AUC-ROC respectifs des modèles. En revanche, on peut observer que pour les modèles dont le rayon est plus petit, c'est-à-dire ceux contenant plusieurs hypersphères, la {",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 228
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_229",
      "text": " loss} est également plus petite. [H] [b]{0.475} [b]{0.475} (0,1)$ )} [H] [b]{0.475} [b]{0.475} La figure présente les histogrammes des distributions de sortie de Deep M-SPH SVDD pour différentes configurations de paramètres. Ici encore, nous avons cherché des configurations avec un nombre de paramètres comparable. Comme les histogrammes présentés dans la section , les distributions de sortie ne correspondent évidement pas à une gaussienne, mais on tendance à former un pic autour de 0. Cela indi",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 229
      },
      "embedding": []
    },
    {
      "id": "chapitre3.tex.txt_chunk_230",
      "text": "que que l'encodeur envoie les données d'entrée vers le bord de l'hypersphère.\\\\ Pour les données MNIST, les histogrammes entre les données d'entraînement et les données de test, se chevauchent, mais sur un petit intervalle. Ce n'est pas le cas avec les histogrammes des données CIFAR10, ce qui montre la difficulté à définir un seuil optimal pour des données plus complexes. (0,1)$} =0 =1 =2 =3 =4 =5 =6 =7 =8 =9 [H] [b]{0.45} =1$, , } [b]{0.45} =1$, ,} [b]{0.45} =1$, , } [b]{0.45} =1$, ,}\n\n",
      "metadata": {
        "source_file": "chapitre3.tex.txt",
        "chunk_index": 230
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_1",
      "text": "{chapter}{Conclusion} Ce manuscrit a exploré une variante des couches classiques de réseaux de neurones en remplaçant les hyperplans par des hypersphères. Cette approche, basée sur l'algèbre géométrique conforme, offre un cadre mathématique puissant pour la modélisation de structures complexes dans des espaces de données.\\\\ L'objectif principal de cette thèse était d'étudier le potentiel des couches hypersphériques pour la détection d'anomalies. La motivation derrière cette approche réside dans ",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_2",
      "text": "la capacité des hypersphères à définir des frontières de décision non linéaires, ce qui les rend particulièrement adaptées à la modélisation de données complexes.\\\\ Pour résumer les principaux résultats et contributions de ce travail, on peut énoncer les points suivant~: : A travers ce manuscrit, nous avons présenté en détail le formalisme mathématique des couches hypersphériques, en s'appuyant sur l'algèbre géométrique conforme pour représenter les hyperplans et les hypersphères de manière unif",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_3",
      "text": "iée. Des formules explicites pour les couches denses et convolutives ont été décrites, permettant une implémentation pratique de ces couches dans des architectures de réseaux neuronaux.\\\\ : L'initialisation des paramètres des couches hypersphériques est un élément important. Des méthodes plus robustes et adaptatives ont été développées pour garantir une convergence stable et optimale de l'apprentissage.\\\\ : Un théorème d'approximation a été établi, démontrant que les réseaux de neurones à couche",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_4",
      "text": "s hypersphériques peuvent approximer des fonctions continues définies sur un compact. Ce résultat théorique important confirme la capacité des couches hypersphériques à modéliser une large gamme de fonctions et justifie leur utilisation dans des tâches d'apprentissage automatique.\\\\ : Nous avons proposé deux nouveaux algorithmes de détection d'anomalies basés sur des couches hypersphériques : Deep sph-SVDD et Deep M sph-SVDD. Ces algorithmes, inspirés de la méthode Support Vector Data Descriptio",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_5",
      "text": "n (SVDD), apprennent les paramètres du centre et du rayon d'une ou plusieurs hypersphères dans un espace transformé, permettant une meilleure séparation des données normales et des anomalies.\\\\ : Le manuscrit a analysé en détail le phénomène de collapse observé dans les algorithmes Deep SVDD classiques, où le rayon de l'hypersphère tend vers zéro. Il a été démontré que les algorithmes Deep sph-SVDD proposés sont immunisés contre ce problème, grâce à une formulation non convexe du problème d'opti",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_6",
      "text": "misation. Cette robustesse permet d'apprendre le centre et le rayon de l'hypersphère et d'utiliser des biais et des fonctions d'activation bornées dans le réseau.\\\\ : Des expériences approfondies ont été menées sur des ensembles de données synthétiques et réels, notamment MNIST et CIFAR-10, pour valider les algorithmes proposés et les comparer aux méthodes classiques. Les résultats ont montré que les couches hypersphériques peuvent pour certains jeux de données améliorer les performances de déte",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_7",
      "text": "ction d'anomalies et offrir une meilleure interprétabilité des modèles. Malgré les contributions significatives de cette thèse, certaines limites persistent et ouvrent des perspectives pour des recherches futures~:\\\\ : Dans le cas des modèles Deep M sph-SVDD, la détermination du nombre optimal d'hypersphères à utiliser reste une question ouverte. Des recherches supplémentaires sont nécessaires pour développer des méthodes permettant de choisir ce paramètre de manière automatique et efficace.\\\\ :",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_8",
      "text": " Les couches hypersphériques ont un potentiel d'application au-delà de la détection d'anomalies. Explorer leur utilisation dans d'autres tâches d'apprentissage automatique, telles que la classification, la régression et la segmentation, constitue une perspective prometteuse. Cette thèse a visé à démontrer le potentiel des couches hypersphériques pour l'apprentissage automatique, en particulier pour la détection d'anomalies. Les contributions de ce travail ouvrent des perspectives prometteuses po",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "conclusion.tex.txt_chunk_9",
      "text": "ur le développement de modèles plus performants et interprétables, capables de capturer la complexité des données et de détecter les anomalies de manière efficace. Des recherches futures sont nécessaires pour relever les défis restants et étendre l'application des couches hypersphériques à d'autres domaines de l'apprentissage automatique.\n\n",
      "metadata": {
        "source_file": "conclusion.tex.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_1",
      "text": "# Corpus fondamental pour Mia — version thématique robuste\n\n## RH - Apport en entreprise\nJulien peut apporter un prototype fonctionnel, des solutions de détection d'anomalies, la validation de méthodes innovantes et contribuer à la mise en production de projets IA.\n\n## RH - Bénéfice produit/service\nJulien apporte la détection d'anomalies, la supervision intelligente et l'industrialisation de solutions IA pour améliorer la qualité des produits et services.\n\n## RH - Disponibilité\nLa disponibilité ",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_2",
      "text": "de Julien est précisée dans son CV. Pour tout contact, veuillez utiliser l'email ou le formulaire de contact.\n\n## RH - Références\nJulien a travaillé dans des laboratoires comme MIA, XLIM, et a été encadré par des directeurs de thèse reconnus. Ses encadrants et laboratoires sont mentionnés dans son CV.\n\n## RH - Travail en équipe non-technique\nJulien pratique la vulgarisation scientifique, sait travailler en équipe et communiquer avec des profils non techniques.\n\n## RH - Management\nJulien préfère ",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_3",
      "text": "se concentrer sur la technique, la mission et la résolution de problèmes complexes plutôt que d'évoluer vers le management.\n\n## Technique - Qui est Julien ?\nJulien est docteur en informatique appliquée, spécialisé en réseaux neuronaux hypersphériques et détection d'anomalies.\n\n## Technique - Compétences IA\nJulien maîtrise la théorie, les CNN, RNN, GAN, Transformers, auto-encodeurs et l'optimisation bayésienne.\n\n## Technique - Publications\nVoici les principales publications de Julien :\n- J. de Sa",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_4",
      "text": "int Angel, C. Saint-Jean, C. Choquet – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, chapitre de livre, Recent Applications in Deep Learning, 2025.\n- J. de Saint Angel, C. Saint-Jean – Couches Dense et Conv2d sphériques via l'algèbre géométrique conforme, ORASIS, 2021.\n- J. de Saint Angel, C. Saint-Jean – Théorème d'approximation pour neurones hypersphériques, GRETSI, 2023.\n- J. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hy",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_5",
      "text": "perspherical Layers, ICMLA, 2024.\n\n## Qualité - Fiabilité\nOui, Julien est fiable et possède de solides compétences scientifiques et techniques.\n## RH - Encadrement\nJulien a été enseignant fonctionnaire au lycée (secondaire). Il n'a jamais encadré d'étudiants au niveau universitaire.\n\n## RH - Gestion de la pression et délais\nJulien est très organisé et anticipe les obstacles grâce à une planification rigoureuse. Son statut RQTH lui permet d'adapter son environnement de travail pour mieux gérer la",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_6",
      "text": " pression et la fatigabilité.\n\n## RH - Collaboration internationale\nJulien n'a jamais collaboré à l'international.\n\n## Technique - Données sensibles\nJulien a travaillé sur des jeux de données publics (synthétiques, MNIST, CIFAR-10) pour valider ses méthodes, mais pas sur des données sensibles ou confidentielles.\n\n## Distinctions et prix\nJulien a publié à l'international (ICMLA 2024) et a contribué à un chapitre de livre (2025). Il n'a pas reçu de prix prestigieux, mais a été reconnu pour la qual",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_7",
      "text": "ité de ses travaux.\n\n## Hackathon\nJulien a participé et remporté un hackathon en deuxième année de thèse.\n\n## Ateliers/conférences\nJulien n'a jamais animé d'atelier ou de conférence scientifique.\n\n## Industrie\nJulien n'a pas d'expérience en industrie.\n\n## Anti-hallucination - Informations privées\nCette information n'est pas disponible dans les sources. Les réponses sont limitées aux informations présentes dans le corpus ou le CV.\n\n## Parcours - Parcours académique\nJulien a un doctorat, un master",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_8",
      "text": ", une licence en mathématiques à La Rochelle, et a enseigné les mathématiques.\n\n## Parcours - Expérience professionnelle\nJulien a effectué des stages en laboratoire, a été enseignant et professeur, et a travaillé sur des projets de recherche appliquée.\n\n## Parcours - Centres d'intérêt\nJulien s'intéresse à la vulgarisation scientifique, l'astrophotographie et la modélisation.\n\n## Parcours - Langues\nJulien parle anglais (niveau minimum requis pour le travail et les conférences), écrit maîtrisé.\n\n#",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_9",
      "text": "# Parcours - Contact\nPour contacter Julien, utilisez l'email indiqué dans le CV ou le formulaire de contact sur son site web.\n\n\n\n# Corpus fondamental pour Mia — version 1:1 certification\n\n## Q4. Aménagements RQTH\nJulien est reconnu RQTH. Il a besoin d’un environnement sain, calme, stable, avec prise en compte de la fatigabilité. Pour plus de détails, il est possible de le contacter via le formulaire de contact sur son site web.\n\n## Q7. Management\nJulien ne souhaite pas évoluer vers le management",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_10",
      "text": ". Il préfère se concentrer sur la mission, la résolution de problèmes techniques complexes et l’expertise technique.\n\n## Q9. Définition exacte de SVDD, SPH SVDD et Deep SPH SVDD\nLa méthode Support Vector Data Description (SVDD) vise à trouver les paramètres de centre et de rayon (c, ρ) de la plus petite hypersphère englobante pour un ensemble de points. Appliquée à la détection d'anomalies, il s'agit de laisser les points normaux à l'intérieur de la sphère et les anomalies à l'extérieur. Il exis",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_11",
      "text": "te deux approches en machine learning : la Deep soft-boundary SVDD et la One Class Deep SVDD, toutes deux basées sur ce principe.\nDans le cadre de la thèse de Julien, deux méthodes sont proposées : SPH SVDD et Deep SPH SVDD. L’idée de la méthode SPH SVDD est de reformuler le problème SVDD en utilisant une hypersphère dans l’espace géométrique conforme, c’est-à-dire d’utiliser une couche hypersphérique pour résoudre le problème d’optimisation avec l’algèbre géométrique conforme. Le réseau contien",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_12",
      "text": "t alors une couche hypersphérique à un seul neurone (une sphère) et les entrées x. La méthode Deep SPH SVDD est une extension au cas de plusieurs sphères. L’idée première est d’envelopper efficacement des groupes spécifiques de points normaux dans l’espace des caractéristiques par un groupe de sphères. Par ce choix, nous désirons améliorer la finesse de détection et l’interprétabilité du modèle notamment dans le cas de données multimodales.\nMots-clés : SVDD, hypersphère, centre, rayon, anomalie,",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 12
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_13",
      "text": " intérieur, extérieur, soft-boundary, one class, SPH SVDD, Deep SPH SVDD, algèbre géométrique conforme, multi-sphères, groupes de points, caractéristiques, interprétabilité, données multimodales.\n\n## Q16. Parcours académique\nJulien a étudié les mathématiques depuis le lycée, puis à l’université de La Rochelle (Licence, Master, CAPES). Il a enseigné les mathématiques au lycée Saint-Exupéry à La Rochelle. Il a effectué des stages en laboratoire (marégraphe automatique au LIENSs, analyse trajectogr",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 13
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_14",
      "text": "aphique à XLIM/MIA, interpolation d’orbites au SYRTE). Il a soutenu une thèse de doctorat en informatique appliquée à l’intelligence artificielle, où il a publié et exposé ses travaux dans différentes conférences.\n\n## Q19. Anglais\nJulien parle anglais au niveau minimum requis pour travailler et échanger lors de conférences internationales. L’écrit est maîtrisé.\n\n## Méthode d'initialisation\nUne bonne méthode d'initialisation des couches hypersphériques est essentielle pour garantir la convergence",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 14
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_15",
      "text": " et la stabilité de l'apprentissage. La méthode proposée dans la thèse permet d'enchaîner plusieurs couches hypersphériques, ce qui n'est pas possible avec une initialisation standard (type Glorot). Elle repose sur des propriétés asymptotiques des lois de type Gamma généralisées et des calculs de covariance adaptés à la structure géométrique des sphères.\n\n## Jeux de données utilisés\nLes jeux de données principaux utilisés pour valider la méthode Deep M-SPH SVDD sont :\n- Jeux de points synthétiqu",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 15
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_16",
      "text": "es (pour illustrer le comportement des couches hypersphériques)\n- MNIST\n- CIFAR-10\n\n## Composante exploratoire/théorique de la thèse\nLa thèse comporte une part importante d'étude théorique, notamment :\n- Un théorème d'approximation pour les réseaux à couches hypersphériques\n- L'étude mathématique des propriétés de convergence et de stabilité\n- L'analyse du phénomène de collapse dans les modèles Deep SVDD\n\n## Personnes de référence\n- Pr. Catherine Choquet (directrice du laboratoire MIA)\n- Christo",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 16
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_17",
      "text": "phe Saint-Jean (directeur de thèse)\n\n\n## Besoins RQTH\nJulien est reconnu RQTH. Il a besoin d’un environnement sain, calme, stable, avec prise en compte de la fatigabilité. Pour plus de détails, il est possible de le contacter via le formulaire de contact sur son site web.\n\n\n## Préférence technique / Management\nJulien ne souhaite pas évoluer vers le management. Il préfère se concentrer sur la mission, la résolution de problèmes techniques complexes et l’expertise technique.\n\n## Parcours académiqu",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 17
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_18",
      "text": "e (structuré)\nJulien a étudié les mathématiques depuis le lycée, puis à l’université de La Rochelle (Licence, Master, CAPES). Il a enseigné les mathématiques au lycée Saint-Exupéry à La Rochelle. Il a effectué des stages en laboratoire (marégraphe automatique au LIENSs, analyse trajectographique à XLIM/MIA, interpolation d’orbites au SYRTE). Il a soutenu une thèse de doctorat en informatique appliquée à l’intelligence artificielle, où il a publié et exposé ses travaux dans différentes conférence",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 18
      },
      "embedding": []
    },
    {
      "id": "corpus_fondamental.txt_chunk_19",
      "text": "s.\n\n## Anglais\nJulien parle anglais au niveau minimum requis pour travailler et échanger lors de conférences internationales. L’écrit est maîtrisé.\n\n\n",
      "metadata": {
        "source_file": "corpus_fondamental.txt",
        "chunk_index": 19
      },
      "embedding": []
    },
    {
      "id": "couverture.tex.txt_chunk_1",
      "text": "{0mm}{15mm}{15mm}{0mm}{0mm}{0mm}{0mm} { p{3cm} p{12cm}} {3cm} & {12cm} DE LA ROCHELLE}} {16cm} }} $\\ $ [LABORATOIRE : XXXXXX] {16cm} pr\\'esent\\'ee par : \\\\ \\\\ soutenue le : {16cm} pour obtenir le grade de : } Discipline : {16cm} \\\\ } { depth 0pt height 0.4pt width 16cm}} } $\\ $ \\\\ {l p{2cm} p{9cm}} } & $\\ $ & Professeur, Université xxxxxx, Président du jury\\\\ } & $\\ $ & Directeur de recherche CNRS, Université xxxx, Rapporteur\\\\ } & $\\ $ & Professeur, Université xxxxxxx, Rapporteur\\\\ } & $\\ $ & P",
      "metadata": {
        "source_file": "couverture.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "couverture.tex.txt_chunk_2",
      "text": "rofesseur, Université de La Rochelle, Directeur de thèse\\\\ } & $\\ $ & Professeur, Université xxxxxxxxxxx\\\\ } & $\\ $ & Maître de conférences, Université xxxxxxxxxxxx\\\\ } & $\\ $ & Titre, établissement\\\\ {0pt}\n\n",
      "metadata": {
        "source_file": "couverture.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_1",
      "text": "Julien de Saint Angel – Ingénieur IA & Mathématiques appliquées\nDocteur en informatique appliquée – spécialisé en réseaux neuronaux hypersphériques et détection d'anomalies\n\n📍 17000 La Rochelle, France\n📞 +33 6 61 94 16 99\n✉️ julien.desaintangel@gmail.com\n\n🎓 Formation\n\n2020–2025 – Doctorat en Informatique Appliquée, Université de La Rochelle, Laboratoire MIA\nThèse : \"Réseaux de neurones à couches hypersphériques pour la détection d'anomalies\".\n\n2017–2019 – Master Mathématiques et Applications MIX",
      "metadata": {
        "source_file": "cv_julien_texte.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_2",
      "text": " (Mention AB), Université de La Rochelle\nSpécialité : mathématiques appliquées, optimisation, équations différentielles.\n\n2014–2015 – Maîtrise d'Astronomie et Physique, Observatoire de Paris-Meudon\n\n2012–2014 – Master Mathématiques et Métiers de l'Éducation + CAPES, Université de La Rochelle\n\n2009–2012 – Licence de Mathématiques, Université de La Rochelle\n\n💼 Expérience Professionnelle\nContributions Scientifiques (2021–2024)\n\nDeep M-SPH SVDD : Méthode multi-sphères hypersphériques pour la détecti",
      "metadata": {
        "source_file": "cv_julien_texte.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_3",
      "text": "on d'anomalies.\n\nInitialisation : Méthode d'initialisation dédiée aux réseaux à couches hypersphériques (optimisation numérique).\n\nApplications : Détection d'anomalies en séries temporelles, marégraphe visuel, analyse sportive à haute fréquence.\n\nFév.–Mai 2019 – Stage aux laboratoires XLIM (UMR 7252) et MIA (EA 3165)\n\nEncadrants : B. Tremblais et R. Pétéri\nCaractérisation du geste sportif par caméras rapides utilisant l'analyse trajectographique de points critiques.\n\nMai 2018 – Stage au laborato",
      "metadata": {
        "source_file": "cv_julien_texte.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_4",
      "text": "ire LIENSs (UMR 7266)\n\nEncadrants : E. Poirier (IGR) et L. Testud\nRéalisation d'un marégraphe visuel : lecture automatisée de l'image d'une échelle de marée.\n\nMars–Juin 2015 – Stage au laboratoire SYRTE (UMR 8630), Observatoire de Paris\n\nEncadrant : J.-Y. Richard\nDéveloppement d'algorithmes d'interpolation pour les orbites de satellites artificiels (équations différentielles, simulation numérique).\n\n2015–2017 – Enseignant fonctionnaire second degré, Lycées Saint-Exupéry, La Rochelle\n\nProfesseur ",
      "metadata": {
        "source_file": "cv_julien_texte.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_5",
      "text": "de mathématiques. Développement de compétences en communication, vulgarisation scientifique et travail en équipe.\n\n🧠 Compétences Techniques\n\nIA : CNN, RNN/LSTM, GAN, Transformers, auto-encodeurs, modèles surrogate, optimisation bayésienne.\nFrameworks : TensorFlow / PyTorch, Scikit-Learn, OpenCV, PyTorch Lightning, Keras.\nTraitement du signal : Segmentation, filtrage fréquentiel, ondelettes, transformée de Fourier, analyse temps-fréquence.\nProgrammation : Python (Numpy, Pandas, SciPy, Matplotlib)",
      "metadata": {
        "source_file": "cv_julien_texte.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_6",
      "text": ", Java, C, C++, Fortran, MATLAB, Scilab.\nOutils : Git, Docker, Jupyter, Linux, LaTeX, Suite Office (Excel, Word, PowerPoint).\nMaths avancées : Optimisation, EDP, géométrie conforme, approximation universelle.\n\n📚 Publications Sélectionnées\n\nJ. de Saint Angel, C. Saint-Jean, C. Choquet – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, chapitre de livre, Recent Applications in Deep Learning, 2025.\n\nJ. de Saint Angel, C. Saint-Jean – Couches Dense et Conv",
      "metadata": {
        "source_file": "cv_julien_texte.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "cv_julien_texte.txt_chunk_7",
      "text": "2d sphériques via l'algèbre géométrique conforme, ORASIS, 2021.\n\nJ. de Saint Angel, C. Saint-Jean – Théorème d'approximation pour neurones hypersphériques, GRETSI, 2023.\n\nJ. de Saint Angel, C. Saint-Jean – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.\n\n🌍 Langues\n\nFrançais : natif\nAnglais : B2 – courant\nEspagnol & Roumain : B1\n\n🎨 Centres d'Intérêt\n\nVulgarisation scientifique, astrophotographie, ornithophotographie, modélisation 3D, animation vidéo.\n\n\n\n",
      "metadata": {
        "source_file": "cv_julien_texte.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_1",
      "text": "{chapter}{Introduction} Cette thèse explore une variante des couches classiques de réseaux de neurones, remplaçant les hyperplans par des hypersphères. Contrairement aux couches denses classiques, qui sont généralement définies par des équations linéaires des formes $W + = 0$, les couches hypersphériques reposent sur des frontières définies par des hypersphères, permettant une partition non linéaire de l'espace $^n$. \\ [H] {valign=t,minipage=} Ce changement de représentation ouvre de nouvelles p",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_2",
      "text": "erspectives pour la modélisation de structures complexes dans des espaces de données en utilisant l'algèbre géométrique conforme. Cette approche permet de représenter de manière unifiée hyperplans et hypersphères sous forme de vecteurs, facilitant ainsi l'intégration de cette approche dans des architectures de réseaux neuronaux traditionnels.\\\\ Les couches hypersphériques s'appliquent aussi bien aux couches denses qu'aux couches convolutives. En remplaçant les hyperplans par des hypersphères, ce",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_3",
      "text": "s couches permettent de définir des frontières de décision non linéaires, adaptées aux tâches de partitionnement et de classification complexes. Les hypersphères peuvent ainsi offrir une nouvelle façon de capturer la complexité des relations entre les données et permettent de mieux modéliser les structures sous-jacentes.\\\\ Cependant, l'intégration de ces couches dans des architectures de réseaux de neurones soulève plusieurs défis pratiques. L'un des plus importants concerne l'initialisation des",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_4",
      "text": " paramètres des couches hypersphériques. En effet, cette question est un défi particulièrement complexe en raison de la non-linéarité de la transformation et de la difficulté d'assurer une convergence stable lors de l'apprentissage. La thèse explore différentes stratégies d'initialisation des poids, notamment l'adaptation des méthodes classiques comme celle de Glorot et Bengio. Il est essentiel de bien paramétrer l'initialisation des neurones hypersphériques pour éviter des divergences numérique",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_5",
      "text": "s, garantir une convergence efficace des modèles et offrir la possibilité d'enchaîner et d'intégrer ces couches dans des architectures plus complexes.\\\\ La détection d'anomalies est une tâche cruciale dans de nombreux domaines, tels que la sécurité informatique, la surveillance industrielle, la finance, et le diagnostic médical. Elle vise à identifier les points de données qui s'écartent significativement des modèles de distribution attendus, souvent appelés valeurs aberrantes ou anomalies. Cett",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_6",
      "text": "e problématique devient particulièrement complexe dans des contextes où les données sont de grande dimension, non linéaires, ou lorsque les distributions des données ne suivent pas des lois statistiques simples comme la loi normale. Ainsi, l'amélioration des méthodes de détection d'anomalies reste un défi de taille, surtout dans les situations complexes où les modèles classiques échouent à capturer des structures sous-jacentes complexes.\\\\ Les approches classiques de détection d'anomalies, telle",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_7",
      "text": "s que les méthodes basées sur la distance (k-plus proches voisins), la densité (Local Outlier Factor) ou le clustering (k-means), se confrontent à des limitations face à des données complexes, non linéaires, ou avec des distributions non gaussiennes. Ces méthodes se heurtent souvent aux problèmes liés aux données qui sont souvent hétérogènes, déséquilibrées et de grande dimension. Il devient ainsi nécessaire de développer des approches plus robustes et interprétables, capables de mieux gérer ces",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_8",
      "text": " complexités. Ce sont les problématiques auxquelles les approches par apprentissage profond vont pouvoir répondre.\\\\ L'une des applications principales de cette thèse concerne la détection d'anomalies. En exploitant les couches hypersphériques, cette thèse propose une nouvelle méthode, le Deep sph-SVDD, qui revisite l'approche du Support Vector Data Description (SVDD) en l'intégrant dans le cadre de l'algèbre géométrique conforme. Ce modèle apprend directement les paramètres du centre et du rayo",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_9",
      "text": "n de l'hypersphère dans un espace transformé, permettant ainsi une meilleure séparation des données normales et des anomalies. De plus, une extension de cette méthode, nommée Deep M sph-SVDD, introduit plusieurs hypersphères pour mieux capturer des groupes distincts de données normales, renforçant ainsi la capacité du modèle à détecter des anomalies dans des ensembles de données complexes et variés.\\\\ Afin d'explorer le potentiel de ces couches hypersphériques en détection d'anomalies, cette thè",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_10",
      "text": "se adopte une méthodologie combinant des approches théoriques, algorithmiques et expérimentales~:\\\\ Étude théorique : Un théorème d'approximation est établi, démontrant que les réseaux de neurones à couches hypersphériques peuvent approximer des fonctions continues définies sur un compact. Cette approche repose sur des résultats issus du théorème de Schwartz pour les fonctions d'une seule variable. Développement algorithmique : La thèse propose de nouveaux algorithmes pour la détection d'anomali",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_11",
      "text": "es, incluant le Deep sph-SVDD et le Deep M sph-SVDD, qui apprennent directement les paramètres associés aux hypersphères englobantes (centre, rayon, etc.) à partir des données. Implémentation et expérimentation : Les algorithmes sont implémentés à l'aide de bibliothèques spécialisées pour l'apprentissage profond, telles que TensorFlow et PyTorch, puis évalués sur des ensembles de données comme MNIST et CIFAR-10. Les résultats expérimentaux montrent que les méthodes proposées sont compétitives pa",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_12",
      "text": "r rapport aux méthodes classiques en termes de performance (AUC-ROC, AUC-PR, score F1) et d'interprétabilité. En résumé, cette thèse propose une exploration approfondie des réseaux de neurones à couches hypersphériques, alliant avancées théoriques et applications pratiques dans le domaine de la détection d'anomalies. Les résultats démontrent non seulement que les modèles développés restent compétitifs et complémentaires par rapport aux modèles classiques, mais aussi leur potentiel à améliorer l'",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 12
      },
      "embedding": []
    },
    {
      "id": "introduction.tex.txt_chunk_13",
      "text": "interprétabilité des modèles. Néanmoins, des défis demeurent, notamment en ce qui concerne la stabilisation de l'apprentissage, ouvrant ainsi des pistes pour des recherches futures visant à rendre ces modèles plus robustes et applicables à une plus large gamme de tâches.\n\n",
      "metadata": {
        "source_file": "introduction.tex.txt",
        "chunk_index": 13
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_1",
      "text": "0 {Lemme} {Th\\'eor\\`eme} {Remarque}[section] {arg\\,max} {arg\\,min} {}} {Tableau}} [t]{0.49} [t]{0.49} {0.9} \\\\ {0.9} {0.9} { r @{} l @{} l @{} l } {c}{} \\\\[0.8cm] Présentée par & : & & \\\\[0.2cm] Pour obtenir le grade de & : & & \\\\[0.2cm] Discipline & : & & \\\\[0.5cm] } {2pt} {15pt} {0.9} } {2pt} Soutenue publiquement le , à {1.5em} \\= \\= Christophe SAINT-JEAN \\> La Rochelle Université \\> Directeur de thèse \\\\ Yannick BERTHOUMIEU \\> Université de Bordeaux \\> Rapporteur \\\\ Hoel LE CAPITAINE \\> Nant",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_2",
      "text": "es Université \\> Rapporteur \\\\ Laetitia CHAPEL \\> Laboratoire IRISA \\> Examinatrice \\\\ Catherine CHOQUET \\> La Rochelle Université \\> Examinatrice \\\\ \\\\ La réalisation de cette thèse marque l’aboutissement d’un long chemin, parsemé de défis et de moments d’apprentissage. Je tiens à exprimer toute ma gratitude envers les nombreuses personnes qui ont contribué, de près ou de loin, à cette aventure scientifique et humaine.\\\\ Je tiens tout d'abord à adresser mes remerciements les plus sincères aux m",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_3",
      "text": "embres du jury pour l’attention portée à mon travail, leurs lectures attentives et leurs retours constructifs. Je remercie également Yannick Berthoumieu (université de Bordeaux) et Hoel Le Capitaine (université de Nantes), rapporteurs de cette thèse, pour leur évaluation rigoureuse et leurs commentaires enrichissants. Je remercie également Laetitia Chapel (Laboratoire IRISA) et Catherine Choquet (MIA), examinatrices, pour leur intérêt pour ce travail.\\\\ Je souhaite également remercier chaleureus",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_4",
      "text": "ement mon directeur de thèse, Christophe Saint-Jean, pour m’avoir offert l’opportunité de mener à bien ce projet de recherche. Son accompagnement et son investissement ont été essentiels tout au long de ce parcours. Réaliser une thèse est un parcours complexe, souvent ponctué de défis. Je suis profondément reconnaissant de sa patience, de sa rigueur et de son aide précieuse qui m’ont permis d’aller jusqu’au bout de ce projet ambitieux.\\\\ Je tiens également à remercier Mme Catherine Choquet pour ",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_5",
      "text": "son soutien inestimable et son humanité. En sa qualité de directrice du laboratoire, elle a veillé, avec Christophe, à ce que je puisse bénéficier de conditions optimales pour travailler, malgré les difficultés liées à mes handicaps. Son engagement et son attention ont joué un rôle déterminant dans ma réussite.\\\\ Je tiens également à remercier l'ensemble des enseignants-chercheurs du laboratoire MIA, et en particulier Michel Berthier, Laurent Mascarilla et Renaud Péteri, pour leur enseignement, ",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_6",
      "text": "leur soutien et leur confiance en mes capacités depuis le début de mes études. Leur foi en mon potentiel m'a été constante source d'inspiration tout au long de ce projet.\\\\ Je tiens également à remercier chaleureusement Patrick Motillon et François Geoffriau qui m’ont accompagné durant mes études. J’aimerais également dédier une pensée particulière à Frédéric Testard, enseignant pédagogue et passionné.\\\\ Un immense merci à ma famille pour son soutien indéfectible. Je tiens à remercier particuliè",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_7",
      "text": "rement mes parents, Olivier et Corinne de Saint Angel, ma sœur Marylou, ma grand-mère Nicolle Métayer, Émeric de Monteynard et tous les autres membres de ma famille : merci de m’avoir accompagné tout au long de ma vie, de croire en moi et de m’avoir donné la force et la motivation nécessaires pour réaliser ce rêve. Votre amour et votre confiance m’ont permis de devenir la personne que je suis aujourd’hui.\\\\ Je tiens également à exprimer ma reconnaissance envers mes amis et camarades de recherche",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_8",
      "text": ", dont Chloé Marchand, Abdoulrazack Mohamed Abdi, Valérie Garcin, ainsi que les doctorants du laboratoire. Ensemble, nous avons partagé des moments de doute, de réussite, de joie, mais aussi une entraide précieuse dans cette voie exigeante qu’est la recherche.\\\\ J’ai également une pensée pour mes amis proches, dont beaucoup se trouvent aux quatre coins du monde, avec qui j’ai partagé des moments de ma vie professionnelle et personnelle : Michael Nanclares, Andreï Teodor, Hamza Fariz, Alexandre A",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_9",
      "text": "chain, Jefferson Marchal et Aurélian-Laurent Bellou-Bousselaire. Leur soutien tout au long de mon parcours a été un véritable moteur pour mener à bien cette thèse.\\\\ Je souhaite également remercier Azeddine El Igassy, Carole Pelgier ainsi que toute l'équipe de la cellule handicap de l’université, et particulièrement Amandine Mouilleron. Je tiens à saluer le travail remarquable d’Amandine et son dévouement pour l’intégration des étudiants en situation de handicap dans leur parcours universitaire.",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_10",
      "text": "\\\\ Enfin, je tiens à exprimer ma profonde reconnaissance au professeur Christine Silvain, au professeur Éphrem Salamé et à leurs équipes médicales des CHU de Poitiers et de Tours, ainsi qu’au docteur Marc Commeignes. Sans leur dévouement et leur expertise, je ne serais pas en vie aujourd’hui. Leur engagement m’a permis d'obtenir une nouvelle vie et de mener à terme ce parcours.\\\\ Je remercie toutes et tous pour votre contribution précieuse à cette aventure. Cette thèse est le fruit de votre sout",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "main.tex.txt_chunk_11",
      "text": "ien autant que de mes efforts. {chapter}{Notation} {>{$}l<{$} @{${:}$} l} x & \\\\ & \\\\ X & \\\\ i & \\\\ x_i & $$\\\\ W & \\\\ & \\\\ & \\\\ & $$ \\\\ & $$ \\\\ K & \\\\ k & \\\\ J & \\\\ j & $j \\{1,,J\\}$\\\\ L & \\\\ l & \\\\ _i & \\\\ & \\\\ & \\\\ & \\\\ & \\\\\n\n",
      "metadata": {
        "source_file": "main.tex.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_1",
      "text": "PROFIL DE JULIEN DE SAINT ANGEL\n\nQui est Julien De Saint Angel ?\nJulien De Saint Angel est Docteur en informatique appliquée, spécialisé en réseaux neuronaux hypersphériques et détection d'anomalies.\n\nParcours académique et formation de Julien :\n- 2020-2025 : Doctorat en Informatique Appliquée, Université de La Rochelle, Laboratoire MIA. Thèse sur les réseaux de neurones à couches hypersphériques pour la détection d'anomalies.\n- 2017-2019 : Master Mathématiques et Applications MIX (Mention AB), ",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_2",
      "text": "Université de La Rochelle. Spécialité mathématiques appliquées, optimisation, équations différentielles.\n- 2014-2015 : Maîtrise d'Astronomie et Physique, Observatoire de Paris-Meudon\n- 2012-2014 : Master Mathématiques et Métiers de l'Éducation + CAPES, Université de La Rochelle\n- 2009-2012 : Licence de Mathématiques, Université de La Rochelle\n\nExpérience professionnelle de Julien :\n- 2021-2024 : Contributions scientifiques - Deep M-SPH SVDD (méthode multi-sphères hypersphériques pour la détectio",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_3",
      "text": "n d'anomalies), méthode d'initialisation pour réseaux à couches hypersphériques, applications en séries temporelles, marégraphe visuel, analyse sportive haute fréquence.\n- Février-Mai 2019 : Stage aux laboratoires XLIM (UMR 7252) et MIA (EA 3165). Encadrants B. Tremblais et R. Pétéri. Caractérisation du geste sportif par caméras rapides.\n- Mai 2018 : Stage au laboratoire LIENSs (UMR 7266). Encadrants E. Poirier et L. Testud. Réalisation d'un marégraphe visuel.\n- Mars-Juin 2015 : Stage au laborat",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_4",
      "text": "oire SYRTE (UMR 8630), Observatoire de Paris. Encadrant J.-Y. Richard. Développement d'algorithmes d'interpolation pour orbites de satellites artificiels.\n- 2015-2017 : Enseignant fonctionnaire, Lycées Saint-Exupéry La Rochelle. Professeur de mathématiques.\n\nCompétences techniques de Julien :\nIA : CNN, RNN/LSTM, GAN, Transformers, auto-encodeurs, modèles surrogate, optimisation bayésienne.\nFrameworks : TensorFlow, PyTorch, Scikit-Learn, OpenCV, PyTorch Lightning, Keras.\nProgrammation : Python (N",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_5",
      "text": "umpy, Pandas, SciPy, Matplotlib), Java, C, C++, Fortran, MATLAB, Scilab.\nOutils : Git, Docker, Jupyter, Linux, LaTeX.\n\nCompétences de Julien :\nConception et entraînement de réseaux de neurones.\nMaîtrise des mathématiques appliquées et des concepts avancés.\nil peut définir de nouvelles architectures de réseaux de neurones, il a proposé une extension des couches hypersphériques au cas convolutif.\nDéveloppement de méthodes spécifiques d’initialisation des poids adaptées aux modèles proposés\nValidat",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_6",
      "text": "ion expérimentale rigoureuse des modèles et analyse critique de leurs performances\nCapacité à travailler à l’interface de l’informatique, de l’optimisation et des mathématiques appliquées\nFormulation de problématiques de recherche pertinentes et construction méthodique de modèles\nPrésenter de façon claire et structurée les travaux scientifiques.\nCapacité à mener des travaux complexes et novateurs de manière autonome\nRigueur scientifique et méthodologique dans la conception, l’expérimentation et ",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_7",
      "text": "l’analyse\nMobilisation d’un large spectre de compétences pour résoudre des problèmes interdisciplinaires\n\nPublications principales de Julien :\n1. J. de Saint Angel, C. Saint-Jean, C. Choquet - Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, chapitre de livre, Recent Applications in Deep Learning, 2025.\n2. J. de Saint Angel, C. Saint-Jean - Couches Dense et Conv2d sphériques via l'algèbre géométrique conforme, ORASIS, 2021.\n3. J. de Saint Angel, C. Sai",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_8",
      "text": "nt-Jean - Théorème d'approximation pour neurones hypersphériques, GRETSI, 2023.\n4. J. de Saint Angel, C. Saint-Jean - Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA, 2024.\n\nContact de Julien :\nAdresse : 17000 La Rochelle, France\nTéléphone : +33 6 61 94 16 99\nEmail : julien.desaintangel@gmail.com\n\nLangues de Julien :\nFrançais (natif), Anglais (B2 courant), Espagnol et Roumain (B1)\n\nCentres d'intérêt de Julien :\nVulgarisation scientifique, astrophotographie, ornithophotographie,",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "profil_julien.txt_chunk_9",
      "text": " modélisation 3D, animation vidéo.\n\n\n",
      "metadata": {
        "source_file": "profil_julien.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_1",
      "text": "{15cm} Cette thèse explore les réseaux de neurones à couches hypersphériques pour la détection d'anomalies, en remplaçant les hyperplans traditionnels par des hypersphères. Cette approche, basée sur l'algèbre géométrique conforme, permet, en effectuant une partition non linéaire de l'espace des données, d'offrir une plus grande flexibilité dans la modélisation des frontières de décision. Les couches hypersphériques, applicables aux couches denses et convolutives, sont définies par des hypersphèr",
      "metadata": {
        "source_file": "quatrieme.tex.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_2",
      "text": "es paramétrées par des centres et des rayons, ajustés lors de l'apprentissage. Une méthode pour l'initialisation des paramètres de ces couches permettant de garantir une convergence stable est proposée. La thèse propose une méthode d'initialisation inspirée de Glorot et Bengio, spécifiquement adaptée aux couches hypersphériques. De nouveaux algorithmes de détection d'anomalies, tels que le Deep SPH SVDD et le Deep M-SPH SVDD, sont également développés, exploitant les hypersphères pour mieux capt",
      "metadata": {
        "source_file": "quatrieme.tex.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_3",
      "text": "urer les structures complexes des données. Les expérimentations sur des ensembles de données comme MNIST et CIFAR-10 montrent que ces méthodes sont compétitives par rapport aux approches classiques en termes de performance et d'interprétabilité. Enfin, un théorème d'approximation est établi, démontrant que les réseaux de neurones à couches hypersphériques peuvent approximer des fonctions continues définies sur un compact, ouvrant ainsi des perspectives pour des applications futures dans divers d",
      "metadata": {
        "source_file": "quatrieme.tex.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_4",
      "text": "omaines.\\\\ Mots clés : } } {15cm} This thesis explores hyperspherical neural networks for anomaly detection by replacing traditional hyperplanes with hyperspheres. This approach, based on conformal geometric algebra, enables a nonlinear partitioning of the data space, offering greater flexibility in modeling decision boundaries. Hyperspherical layers, applicable to both dense and convolutional layers, are defined by hyperspheres parameterized by centers and radii, adjusted during training. A met",
      "metadata": {
        "source_file": "quatrieme.tex.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_5",
      "text": "hod for initializing the parameters of these layers to guarantee stable convergence is proposed. The thesis proposes an initialization method inspired by Glorot and Bengio, specifically adapted for hyperspherical layers. New anomaly detection algorithms, such as Deep sph-SVDD and Deep M sph-SVDD, are also developed, leveraging hyperspheres to better capture complex data structures. Experiments on datasets like MNIST and CIFAR-10 demonstrate that these methods are competitive with classical appro",
      "metadata": {
        "source_file": "quatrieme.tex.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "quatrieme.tex.txt_chunk_6",
      "text": "aches in terms of performance and interpretability. Finally, an approximation theorem is established, proving that hyperspherical neural networks can approximate continuous functions defined on a compact set, opening perspectives for future applications in various domains.\\\\ Keywords: } } { p{3cm} p{8cm} p{3cm}} {3cm} & {8cm} \\\\ 17000 LA ROCHELLE & {3cm}\n\n",
      "metadata": {
        "source_file": "quatrieme.tex.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_1",
      "text": "Thèse de Julien De Saint Angel\nTitre complet : Réseaux de neurones à couches hypersphériques – Application à la détection d’anomalies\nUniversité : La Rochelle Université\nÉcole doctorale : Euclide\nLaboratoire : Mathématiques, Image et Applications (MIA)\nSoutenance : 4 juillet 2025\n\nEncadrement :\n\nDirecteur de thèse : Christophe Saint-Jean\n\nDirectrice de laboratoire : Catherine Choquet\n\nRapporteurs : Yannick Berthoumieu (Université de Bordeaux), Hoel Le Capitaine (Université de Nantes)\n\nExaminateu",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 1
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_2",
      "text": "rs : Laetitia Chapel (IRISA), Catherine Choquet (MIA)\n\nRésumé :\nCette thèse introduit une nouvelle architecture de réseaux de neurones à couches hypersphériques fondée sur le formalisme de l’algèbre géométrique conforme (CGA). Elle vise à améliorer la représentation des données dans des espaces de grande dimension et à renforcer la performance des modèles de détection d’anomalies dans des contextes complexes, tels que les séries temporelles, la vision par ordinateur et l’analyse de signaux non l",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 2
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_3",
      "text": "inéaires. Les couches hypersphériques remplacent les hyperplans classiques des réseaux de neurones par des hypersphères apprenables, permettant des séparations de classes non linéaires et une meilleure robustesse aux distributions non gaussiennes.\n\nObjectifs scientifiques :\n\nProposer un formalisme géométrique unifié pour représenter hyperplans et hypersphères à l’aide de l’algèbre conforme.\n\nDévelopper des couches neuronales hypersphériques (denses et convolutives) adaptées à l’apprentissage pro",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 3
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_4",
      "text": "fond.\n\nDéfinir une méthode d’initialisation spécifique assurant stabilité et convergence dans ces couches.\n\nAppliquer ces modèles à la détection d’anomalies en s’appuyant sur des variantes du Support Vector Data Description (SVDD) adaptées à la géométrie hypersphérique.\n\nContributions principales :\n\nDéveloppement du modèle Deep M-SPH SVDD, une approche multi-sphères pour la détection d’anomalies basée sur la géométrie conforme.\n\nConception d’une méthode d’initialisation hypersphérique garantissa",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 4
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_5",
      "text": "nt la stabilité numérique et une convergence efficace.\n\nImplémentation pratique de couches hypersphériques denses et convolutives dans PyTorch et TensorFlow.\n\nÉtablissement d’un théorème d’approximation universelle démontrant la capacité de ces réseaux à approximer toute fonction continue sur un compact.\n\nValidation expérimentale sur des données synthétiques et réelles (MNIST, CIFAR-10) montrant des gains en performance et en stabilité.\n\nApplications :\nDétection d’anomalies dans les séries tempo",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 5
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_6",
      "text": "relles, surveillance industrielle, analyse marégraphique visuelle, diagnostic sportif haute fréquence et reconnaissance d’images complexes.\n\nExtraits représentatifs :\n« Les couches hypersphériques permettent de projeter les données sur des surfaces sphériques, facilitant la séparation des classes et la détection des points atypiques. »\n« La méthode Deep M-SPH SVDD améliore la robustesse de la détection d’anomalies en exploitant la géométrie des hypersphères dans l’espace des caractéristiques. »\n",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 6
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_7",
      "text": "« L’initialisation adaptée aux couches hypersphériques optimise la convergence et la stabilité des modèles. »\n\nPublications associées :\n\nJ. de Saint Angel, C. Saint-Jean, C. Choquet (2025) – Improving Learning for Deep Multi-Sphere Anomaly Detection with Conformal Geometric Algebra, in Recent Applications in Deep Learning.\n\nJ. de Saint Angel, C. Saint-Jean (2024) – Multi-Spheres Anomaly Detection with Hyperspherical Layers, ICMLA.\n\nJ. de Saint Angel, C. Saint-Jean (2023) – Théorème d’approximati",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 7
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_8",
      "text": "on pour neurones hypersphériques, GRETSI.\n\nJ. de Saint Angel, C. Saint-Jean (2021) – Couches Dense et Conv2D sphériques via l’algèbre géométrique conforme, ORASIS.\n\nMots-clés : réseaux de neurones, hypersphères, algèbre géométrique conforme, détection d’anomalies, séries temporelles, apprentissage profond, initialisation, SVDD, IA interprétable, optimisation, convergence.\n\nDétail par chapitre :\n\nChapitre 1 : Réseaux inspirés du formalisme de l’algèbre géométrique conforme\nCe chapitre présente le",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 8
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_9",
      "text": "s fondements mathématiques et conceptuels de l’approche. Il introduit le modèle conforme d’Hestenes, permettant de représenter hyperplans et hypersphères dans un espace vectoriel augmenté Rⁿ⁺¹,¹.\nLes couches hypersphériques y sont définies comme une extension naturelle des couches denses et convolutives classiques. Le texte décrit le produit interne conforme (x̃·is̃), les règles de calcul associées, et les contraintes de normalisation nécessaires (s̃² = ρ², e∞·is̃ = -1).\nL’auteur compare les per",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 9
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_10",
      "text": "formances entre couches denses classiques et couches hypersphériques sur des jeux de données synthétiques (Easy et Dif). Les résultats montrent une convergence plus rapide et une meilleure stabilité avec les couches hypersphériques. Le chapitre introduit également la notion de convolution hypersphérique et détaille sa mise en œuvre dans des architectures de type Conv2D.\nEnfin, une comparaison avec les réseaux RBF et les réseaux de neurones Clifford est réalisée, montrant que les couches hypersph",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 10
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_11",
      "text": "ériques constituent une généralisation géométriquement cohérente de ces modèles.\n\nChapitre 2 : Réseaux de neurones à couches hypersphériques et théorème d’approximation\nCe chapitre établit le cadre théorique garantissant la capacité d’approximation des réseaux hypersphériques. En s’appuyant sur le théorème de densité de Schwartz, l’auteur démontre qu’un réseau à une couche hypersphérique peut approximer toute fonction continue sur un compact de Rⁿ, à condition que la fonction d’activation soit r",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 11
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_12",
      "text": "égulière.\nUne comparaison est réalisée entre la structure des fonctions produites par des réseaux à couches classiques (linéaires) et celles produites par des couches hypersphériques (quadratiques). Des expérimentations sur des fonctions 1D illustrent les capacités d’approximation et la stabilité numérique des réseaux hypersphériques.\nLe chapitre conclut que ces réseaux disposent d’une expressivité au moins équivalente aux MLP classiques, tout en offrant une meilleure interprétation géométrique ",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 12
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_13",
      "text": "des frontières de décision.\n\nChapitre 3 : Application à la détection d’anomalies\nLe troisième chapitre est consacré à l’application principale de la thèse : la détection d’anomalies.\nIl commence par un état de l’art complet des méthodes classiques (Elliptic Envelope, Isolation Forest, One-Class SVM, Support Vector Data Description et variantes profondes). L’auteur montre les limites de ces approches face à la non-linéarité et à la complexité géométrique des données.\nDe nouveaux modèles sont ensu",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 13
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_14",
      "text": "ite introduits : SPH-Anomaly, SPH-SVDD, Deep SPH-SVDD et Deep M-SPH SVDD. Ces modèles exploitent la représentation conforme pour apprendre directement les centres et rayons des hypersphères englobantes des données normales.\nLe Deep M-SPH SVDD combine plusieurs hypersphères pour modéliser des sous-groupes de comportements normaux.\nDes expériences approfondies sont présentées sur données synthétiques 2D, MNIST et CIFAR-10, avec étude du paramètre ν, des métriques de performance (AUC-ROC, AUC-PR, F",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 14
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_15",
      "text": "1-score), et de la stabilité face au phénomène de “collapse” observé dans le Deep SVDD classique.\nLes résultats montrent que les couches hypersphériques apportent une meilleure robustesse et un pouvoir de détection supérieur tout en restant interprétables.\n\nAnnexes :\nLes annexes fournissent les détails mathématiques et techniques complémentaires :\n\nA : calcul des covariances et méthodes d’initialisation des couches hypersphériques.\n\nB : preuve complète du théorème d’approximation.\n\nC : analyse d",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 15
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_16",
      "text": "u phénomène de collapse et justification géométrique de sa prévention dans les architectures hypersphériques.\n\nStructure du manuscrit :\n\nFondements théoriques : algèbre géométrique conforme et représentation vectorielle des hypersphères.\n\nDéfinition et implémentation des couches hypersphériques (denses et convolutives).\n\nThéorème d’approximation pour neurones hypersphériques.\n\nApplication à la détection d’anomalies avec les modèles SPH-SVDD et M-SPH SVDD.\n\nValidation expérimentale et perspective",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 16
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_17",
      "text": "s d’évolution.\n\nRésumé synthétique :\nLa thèse de Julien de Saint Angel propose une approche innovante de l’apprentissage profond basée sur des couches hypersphériques intégrées au formalisme de l’algèbre géométrique conforme. Cette approche unifie hyperplans et hypersphères dans une même représentation, permet une meilleure compréhension géométrique des réseaux neuronaux et améliore la détection d’anomalies dans des environnements complexes. Grâce à l’introduction de modèles Deep SPH SVDD et Dee",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 17
      },
      "embedding": []
    },
    {
      "id": "these_julien.txt_chunk_18",
      "text": "p M-SPH SVDD, ainsi qu’à une méthode d’initialisation spécifique, l’auteur démontre des gains de performance significatifs, une convergence plus rapide et une interprétabilité accrue des modèles d’intelligence artificielle.\n\n",
      "metadata": {
        "source_file": "these_julien.txt",
        "chunk_index": 18
      },
      "embedding": []
    }
  ],
  "metadata": {
    "total_chunks": 15,
    "model": "none",
    "exported_at": "1761330293.6705604"
  }
}